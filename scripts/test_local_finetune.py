#!/usr/bin/env python3
"""
æœ¬åœ° LLM å¾®è°ƒæµ‹è¯•è„šæœ¬

ç”¨äºéªŒè¯å¾®è°ƒç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®ã€‚

Usage:
    python scripts/test_local_finetune.py
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def test_imports():
    """æµ‹è¯•ä¾èµ–å¯¼å…¥"""
    print("=" * 50)
    print("ğŸ“¦ æµ‹è¯•ä¾èµ–å¯¼å…¥...")
    print("=" * 50)
    
    errors = []
    
    # æµ‹è¯• torch
    try:
        import torch
        print(f"âœ… torch: {torch.__version__}")
        print(f"   - CUDA å¯ç”¨: {torch.cuda.is_available()}")
        print(f"   - MPS å¯ç”¨: {torch.backends.mps.is_available()}")
    except ImportError as e:
        errors.append(f"âŒ torch: {e}")
    
    # æµ‹è¯• transformers
    try:
        import transformers
        print(f"âœ… transformers: {transformers.__version__}")
    except ImportError as e:
        errors.append(f"âŒ transformers: {e}")
    
    # æµ‹è¯• peft
    try:
        import peft
        print(f"âœ… peft: {peft.__version__}")
    except ImportError as e:
        errors.append(f"âŒ peft: {e}")
    
    # æµ‹è¯• datasets
    try:
        import datasets
        print(f"âœ… datasets: {datasets.__version__}")
    except ImportError as e:
        errors.append(f"âŒ datasets: {e}")
    
    # æµ‹è¯• accelerate
    try:
        import accelerate
        print(f"âœ… accelerate: {accelerate.__version__}")
    except ImportError as e:
        errors.append(f"âŒ accelerate: {e}")
    
    if errors:
        print("\nâš ï¸ å‘ç°ä»¥ä¸‹é”™è¯¯:")
        for err in errors:
            print(f"   {err}")
        print("\nè¯·è¿è¡Œ: pip install torch transformers peft datasets accelerate trl")
        return False
    
    return True


def test_module_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("\n" + "=" * 50)
    print("ğŸ”§ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    print("=" * 50)
    
    try:
        from src.stage_4.fine_tuning import (
            LocalLLMFineTuner,
            LocalFineTuneConfig,
            quick_finetune,
        )
        print("âœ… LocalLLMFineTuner å¯¼å…¥æˆåŠŸ")
        print("âœ… LocalFineTuneConfig å¯¼å…¥æˆåŠŸ")
        print("âœ… quick_finetune å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_config():
    """æµ‹è¯•é…ç½®åˆ›å»º"""
    print("\n" + "=" * 50)
    print("âš™ï¸ æµ‹è¯•é…ç½®...")
    print("=" * 50)
    
    try:
        from src.stage_4.fine_tuning import LocalFineTuneConfig
        
        config = LocalFineTuneConfig(
            base_model="Qwen/Qwen2.5-0.5B-Instruct",
            output_dir="./models/test_finetune",
            lora_rank=4,
            epochs=1,
            device="cpu",
        )
        
        print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ:")
        print(f"   - åŸºç¡€æ¨¡å‹: {config.base_model}")
        print(f"   - è¾“å‡ºç›®å½•: {config.output_dir}")
        print(f"   - LoRA rank: {config.lora_rank}")
        print(f"   - è®­ç»ƒè½®æ•°: {config.epochs}")
        print(f"   - è®­ç»ƒè®¾å¤‡: {config.device}")
        return True
    except Exception as e:
        print(f"âŒ é…ç½®åˆ›å»ºå¤±è´¥: {e}")
        return False


def test_finetuner_init():
    """æµ‹è¯•å¾®è°ƒå™¨åˆå§‹åŒ–"""
    print("\n" + "=" * 50)
    print("ğŸš€ æµ‹è¯•å¾®è°ƒå™¨åˆå§‹åŒ–...")
    print("=" * 50)
    
    try:
        from src.stage_4.fine_tuning import LocalLLMFineTuner, LocalFineTuneConfig
        
        config = LocalFineTuneConfig(
            base_model="Qwen/Qwen2.5-0.5B-Instruct",
            device="cpu",
        )
        
        finetuner = LocalLLMFineTuner(config)
        print(f"âœ… å¾®è°ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - è®¾å¤‡: {finetuner.device}")
        return True
    except Exception as e:
        print(f"âŒ å¾®è°ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def test_data_exists():
    """æµ‹è¯•è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨"""
    print("\n" + "=" * 50)
    print("ğŸ“‚ æµ‹è¯•è®­ç»ƒæ•°æ®...")
    print("=" * 50)
    
    data_path = "./data/finetune/train_alpaca.json"
    
    if os.path.exists(data_path):
        import json
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… è®­ç»ƒæ•°æ®å­˜åœ¨: {data_path}")
        print(f"   - æ ·æœ¬æ•°: {len(data)}")
        return True
    else:
        print(f"âš ï¸ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {data_path}")
        print("   è¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆå‘½ä»¤ç”Ÿæˆè®­ç»ƒæ•°æ®")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("   ğŸ§ª UltimateRAG æœ¬åœ° LLM å¾®è°ƒç¯å¢ƒæµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("ä¾èµ–å¯¼å…¥", test_imports()))
    results.append(("æ¨¡å—å¯¼å…¥", test_module_import()))
    results.append(("é…ç½®æµ‹è¯•", test_config()))
    results.append(("å¾®è°ƒå™¨åˆå§‹åŒ–", test_finetuner_init()))
    results.append(("è®­ç»ƒæ•°æ®", test_data_exists()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("   ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = 0
    failed = 0
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {name}: {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print(f"\n   æ€»è®¡: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹å¾®è°ƒäº†ã€‚")
        print("\nå¿«é€Ÿå¼€å§‹:")
        print(">>> from src.stage_4.fine_tuning import quick_finetune")
        print(">>> quick_finetune(model='Qwen/Qwen2.5-0.5B-Instruct', epochs=1)")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šè¿°æç¤ºä¿®å¤é—®é¢˜ã€‚")
    
    return failed == 0


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

