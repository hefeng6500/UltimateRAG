"""
æœ¬åœ° LLM å¾®è°ƒå™¨

ä½¿ç”¨ Transformers + PEFT (LoRA) åœ¨æœ¬åœ°è¿›è¡Œ LLM å¾®è°ƒã€‚
æ”¯æŒ CPU è®­ç»ƒï¼Œé€‚é… Apple Silicon å’Œæ™®é€š x86 è®¾å¤‡ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒ Qwen2.5ã€LLaMAã€Mistral ç­‰ä¸»æµå¼€æºæ¨¡åž‹
- ä½¿ç”¨ LoRA ä½Žèµ„æºå¾®è°ƒï¼Œæ˜¾è‘—é™ä½Žå†…å­˜éœ€æ±‚
- æ”¯æŒ CPU è®­ç»ƒï¼ˆä¹Ÿæ”¯æŒ MPS/CUDA å¦‚æžœå¯ç”¨ï¼‰
- å¤ç”¨çŽ°æœ‰çš„ Alpaca æ ¼å¼è®­ç»ƒæ•°æ®
"""

import os
import json
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass, field
from pathlib import Path

from loguru import logger

# å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…æœªå®‰è£…æ—¶æŠ¥é”™
try:
    import torch
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        Trainer,
        DataCollatorForSeq2Seq,
        BitsAndBytesConfig,
    )
    from peft import (
        LoraConfig,
        get_peft_model,
        prepare_model_for_kbit_training,
        PeftModel,
        TaskType,
    )
    from datasets import Dataset
    HAS_TORCH = True
except ImportError as e:
    HAS_TORCH = False
    IMPORT_ERROR = str(e)


@dataclass
class LocalFineTuneConfig:
    """
    æœ¬åœ°å¾®è°ƒé…ç½®
    
    Attributes:
        base_model: HuggingFace æ¨¡åž‹ ID æˆ–æœ¬åœ°è·¯å¾„
        output_dir: å¾®è°ƒæ¨¡åž‹è¾“å‡ºç›®å½•
        lora_rank: LoRA ç§©ï¼Œè¶Šå°è¶ŠèŠ‚çœå†…å­˜ï¼ˆæŽ¨è 4-16ï¼‰
        lora_alpha: LoRA alpha å‚æ•°
        lora_dropout: LoRA dropout æ¯”ä¾‹
        target_modules: è¦å¾®è°ƒçš„æ¨¡å—ï¼ˆNone åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆCPU å»ºè®® 1-2ï¼‰
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ çŽ‡
        max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        warmup_ratio: å­¦ä¹ çŽ‡é¢„çƒ­æ¯”ä¾‹
        save_steps: ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”
        logging_steps: æ—¥å¿—è®°å½•æ­¥æ•°é—´éš”
        device: è®­ç»ƒè®¾å¤‡ (auto/cpu/cuda/mps)
    """
    # æ¨¡åž‹é…ç½®
    base_model: str = "Qwen/Qwen2.5-1.5B-Instruct"
    output_dir: str = "./models/finetuned_llm"
    
    # LoRA é…ç½®
    lora_rank: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    target_modules: Optional[List[str]] = None  # None = è‡ªåŠ¨æ£€æµ‹
    
    # è®­ç»ƒé…ç½®
    epochs: int = 3
    batch_size: int = 1
    gradient_accumulation_steps: int = 8
    learning_rate: float = 2e-4
    max_seq_length: int = 512
    warmup_ratio: float = 0.1
    save_steps: int = 100
    logging_steps: int = 10
    
    # è®¾å¤‡é…ç½® (auto ä¼šè‡ªåŠ¨æ£€æµ‹: MPS > CUDA > CPU)
    device: str = "auto"  # auto / cpu / cuda / mps
    
    # ä¼˜åŒ–é…ç½®
    fp16: bool = False  # CPU ä¸æ”¯æŒ
    bf16: bool = False  # éœ€è¦ç‰¹å®šç¡¬ä»¶æ”¯æŒ
    gradient_checkpointing: bool = True  # èŠ‚çœå†…å­˜


@dataclass
class ChatMessage:
    """å¯¹è¯æ¶ˆæ¯"""
    role: str  # system / user / assistant
    content: str


class LocalLLMFineTuner:
    """
    æœ¬åœ° LLM å¾®è°ƒå™¨
    
    ä½¿ç”¨ Transformers + PEFT (LoRA) è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚
    
    Example:
        >>> finetuner = LocalLLMFineTuner()
        >>> finetuner.load_data("./data/finetune/train_alpaca.json")
        >>> finetuner.train()
        >>> finetuner.save()
        >>> response = finetuner.chat("å°ç±³å…¬å¸æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ")
    """
    
    def __init__(self, config: Optional[LocalFineTuneConfig] = None):
        """
        åˆå§‹åŒ–å¾®è°ƒå™¨
        
        Args:
            config: å¾®è°ƒé…ç½®ï¼ŒNone åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        if not HAS_TORCH:
            raise ImportError(
                f"ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å®‰è£…ï¼š\n"
                f"pip install torch transformers peft datasets accelerate trl\n"
                f"åŽŸå§‹é”™è¯¯: {IMPORT_ERROR}"
            )
        
        self.config = config or LocalFineTuneConfig()
        self._setup_device()
        
        # æ¨¡åž‹å’Œåˆ†è¯å™¨
        self.model = None
        self.tokenizer = None
        self.peft_model = None
        
        # è®­ç»ƒæ•°æ®
        self.train_dataset = None
        self.eval_dataset = None
        
        logger.info(f"ðŸ”§ LocalLLMFineTuner åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - åŸºç¡€æ¨¡åž‹: {self.config.base_model}")
        logger.info(f"   - è¾“å‡ºç›®å½•: {self.config.output_dir}")
        logger.info(f"   - è®­ç»ƒè®¾å¤‡: {self.device}")
    
    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if self.config.device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = self.config.device
        
        logger.info(f"ðŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # CPU ç‰¹å®šé…ç½®
        if self.device == "cpu":
            logger.warning(
                "âš ï¸ ä½¿ç”¨ CPU è®­ç»ƒï¼Œé€Ÿåº¦ä¼šè¾ƒæ…¢ã€‚"
                "å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡åž‹ï¼ˆå¦‚ Qwen2.5-0.5B æˆ– 1.5Bï¼‰"
            )
    
    def load_model(self, model_name_or_path: Optional[str] = None):
        """
        åŠ è½½åŸºç¡€æ¨¡åž‹å’Œåˆ†è¯å™¨
        
        Args:
            model_name_or_path: æ¨¡åž‹åç§°æˆ–è·¯å¾„ï¼ŒNone åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡åž‹
        """
        model_path = model_name_or_path or self.config.base_model
        logger.info(f"ðŸ“¥ åŠ è½½æ¨¡åž‹: {model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="right",
        )
        
        # ç¡®ä¿æœ‰ pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡åž‹
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float32 if self.device == "cpu" else torch.float16,
        }
        
        # CPU åŠ è½½åˆ° CPUï¼Œå…¶ä»–è®¾å¤‡ä½¿ç”¨ device_map
        if self.device == "cpu":
            model_kwargs["device_map"] = {"": "cpu"}
        else:
            model_kwargs["device_map"] = "auto"
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        logger.info(f"âœ… æ¨¡åž‹åŠ è½½å®Œæˆ: {self.model.__class__.__name__}")
        logger.info(f"   - å‚æ•°é‡: {self.model.num_parameters() / 1e6:.1f}M")
    
    def setup_lora(self):
        """é…ç½® LoRA é€‚é…å™¨"""
        if self.model is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_model() åŠ è½½æ¨¡åž‹")
        
        logger.info("ðŸ”§ é…ç½® LoRA é€‚é…å™¨...")
        
        # è‡ªåŠ¨æ£€æµ‹ç›®æ ‡æ¨¡å—
        target_modules = self.config.target_modules
        if target_modules is None:
            # æ ¹æ®æ¨¡åž‹ç±»åž‹è‡ªåŠ¨é€‰æ‹©
            model_type = self.model.config.model_type.lower()
            if "qwen" in model_type:
                target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
            elif "llama" in model_type or "mistral" in model_type:
                target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
            else:
                # é€šç”¨é»˜è®¤
                target_modules = ["q_proj", "v_proj"]
            logger.info(f"   - è‡ªåŠ¨æ£€æµ‹ç›®æ ‡æ¨¡å—: {target_modules}")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=target_modules,
            bias="none",
        )
        
        # åº”ç”¨ LoRA
        self.peft_model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable_params, all_params = self.peft_model.get_nb_trainable_parameters()
        logger.info(
            f"âœ… LoRA é…ç½®å®Œæˆ:\n"
            f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e6:.2f}M\n"
            f"   - æ€»å‚æ•°: {all_params / 1e6:.2f}M\n"
            f"   - å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / all_params:.2f}%"
        )
    
    def load_data(
        self,
        data_path: str,
        eval_ratio: float = 0.1,
        data_format: str = "alpaca",
    ):
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSON æˆ– JSONLï¼‰
            eval_ratio: éªŒè¯é›†æ¯”ä¾‹
            data_format: æ•°æ®æ ¼å¼ (alpaca / openai / raw)
        """
        logger.info(f"ðŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
        
        # è¯»å–æ•°æ®
        if data_path.endswith(".jsonl"):
            with open(data_path, 'r', encoding='utf-8') as f:
                raw_data = [json.loads(line) for line in f if line.strip()]
        else:
            with open(data_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        
        logger.info(f"   - åŽŸå§‹æ ·æœ¬æ•°: {len(raw_data)}")
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        processed_data = []
        for item in raw_data:
            text = self._format_training_example(item, data_format)
            if text:
                processed_data.append({"text": text})
        
        logger.info(f"   - å¤„ç†åŽæ ·æœ¬æ•°: {len(processed_data)}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_list(processed_data)
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        if eval_ratio > 0:
            split = dataset.train_test_split(test_size=eval_ratio, seed=42)
            self.train_dataset = split["train"]
            self.eval_dataset = split["test"]
            logger.info(
                f"   - è®­ç»ƒé›†: {len(self.train_dataset)} æ¡\n"
                f"   - éªŒè¯é›†: {len(self.eval_dataset)} æ¡"
            )
        else:
            self.train_dataset = dataset
            self.eval_dataset = None
        
        # Tokenize æ•°æ®
        self._tokenize_dataset()
        
        logger.info("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    
    def _format_training_example(
        self,
        item: Dict[str, Any],
        data_format: str,
    ) -> str:
        """å°†æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼"""
        
        if data_format == "alpaca":
            # Alpaca æ ¼å¼: instruction, input, output
            instruction = item.get("instruction", "")
            input_text = item.get("input", "")
            output = item.get("output", "")
            
            if input_text:
                user_content = f"{instruction}\n\nå‚è€ƒä¿¡æ¯ï¼š\n{input_text}"
            else:
                user_content = instruction
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®ã€è¯¦ç»†åœ°å›žç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                {"role": "user", "content": user_content},
                {"role": "assistant", "content": output},
            ]
            
        elif data_format == "openai":
            # OpenAI æ ¼å¼: messages
            messages = item.get("messages", [])
            
        elif data_format == "raw":
            # åŽŸå§‹æ–‡æœ¬æ ¼å¼
            return item.get("text", "")
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_format}")
        
        # ä½¿ç”¨åˆ†è¯å™¨çš„ chat template
        if self.tokenizer is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_model() åŠ è½½æ¨¡åž‹")
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False,
            )
            return text
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–å¤±è´¥: {e}")
            return ""
    
    def _tokenize_dataset(self):
        """å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯"""
        
        def tokenize_function(examples):
            # åˆ†è¯
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=self.config.max_seq_length,
                padding=False,
            )
            # å¯¹äºŽå› æžœè¯­è¨€æ¨¡åž‹ï¼Œlabels = input_ids
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        # å¤„ç†è®­ç»ƒé›†
        self.train_dataset = self.train_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"],
            desc="Tokenizing train set",
        )
        
        # å¤„ç†éªŒè¯é›†
        if self.eval_dataset is not None:
            self.eval_dataset = self.eval_dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=["text"],
                desc="Tokenizing eval set",
            )
    
    def train(self):
        """
        æ‰§è¡Œ LoRA å¾®è°ƒè®­ç»ƒ
        """
        if self.peft_model is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ setup_lora() é…ç½® LoRA")
        if self.train_dataset is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_data() åŠ è½½æ•°æ®")
        
        logger.info("ðŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_ratio=self.config.warmup_ratio,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            save_total_limit=3,
            eval_strategy="steps" if self.eval_dataset else "no",  # æ–°ç‰ˆ transformers ä½¿ç”¨ eval_strategy
            eval_steps=self.config.save_steps if self.eval_dataset else None,
            load_best_model_at_end=True if self.eval_dataset else False,
            fp16=self.config.fp16,
            bf16=self.config.bf16,
            optim="adamw_torch",
            report_to="none",  # ç¦ç”¨ wandb ç­‰
            remove_unused_columns=False,
            dataloader_pin_memory=False if self.device == "cpu" else True,
            # CPU ç‰¹å®šä¼˜åŒ–
            use_cpu=True if self.device == "cpu" else False,
        )
        
        # æ•°æ®æ”¶é›†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            padding=True,
            return_tensors="pt",
        )
        
        # åˆ›å»º Trainer
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        logger.info(
            f"è®­ç»ƒé…ç½®:\n"
            f"   - è½®æ•°: {self.config.epochs}\n"
            f"   - æ‰¹æ¬¡å¤§å°: {self.config.batch_size}\n"
            f"   - æ¢¯åº¦ç´¯ç§¯: {self.config.gradient_accumulation_steps}\n"
            f"   - æœ‰æ•ˆæ‰¹æ¬¡: {self.config.batch_size * self.config.gradient_accumulation_steps}\n"
            f"   - å­¦ä¹ çŽ‡: {self.config.learning_rate}"
        )
        
        train_result = trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
        trainer.save_model()
        
        # è®°å½•è®­ç»ƒç»“æžœ
        metrics = train_result.metrics
        logger.info(
            f"âœ… è®­ç»ƒå®Œæˆ!\n"
            f"   - æ€»æ­¥æ•°: {metrics.get('total_steps', 'N/A')}\n"
            f"   - è®­ç»ƒæŸå¤±: {metrics.get('train_loss', 'N/A'):.4f}\n"
            f"   - è®­ç»ƒæ—¶é—´: {metrics.get('train_runtime', 0):.1f}s"
        )
        
        return metrics
    
    def save(self, output_path: Optional[str] = None):
        """
        ä¿å­˜å¾®è°ƒåŽçš„æ¨¡åž‹
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„ï¼ŒNone åˆ™ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        """
        output_path = output_path or self.config.output_dir
        
        if self.peft_model is None:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡åž‹")
        
        logger.info(f"ðŸ’¾ ä¿å­˜æ¨¡åž‹åˆ°: {output_path}")
        
        # ä¿å­˜ LoRA é€‚é…å™¨
        adapter_path = os.path.join(output_path, "adapter")
        self.peft_model.save_pretrained(adapter_path)
        self.tokenizer.save_pretrained(adapter_path)
        
        logger.info(f"âœ… LoRA é€‚é…å™¨å·²ä¿å­˜: {adapter_path}")
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(output_path, "finetune_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({
                "base_model": self.config.base_model,
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "epochs": self.config.epochs,
                "max_seq_length": self.config.max_seq_length,
            }, f, indent=2, ensure_ascii=False)
        
        return adapter_path
    
    def merge_and_save(self, output_path: Optional[str] = None):
        """
        åˆå¹¶ LoRA æƒé‡å¹¶ä¿å­˜å®Œæ•´æ¨¡åž‹
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        """
        output_path = output_path or os.path.join(self.config.output_dir, "merged")
        
        if self.peft_model is None:
            raise ValueError("æ²¡æœ‰å¯åˆå¹¶çš„æ¨¡åž‹")
        
        logger.info("ðŸ”€ åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡åž‹...")
        
        # åˆå¹¶æƒé‡
        merged_model = self.peft_model.merge_and_unload()
        
        # ä¿å­˜åˆå¹¶åŽçš„æ¨¡åž‹
        merged_model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)
        
        logger.info(f"âœ… åˆå¹¶æ¨¡åž‹å·²ä¿å­˜: {output_path}")
        
        return output_path
    
    def load_adapter(self, adapter_path: str, base_model_path: Optional[str] = None):
        """
        åŠ è½½å·²è®­ç»ƒçš„ LoRA é€‚é…å™¨
        
        Args:
            adapter_path: é€‚é…å™¨è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡åž‹è·¯å¾„ï¼ŒNone åˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡åž‹
        """
        base_model_path = base_model_path or self.config.base_model
        
        logger.info(f"ðŸ“¥ åŠ è½½é€‚é…å™¨: {adapter_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            adapter_path,
            trust_remote_code=True,
        )
        
        # åŠ è½½åŸºç¡€æ¨¡åž‹
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float32 if self.device == "cpu" else torch.float16,
        }
        
        if self.device == "cpu":
            model_kwargs["device_map"] = {"": "cpu"}
        else:
            model_kwargs["device_map"] = "auto"
        
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            **model_kwargs
        )
        
        # åŠ è½½ LoRA é€‚é…å™¨
        self.peft_model = PeftModel.from_pretrained(
            self.model,
            adapter_path,
        )
        
        logger.info("âœ… é€‚é…å™¨åŠ è½½å®Œæˆ")
    
    def chat(
        self,
        message: str,
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®ã€è¯¦ç»†åœ°å›žç­”ç”¨æˆ·é—®é¢˜ã€‚",
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        do_sample: bool = True,
    ) -> str:
        """
        ä½¿ç”¨å¾®è°ƒåŽçš„æ¨¡åž‹è¿›è¡Œå¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
        Returns:
            str: æ¨¡åž‹å›žå¤
        """
        model = self.peft_model if self.peft_model else self.model
        if model is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ¨¡åž‹")
        
        # æž„å»ºå¯¹è¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message},
        ]
        
        # åº”ç”¨ chat template
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        
        # ç¼–ç 
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        if self.device != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        model.eval()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True,
        )
        
        return response.strip()
    
    def run_full_pipeline(
        self,
        data_path: str = "./data/finetune/train_alpaca.json",
        test_prompt: str = "å°ç±³å…¬å¸æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿåˆ›å§‹äººæ˜¯è°ï¼Ÿ",
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å¾®è°ƒæµç¨‹
        
        Args:
            data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            test_prompt: æµ‹è¯•æç¤ºè¯
            
        Returns:
            Dict: åŒ…å«è®­ç»ƒæŒ‡æ ‡å’Œæµ‹è¯•ç»“æžœ
        """
        logger.info("=" * 60)
        logger.info("ðŸŽ¯ å¼€å§‹å®Œæ•´å¾®è°ƒæµç¨‹")
        logger.info("=" * 60)
        
        # Step 1: åŠ è½½æ¨¡åž‹
        logger.info("\nðŸ“¥ Step 1: åŠ è½½åŸºç¡€æ¨¡åž‹")
        self.load_model()
        
        # Step 2: é…ç½® LoRA
        logger.info("\nðŸ”§ Step 2: é…ç½® LoRA é€‚é…å™¨")
        self.setup_lora()
        
        # Step 3: åŠ è½½æ•°æ®
        logger.info(f"\nðŸ“‚ Step 3: åŠ è½½è®­ç»ƒæ•°æ®")
        self.load_data(data_path)
        
        # Step 4: è®­ç»ƒ
        logger.info("\nðŸš€ Step 4: å¼€å§‹è®­ç»ƒ")
        metrics = self.train()
        
        # Step 5: ä¿å­˜
        logger.info("\nðŸ’¾ Step 5: ä¿å­˜æ¨¡åž‹")
        adapter_path = self.save()
        
        # Step 6: æµ‹è¯•
        logger.info("\nðŸ§ª Step 6: æµ‹è¯•å¾®è°ƒæ•ˆæžœ")
        response = self.chat(test_prompt)
        
        logger.info(f"\nðŸ“ æµ‹è¯•å¯¹è¯:")
        logger.info(f"   é—®: {test_prompt}")
        logger.info(f"   ç­”: {response}")
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… å®Œæ•´å¾®è°ƒæµç¨‹ç»“æŸ!")
        logger.info("=" * 60)
        
        return {
            "metrics": metrics,
            "adapter_path": adapter_path,
            "test_response": response,
        }


def quick_finetune(
    data_path: str = "./data/finetune/train_alpaca.json",
    model: str = "Qwen/Qwen2.5-1.5B-Instruct",
    output_dir: str = "./models/finetuned_llm",
    epochs: int = 3,
    lora_rank: int = 8,
    device: str = "auto",
) -> Dict[str, Any]:
    """
    å¿«é€Ÿå¾®è°ƒå…¥å£å‡½æ•°
    
    Args:
        data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        model: åŸºç¡€æ¨¡åž‹
        output_dir: è¾“å‡ºç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        lora_rank: LoRA ç§©
        device: è®­ç»ƒè®¾å¤‡
        
    Returns:
        Dict: è®­ç»ƒç»“æžœ
        
    Example:
        >>> from src.stage_4.fine_tuning import quick_finetune
        >>> result = quick_finetune(
        ...     data_path="./data/finetune/train_alpaca.json",
        ...     model="Qwen/Qwen2.5-0.5B-Instruct",
        ...     epochs=3
        ... )
    """
    config = LocalFineTuneConfig(
        base_model=model,
        output_dir=output_dir,
        epochs=epochs,
        lora_rank=lora_rank,
        device=device,
    )
    
    finetuner = LocalLLMFineTuner(config)
    return finetuner.run_full_pipeline(data_path)

