"""
è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨

ç»¼åˆå¤šç§æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
"""

from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass
import os
import json
import random

from loguru import logger
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src.stage_4.config import Stage4Config, get_stage4_config
from .embedding_finetuner import TrainingPair, TrainingTriplet
from .llm_finetuner import QAPair


# æ•°æ®å¢å¼ºæç¤ºè¯
PARAPHRASE_PROMPT = """è¯·å°†ä»¥ä¸‹é—®é¢˜æ”¹å†™æˆä¸åŒçš„è¡¨è¾¾æ–¹å¼ï¼Œä¿æŒæ„æ€ä¸å˜ã€‚
ç”Ÿæˆ {num_paraphrases} ä¸ªä¸åŒçš„æ”¹å†™ç‰ˆæœ¬ã€‚

åŸé—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{"paraphrases": ["æ”¹å†™1", "æ”¹å†™2", ...]}}"""

# éš¾é¢˜ç”Ÿæˆæç¤ºè¯
HARD_QUESTION_PROMPT = """åŸºäºä»¥ä¸‹å¤šæ®µæ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆéœ€è¦ç»¼åˆåˆ†ææ‰èƒ½å›ç­”çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{texts}

è¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥éœ€è¦ç»“åˆå¤šæ®µæ–‡æ¡£çš„ä¿¡æ¯æ‰èƒ½å›ç­”
2. é—®é¢˜åº”è¯¥æœ‰ä¸€å®šéš¾åº¦ï¼Œä¸æ˜¯ç®€å•çš„äº‹å®æŸ¥æ‰¾
3. ç”Ÿæˆ {num_questions} ä¸ªè¿™æ ·çš„é—®é¢˜ï¼Œå¹¶ç»™å‡ºç­”æ¡ˆ

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
    "questions": [
        {{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}},
        {{"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}}
    ]
}}"""


class TrainingDataGenerator:
    """
    è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
    
    ç»¼åˆå¤šç§æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼š
    - é—®ç­”å¯¹ç”Ÿæˆ
    - æ•°æ®å¢å¼ºï¼ˆæ”¹å†™ï¼‰
    - è´Ÿæ ·æœ¬æŒ–æ˜
    - éš¾é¢˜ç”Ÿæˆ
    """
    
    def __init__(
        self,
        config: Optional[Stage4Config] = None,
        output_dir: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
        
        Args:
            config: é…ç½®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config or get_stage4_config()
        self.output_dir = output_dir or os.path.join(
            self.config.finetune_data_output_dir,
            "generated"
        )
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        # LLM
        self._llm = self._create_llm()
        
        # æ•°æ®å­˜å‚¨
        self._qa_pairs: List[QAPair] = []
        self._embedding_pairs: List[TrainingPair] = []
        self._embedding_triplets: List[TrainingTriplet] = []
        
        logger.info(f"ğŸ”§ è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.8,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def generate_paraphrases(
        self,
        question: str,
        num_paraphrases: int = 3,
    ) -> List[str]:
        """
        ç”Ÿæˆé—®é¢˜çš„æ”¹å†™ç‰ˆæœ¬
        
        Args:
            question: åŸå§‹é—®é¢˜
            num_paraphrases: æ”¹å†™æ•°é‡
            
        Returns:
            List[str]: æ”¹å†™ç‰ˆæœ¬åˆ—è¡¨
        """
        try:
            prompt = ChatPromptTemplate.from_template(PARAPHRASE_PROMPT)
            response = self._llm.invoke(
                prompt.format(question=question, num_paraphrases=num_paraphrases)
            )
            
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            return data.get("paraphrases", [])
            
        except Exception as e:
            logger.warning(f"æ”¹å†™ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def generate_hard_questions(
        self,
        documents: List[Document],
        num_questions: int = 5,
    ) -> List[QAPair]:
        """
        ç”Ÿæˆéœ€è¦ç»¼åˆåˆ†æçš„éš¾é¢˜
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            num_questions: ç”Ÿæˆæ•°é‡
            
        Returns:
            List[QAPair]: é—®ç­”å¯¹åˆ—è¡¨
        """
        if len(documents) < 2:
            logger.warning("éœ€è¦è‡³å°‘ 2 ä¸ªæ–‡æ¡£æ‰èƒ½ç”Ÿæˆç»¼åˆæ€§é—®é¢˜")
            return []
        
        # éšæœºé€‰æ‹©å‡ ä¸ªæ–‡æ¡£ç»„åˆ
        selected_docs = random.sample(documents, min(len(documents), 3))
        texts = "\n\n---\n\n".join([
            f"[æ–‡æ¡£ {i+1}]\n{doc.page_content[:1000]}"
            for i, doc in enumerate(selected_docs)
        ])
        
        try:
            prompt = ChatPromptTemplate.from_template(HARD_QUESTION_PROMPT)
            response = self._llm.invoke(
                prompt.format(texts=texts, num_questions=num_questions)
            )
            
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            
            pairs = []
            for qa in data.get("questions", []):
                pair = QAPair(
                    question=qa.get("question", ""),
                    answer=qa.get("answer", ""),
                    context=texts[:500],
                    difficulty="hard",
                    source="multi_doc",
                )
                if pair.question and pair.answer:
                    pairs.append(pair)
            
            return pairs
            
        except Exception as e:
            logger.warning(f"éš¾é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def augment_qa_pairs(
        self,
        qa_pairs: List[QAPair],
        num_augments: int = 2,
    ) -> List[QAPair]:
        """
        é€šè¿‡æ”¹å†™å¢å¼º QA å¯¹
        
        Args:
            qa_pairs: åŸå§‹ QA å¯¹
            num_augments: æ¯ä¸ª QA å¯¹çš„å¢å¼ºæ•°é‡
            
        Returns:
            List[QAPair]: å¢å¼ºåçš„ QA å¯¹åˆ—è¡¨
        """
        augmented = []
        
        for i, pair in enumerate(qa_pairs):
            logger.info(f"å¢å¼º QA å¯¹ {i+1}/{len(qa_pairs)}")
            
            # åŸå§‹æ•°æ®
            augmented.append(pair)
            
            # ç”Ÿæˆæ”¹å†™
            paraphrases = self.generate_paraphrases(pair.question, num_augments)
            
            for paraphrase in paraphrases:
                new_pair = QAPair(
                    question=paraphrase,
                    answer=pair.answer,
                    context=pair.context,
                    difficulty=pair.difficulty,
                    source=pair.source,
                    metadata={"augmented_from": pair.question},
                )
                augmented.append(new_pair)
        
        logger.info(f"âœ… å¢å¼ºå®Œæˆ: {len(qa_pairs)} -> {len(augmented)}")
        return augmented
    
    def generate_hard_negatives(
        self,
        documents: List[Document],
        qa_pairs: List[QAPair],
    ) -> List[TrainingTriplet]:
        """
        ä¸º Embedding è®­ç»ƒç”Ÿæˆå›°éš¾è´Ÿæ ·æœ¬
        
        å›°éš¾è´Ÿæ ·æœ¬ï¼šä¸æ­£æ ·æœ¬ç›¸ä¼¼ä½†å®é™…ä¸Šä¸ç›¸å…³çš„æ ·æœ¬
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            qa_pairs: QA å¯¹åˆ—è¡¨
            
        Returns:
            List[TrainingTriplet]: è®­ç»ƒä¸‰å…ƒç»„åˆ—è¡¨
        """
        triplets = []
        all_texts = [doc.page_content for doc in documents]
        
        for pair in qa_pairs:
            # æ‰¾åˆ°ä¸æ­£æ ·æœ¬ä¸åŒä½†å¯èƒ½æ··æ·†çš„è´Ÿæ ·æœ¬
            positives = pair.context or pair.answer
            
            # é€‰æ‹©ä¸»é¢˜ç›¸ä¼¼ä½†å†…å®¹ä¸åŒçš„è´Ÿæ ·æœ¬
            negative_candidates = [
                t for t in all_texts
                if t != positives and len(t) > 100
            ]
            
            if negative_candidates:
                # éšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºå›°éš¾è´Ÿæ ·æœ¬
                negative = random.choice(negative_candidates)
                
                triplet = TrainingTriplet(
                    anchor=pair.question,
                    positive=positives[:500],
                    negative=negative[:500],
                )
                triplets.append(triplet)
        
        logger.info(f"ç”Ÿæˆäº† {len(triplets)} ä¸ªå›°éš¾è´Ÿæ ·æœ¬ä¸‰å…ƒç»„")
        return triplets
    
    def generate_all(
        self,
        documents: List[Document],
        qa_pairs_per_doc: int = 5,
        augment: bool = True,
        generate_hard: bool = True,
    ) -> Dict[str, Any]:
        """
        ä¸€ç«™å¼ç”Ÿæˆæ‰€æœ‰è®­ç»ƒæ•°æ®
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            qa_pairs_per_doc: æ¯ä¸ªæ–‡æ¡£çš„ QA å¯¹æ•°é‡
            augment: æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
            generate_hard: æ˜¯å¦ç”Ÿæˆéš¾é¢˜
            
        Returns:
            Dict: åŒ…å«æ‰€æœ‰ç”Ÿæˆæ•°æ®çš„å­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹ä¸€ç«™å¼æ•°æ®ç”Ÿæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
        
        from .llm_finetuner import LLMFineTuner
        from .embedding_finetuner import EmbeddingFineTuner
        
        # 1. åŸºç¡€ QA å¯¹ç”Ÿæˆ
        llm_finetuner = LLMFineTuner(config=self.config)
        qa_pairs = llm_finetuner.generate_qa_pairs(
            documents,
            pairs_per_doc=qa_pairs_per_doc,
        )
        
        # 2. æ•°æ®å¢å¼º
        if augment:
            qa_pairs = self.augment_qa_pairs(qa_pairs, num_augments=2)
        
        # 3. éš¾é¢˜ç”Ÿæˆ
        if generate_hard and len(documents) >= 2:
            hard_pairs = self.generate_hard_questions(documents, num_questions=5)
            qa_pairs.extend(hard_pairs)
        
        # 4. Embedding è®­ç»ƒæ•°æ®
        embedding_finetuner = EmbeddingFineTuner(config=self.config)
        emb_pairs, emb_triplets = embedding_finetuner.generate_training_data(documents)
        
        # 5. å›°éš¾è´Ÿæ ·æœ¬
        hard_triplets = self.generate_hard_negatives(documents, qa_pairs)
        emb_triplets.extend(hard_triplets)
        
        # å­˜å‚¨
        self._qa_pairs = qa_pairs
        self._embedding_pairs = emb_pairs
        self._embedding_triplets = emb_triplets
        
        result = {
            "qa_pairs": qa_pairs,
            "embedding_pairs": emb_pairs,
            "embedding_triplets": emb_triplets,
            "statistics": {
                "total_qa_pairs": len(qa_pairs),
                "total_embedding_pairs": len(emb_pairs),
                "total_embedding_triplets": len(emb_triplets),
            }
        }
        
        logger.info(
            f"âœ… ä¸€ç«™å¼æ•°æ®ç”Ÿæˆå®Œæˆ:\n"
            f"   - QA å¯¹: {len(qa_pairs)}\n"
            f"   - Embedding è®­ç»ƒå¯¹: {len(emb_pairs)}\n"
            f"   - Embedding ä¸‰å…ƒç»„: {len(emb_triplets)}"
        )
        
        return result
    
    def save_all(self, prefix: str = ""):
        """ä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®"""
        prefix = prefix or "generated"
        
        # ä¿å­˜ QA å¯¹
        qa_path = os.path.join(self.output_dir, f"{prefix}_qa_pairs.json")
        with open(qa_path, 'w', encoding='utf-8') as f:
            json.dump(
                [p.to_dict() for p in self._qa_pairs],
                f,
                ensure_ascii=False,
                indent=2
            )
        
        # ä¿å­˜ Embedding è®­ç»ƒæ•°æ®
        emb_path = os.path.join(self.output_dir, f"{prefix}_embedding_data.json")
        with open(emb_path, 'w', encoding='utf-8') as f:
            json.dump(
                {
                    "pairs": [p.to_dict() for p in self._embedding_pairs],
                    "triplets": [t.to_dict() for t in self._embedding_triplets],
                },
                f,
                ensure_ascii=False,
                indent=2
            )
        
        logger.info(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        difficulty_dist = {}
        for p in self._qa_pairs:
            difficulty_dist[p.difficulty] = difficulty_dist.get(p.difficulty, 0) + 1
        
        return {
            "qa_pairs": {
                "total": len(self._qa_pairs),
                "difficulty_distribution": difficulty_dist,
            },
            "embedding_data": {
                "pairs": len(self._embedding_pairs),
                "triplets": len(self._embedding_triplets),
            }
        }

