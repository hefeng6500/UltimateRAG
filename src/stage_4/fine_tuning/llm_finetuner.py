"""
LLM å¾®è°ƒæ•°æ®å‡†å¤‡å™¨

è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ LLM å¾®è°ƒæ•°æ®ã€‚
"""

from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
from enum import Enum
import os
import json

from loguru import logger
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src.stage_4.config import Stage4Config, get_stage4_config


class FineTuneDataFormat(str, Enum):
    """å¾®è°ƒæ•°æ®æ ¼å¼"""
    OPENAI = "openai"       # OpenAI æ ¼å¼
    ALPACA = "alpaca"       # Alpaca æ ¼å¼
    SHAREGPT = "sharegpt"   # ShareGPT æ ¼å¼


@dataclass
class QAPair:
    """
    é—®ç­”å¯¹
    
    ç”¨äº LLM å¾®è°ƒçš„æ•°æ®æ ¼å¼ã€‚
    """
    question: str
    answer: str
    context: str = ""  # å¯é€‰çš„ä¸Šä¸‹æ–‡
    difficulty: str = "medium"  # easy / medium / hard
    source: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_openai_format(self, system_prompt: str = "") -> Dict[str, Any]:
        """è½¬æ¢ä¸º OpenAI å¾®è°ƒæ ¼å¼"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯ä¸­
        user_content = self.question
        if self.context:
            user_content = f"å‚è€ƒä¿¡æ¯ï¼š\n{self.context}\n\né—®é¢˜ï¼š{self.question}"
        
        messages.append({"role": "user", "content": user_content})
        messages.append({"role": "assistant", "content": self.answer})
        
        return {"messages": messages}
    
    def to_alpaca_format(self) -> Dict[str, str]:
        """è½¬æ¢ä¸º Alpaca æ ¼å¼"""
        return {
            "instruction": self.question,
            "input": self.context,
            "output": self.answer,
        }
    
    def to_sharegpt_format(self, system_prompt: str = "") -> Dict[str, Any]:
        """è½¬æ¢ä¸º ShareGPT æ ¼å¼"""
        conversations = []
        
        if system_prompt:
            conversations.append({"from": "system", "value": system_prompt})
        
        user_content = self.question
        if self.context:
            user_content = f"å‚è€ƒä¿¡æ¯ï¼š\n{self.context}\n\né—®é¢˜ï¼š{self.question}"
        
        conversations.append({"from": "human", "value": user_content})
        conversations.append({"from": "gpt", "value": self.answer})
        
        return {"conversations": conversations}
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "question": self.question,
            "answer": self.answer,
            "context": self.context,
            "difficulty": self.difficulty,
            "source": self.source,
            "metadata": self.metadata,
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "QAPair":
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            question=data["question"],
            answer=data["answer"],
            context=data.get("context", ""),
            difficulty=data.get("difficulty", "medium"),
            source=data.get("source", ""),
            metadata=data.get("metadata", {}),
        )


# QA å¯¹ç”Ÿæˆæç¤ºè¯
QA_GENERATION_PROMPT = """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ {num_pairs} ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥å¤šæ ·åŒ–ï¼Œæ¶µç›–æ–‡æ¡£çš„ä¸åŒæ–¹é¢
2. ç­”æ¡ˆåº”è¯¥å‡†ç¡®ã€å®Œæ•´ï¼Œå¯ä»¥ç›´æ¥ä»æ–‡æ¡£ä¸­æ‰¾åˆ°ä¾æ®
3. éš¾åº¦çº§åˆ«ï¼š{difficulty}
   - easy: ç®€å•çš„äº‹å®æ€§é—®é¢˜ï¼Œç­”æ¡ˆç›´æ¥åœ¨æ–‡æœ¬ä¸­
   - medium: éœ€è¦ç®€å•ç†è§£å’Œå½’çº³çš„é—®é¢˜
   - hard: éœ€è¦ç»¼åˆåˆ†æå¤šæ®µä¿¡æ¯çš„é—®é¢˜
4. ç­”æ¡ˆè¦è¯¦ç»†ï¼Œä¸è¦å¤ªç®€çŸ­

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
    "qa_pairs": [
        {{"question": "é—®é¢˜1", "answer": "è¯¦ç»†ç­”æ¡ˆ1"}},
        {{"question": "é—®é¢˜2", "answer": "è¯¦ç»†ç­”æ¡ˆ2"}}
    ]
}}"""


class LLMFineTuner:
    """
    LLM å¾®è°ƒæ•°æ®å‡†å¤‡å™¨
    
    è‡ªåŠ¨ä»æ–‡æ¡£ç”Ÿæˆé«˜è´¨é‡çš„ QA å¯¹ï¼Œç”¨äº LLM å¾®è°ƒã€‚
    """
    
    def __init__(
        self,
        config: Optional[Stage4Config] = None,
        output_dir: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ– LLM å¾®è°ƒå™¨
        
        Args:
            config: é…ç½®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config or get_stage4_config()
        self.output_dir = output_dir or self.config.finetune_data_output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # QA å¯¹å­˜å‚¨
        self._qa_pairs: List[QAPair] = []
        
        # LLM
        self._llm = self._create_llm()
        
        logger.info(f"ğŸ“ LLM å¾®è°ƒå™¨åˆå§‹åŒ–å®Œæˆ: è¾“å‡ºç›®å½•={self.output_dir}")
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.8,  # ç¨é«˜çš„æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _parse_qa_response(self, response: str) -> List[Dict[str, str]]:
        """è§£æ QA ç”Ÿæˆå“åº”"""
        try:
            content = response.strip()
            
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            return data.get("qa_pairs", [])
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON è§£æå¤±è´¥: {e}")
            return []
    
    def generate_qa_pairs(
        self,
        documents: List[Document],
        pairs_per_doc: int = None,
        difficulties: List[str] = None,
    ) -> List[QAPair]:
        """
        ä»æ–‡æ¡£ç”Ÿæˆ QA å¯¹
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            pairs_per_doc: æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„ QA å¯¹æ•°é‡
            difficulties: éš¾åº¦çº§åˆ«åˆ—è¡¨
            
        Returns:
            List[QAPair]: ç”Ÿæˆçš„ QA å¯¹åˆ—è¡¨
        """
        pairs_per_doc = pairs_per_doc or self.config.qa_pairs_per_doc
        difficulties = difficulties or self.config.qa_difficulty_levels
        
        logger.info(f"ğŸ”„ ç”Ÿæˆ QA å¯¹: {len(documents)} ä¸ªæ–‡æ¡£")
        
        all_pairs = []
        
        for i, doc in enumerate(documents):
            logger.info(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}")
            
            text = doc.page_content
            source = doc.metadata.get("file_name", f"doc_{i}")
            
            # ä¸ºæ¯ä¸ªéš¾åº¦çº§åˆ«ç”Ÿæˆ QA å¯¹
            pairs_per_difficulty = max(1, pairs_per_doc // len(difficulties))
            
            for difficulty in difficulties:
                prompt = ChatPromptTemplate.from_template(QA_GENERATION_PROMPT)
                
                try:
                    response = self._llm.invoke(
                        prompt.format(
                            text=text[:3000],  # é™åˆ¶é•¿åº¦
                            num_pairs=pairs_per_difficulty,
                            difficulty=difficulty,
                        )
                    )
                    
                    qa_data = self._parse_qa_response(response.content)
                    
                    for qa in qa_data:
                        pair = QAPair(
                            question=qa.get("question", ""),
                            answer=qa.get("answer", ""),
                            context=text[:500],  # ä¿ç•™éƒ¨åˆ†ä¸Šä¸‹æ–‡
                            difficulty=difficulty,
                            source=source,
                        )
                        if pair.question and pair.answer:
                            all_pairs.append(pair)
                            
                except Exception as e:
                    logger.warning(f"QA ç”Ÿæˆå¤±è´¥: {e}")
        
        self._qa_pairs.extend(all_pairs)
        
        logger.info(f"âœ… ç”Ÿæˆå®Œæˆ: {len(all_pairs)} ä¸ª QA å¯¹")
        
        return all_pairs
    
    def add_qa_pair(
        self,
        question: str,
        answer: str,
        context: str = "",
        difficulty: str = "medium",
        source: str = "",
    ):
        """æ‰‹åŠ¨æ·»åŠ  QA å¯¹"""
        pair = QAPair(
            question=question,
            answer=answer,
            context=context,
            difficulty=difficulty,
            source=source,
        )
        self._qa_pairs.append(pair)
    
    def export_jsonl(
        self,
        filepath: Optional[str] = None,
        format: FineTuneDataFormat = FineTuneDataFormat.OPENAI,
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
    ):
        """
        å¯¼å‡ºä¸º JSONL æ ¼å¼
        
        Args:
            filepath: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: æ•°æ®æ ¼å¼
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        filepath = filepath or os.path.join(
            self.output_dir,
            f"train_{format.value}.jsonl"
        )
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for pair in self._qa_pairs:
                if format == FineTuneDataFormat.OPENAI:
                    data = pair.to_openai_format(system_prompt)
                elif format == FineTuneDataFormat.ALPACA:
                    data = pair.to_alpaca_format()
                elif format == FineTuneDataFormat.SHAREGPT:
                    data = pair.to_sharegpt_format(system_prompt)
                else:
                    data = pair.to_dict()
                
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        logger.info(f"æ•°æ®å·²å¯¼å‡º: {filepath} ({len(self._qa_pairs)} æ¡)")
    
    def export_json(
        self,
        filepath: Optional[str] = None,
        format: FineTuneDataFormat = FineTuneDataFormat.ALPACA,
        system_prompt: str = "",
    ):
        """
        å¯¼å‡ºä¸º JSON æ ¼å¼
        
        Args:
            filepath: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: æ•°æ®æ ¼å¼
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        filepath = filepath or os.path.join(
            self.output_dir,
            f"train_{format.value}.json"
        )
        
        data = []
        for pair in self._qa_pairs:
            if format == FineTuneDataFormat.OPENAI:
                data.append(pair.to_openai_format(system_prompt))
            elif format == FineTuneDataFormat.ALPACA:
                data.append(pair.to_alpaca_format())
            elif format == FineTuneDataFormat.SHAREGPT:
                data.append(pair.to_sharegpt_format(system_prompt))
            else:
                data.append(pair.to_dict())
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®å·²å¯¼å‡º: {filepath} ({len(self._qa_pairs)} æ¡)")
    
    def save_qa_pairs(self, filepath: Optional[str] = None):
        """ä¿å­˜ QA å¯¹ï¼ˆåŸå§‹æ ¼å¼ï¼‰"""
        filepath = filepath or os.path.join(self.output_dir, "qa_pairs.json")
        
        data = [pair.to_dict() for pair in self._qa_pairs]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"QA å¯¹å·²ä¿å­˜: {filepath}")
    
    def load_qa_pairs(self, filepath: str):
        """åŠ è½½ QA å¯¹"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self._qa_pairs = [QAPair.from_dict(item) for item in data]
        
        logger.info(f"QA å¯¹å·²åŠ è½½: {len(self._qa_pairs)} æ¡")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        difficulty_counts = {}
        source_counts = {}
        
        for pair in self._qa_pairs:
            difficulty_counts[pair.difficulty] = difficulty_counts.get(pair.difficulty, 0) + 1
            source_counts[pair.source] = source_counts.get(pair.source, 0) + 1
        
        return {
            "total_pairs": len(self._qa_pairs),
            "difficulty_distribution": difficulty_counts,
            "source_distribution": source_counts,
        }
    
    def filter_by_difficulty(self, difficulty: str) -> List[QAPair]:
        """æŒ‰éš¾åº¦ç­›é€‰"""
        return [p for p in self._qa_pairs if p.difficulty == difficulty]
    
    def split_train_test(
        self,
        test_ratio: float = 0.1,
    ) -> tuple[List[QAPair], List[QAPair]]:
        """
        åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        
        Args:
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Tuple: (è®­ç»ƒé›†, æµ‹è¯•é›†)
        """
        import random
        
        pairs = self._qa_pairs.copy()
        random.shuffle(pairs)
        
        split_idx = int(len(pairs) * (1 - test_ratio))
        
        return pairs[:split_idx], pairs[split_idx:]
    
    @property
    def num_qa_pairs(self) -> int:
        """QA å¯¹æ•°é‡"""
        return len(self._qa_pairs)

