"""
Embedding å¾®è°ƒå™¨

ä½¿ç”¨ Sentence Transformers å¾®è°ƒ Embedding æ¨¡å‹ã€‚
"""

from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass, field
import os
import json
import random

from loguru import logger
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src.stage_4.config import Stage4Config, get_stage4_config


@dataclass
class TrainingPair:
    """
    è®­ç»ƒå¯¹ï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰
    
    ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼šanchor å’Œ positive åº”è¯¥ç›¸ä¼¼
    """
    anchor: str      # é”šç‚¹æ–‡æœ¬ï¼ˆå¦‚ï¼šé—®é¢˜ï¼‰
    positive: str    # æ­£æ ·æœ¬ï¼ˆå¦‚ï¼šç›¸å…³ç­”æ¡ˆï¼‰
    
    def to_dict(self) -> Dict[str, str]:
        return {"anchor": self.anchor, "positive": self.positive}


@dataclass
class TrainingTriplet:
    """
    è®­ç»ƒä¸‰å…ƒç»„
    
    ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼š
    - anchor å’Œ positive åº”è¯¥ç›¸ä¼¼
    - anchor å’Œ negative åº”è¯¥ä¸ç›¸ä¼¼
    """
    anchor: str      # é”šç‚¹æ–‡æœ¬
    positive: str    # æ­£æ ·æœ¬
    negative: str    # è´Ÿæ ·æœ¬
    
    def to_dict(self) -> Dict[str, str]:
        return {
            "anchor": self.anchor,
            "positive": self.positive,
            "negative": self.negative,
        }


# é—®é¢˜ç”Ÿæˆæç¤ºè¯
QUESTION_GENERATION_PROMPT = """åŸºäºä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆ {num_questions} ä¸ªå¯ä»¥ç”¨è¿™æ®µæ–‡æœ¬å›ç­”çš„é—®é¢˜ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥å¤šæ ·åŒ–ï¼Œè¦†ç›–æ–‡æœ¬çš„ä¸åŒæ–¹é¢
2. é—®é¢˜åº”è¯¥å…·ä½“ï¼Œä¸è¦å¤ªå®½æ³›
3. ç¡®ä¿é—®é¢˜å¯ä»¥ç”¨ç»™å®šæ–‡æœ¬å›ç­”

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{"questions": ["é—®é¢˜1", "é—®é¢˜2", ...]}}"""


class EmbeddingFineTuner:
    """
    Embedding å¾®è°ƒå™¨
    
    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¾®è°ƒ Embedding æ¨¡å‹ï¼Œä½¿å…¶æ›´é€‚åº”ç‰¹å®šé¢†åŸŸã€‚
    """
    
    def __init__(
        self,
        base_model: Optional[str] = None,
        output_dir: Optional[str] = None,
        config: Optional[Stage4Config] = None,
    ):
        """
        åˆå§‹åŒ– Embedding å¾®è°ƒå™¨
        
        Args:
            base_model: åŸºç¡€æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
            config: é…ç½®
        """
        self.config = config or get_stage4_config()
        self.base_model = base_model or self.config.embedding_finetune_model
        self.output_dir = output_dir or self.config.embedding_finetune_output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®­ç»ƒæ•°æ®
        self._training_pairs: List[TrainingPair] = []
        self._training_triplets: List[TrainingTriplet] = []
        
        # LLM ç”¨äºç”Ÿæˆé—®é¢˜
        self._llm = self._create_llm()
        
        logger.info(f"ğŸ“š Embedding å¾®è°ƒå™¨åˆå§‹åŒ–å®Œæˆ: åŸºç¡€æ¨¡å‹={self.base_model}")
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.7,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _generate_questions(self, text: str, num_questions: int = 3) -> List[str]:
        """ä»æ–‡æœ¬ç”Ÿæˆé—®é¢˜"""
        try:
            prompt = ChatPromptTemplate.from_template(QUESTION_GENERATION_PROMPT)
            response = self._llm.invoke(
                prompt.format(text=text[:2000], num_questions=num_questions)
            )
            
            content = response.content.strip()
            
            # è§£æ JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            data = json.loads(content)
            return data.get("questions", [])
            
        except Exception as e:
            logger.warning(f"é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def generate_training_data(
        self,
        documents: List[Document],
        questions_per_doc: int = 3,
        include_triplets: bool = True,
    ) -> Tuple[List[TrainingPair], List[TrainingTriplet]]:
        """
        ä»æ–‡æ¡£ç”Ÿæˆè®­ç»ƒæ•°æ®
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            questions_per_doc: æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°
            include_triplets: æ˜¯å¦ç”Ÿæˆä¸‰å…ƒç»„
            
        Returns:
            Tuple: (è®­ç»ƒå¯¹åˆ—è¡¨, è®­ç»ƒä¸‰å…ƒç»„åˆ—è¡¨)
        """
        logger.info(f"ğŸ”„ ç”Ÿæˆè®­ç»ƒæ•°æ®: {len(documents)} ä¸ªæ–‡æ¡£")
        
        pairs = []
        triplets = []
        all_texts = [doc.page_content for doc in documents]
        
        for i, doc in enumerate(documents):
            logger.info(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}")
            
            text = doc.page_content
            
            # ç”Ÿæˆé—®é¢˜
            questions = self._generate_questions(text, questions_per_doc)
            
            for question in questions:
                # åˆ›å»ºæ­£æ ·æœ¬å¯¹
                pair = TrainingPair(anchor=question, positive=text)
                pairs.append(pair)
                
                # åˆ›å»ºä¸‰å…ƒç»„ï¼ˆæ·»åŠ è´Ÿæ ·æœ¬ï¼‰
                if include_triplets and len(all_texts) > 1:
                    # éšæœºé€‰æ‹©ä¸€ä¸ªä¸åŒçš„æ–‡æ¡£ä½œä¸ºè´Ÿæ ·æœ¬
                    negative_texts = [t for t in all_texts if t != text]
                    if negative_texts:
                        negative = random.choice(negative_texts)
                        triplet = TrainingTriplet(
                            anchor=question,
                            positive=text,
                            negative=negative,
                        )
                        triplets.append(triplet)
        
        self._training_pairs.extend(pairs)
        self._training_triplets.extend(triplets)
        
        logger.info(f"âœ… ç”Ÿæˆå®Œæˆ: {len(pairs)} ä¸ªè®­ç»ƒå¯¹, {len(triplets)} ä¸ªä¸‰å…ƒç»„")
        
        return pairs, triplets
    
    def add_training_pair(self, anchor: str, positive: str):
        """æ‰‹åŠ¨æ·»åŠ è®­ç»ƒå¯¹"""
        self._training_pairs.append(TrainingPair(anchor=anchor, positive=positive))
    
    def add_training_triplet(self, anchor: str, positive: str, negative: str):
        """æ‰‹åŠ¨æ·»åŠ è®­ç»ƒä¸‰å…ƒç»„"""
        self._training_triplets.append(
            TrainingTriplet(anchor=anchor, positive=positive, negative=negative)
        )
    
    def save_training_data(self, filepath: Optional[str] = None):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        filepath = filepath or os.path.join(self.output_dir, "training_data.json")
        
        data = {
            "pairs": [p.to_dict() for p in self._training_pairs],
            "triplets": [t.to_dict() for t in self._training_triplets],
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜: {filepath}")
    
    def load_training_data(self, filepath: str):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self._training_pairs = [
            TrainingPair(**p) for p in data.get("pairs", [])
        ]
        self._training_triplets = [
            TrainingTriplet(**t) for t in data.get("triplets", [])
        ]
        
        logger.info(
            f"è®­ç»ƒæ•°æ®å·²åŠ è½½: {len(self._training_pairs)} ä¸ªè®­ç»ƒå¯¹, "
            f"{len(self._training_triplets)} ä¸ªä¸‰å…ƒç»„"
        )
    
    def train(
        self,
        epochs: int = None,
        batch_size: int = None,
        learning_rate: float = None,
        use_triplets: bool = True,
    ):
        """
        æ‰§è¡Œå¾®è°ƒè®­ç»ƒ
        
        Args:
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            use_triplets: æ˜¯å¦ä½¿ç”¨ä¸‰å…ƒç»„è®­ç»ƒ
        """
        epochs = epochs or self.config.embedding_finetune_epochs
        batch_size = batch_size or self.config.embedding_finetune_batch_size
        learning_rate = learning_rate or self.config.embedding_finetune_lr
        
        try:
            from sentence_transformers import SentenceTransformer, InputExample, losses
            from torch.utils.data import DataLoader
        except ImportError:
            logger.error("è¯·å®‰è£… sentence-transformers: pip install sentence-transformers")
            return
        
        logger.info(f"ğŸš€ å¼€å§‹å¾®è°ƒè®­ç»ƒ: {self.base_model}")
        logger.info(f"   - è½®æ•°: {epochs}")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - å­¦ä¹ ç‡: {learning_rate}")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = SentenceTransformer(self.base_model)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_examples = []
        
        if use_triplets and self._training_triplets:
            # ä½¿ç”¨ä¸‰å…ƒç»„
            for triplet in self._training_triplets:
                train_examples.append(InputExample(
                    texts=[triplet.anchor, triplet.positive, triplet.negative]
                ))
            
            train_dataloader = DataLoader(
                train_examples,
                shuffle=True,
                batch_size=batch_size,
            )
            
            # ä½¿ç”¨ TripletLoss
            train_loss = losses.TripletLoss(model=model)
            
        else:
            # ä½¿ç”¨å¯¹æ¯”å¯¹
            for pair in self._training_pairs:
                train_examples.append(InputExample(
                    texts=[pair.anchor, pair.positive],
                    label=1.0,  # ç›¸ä¼¼åº¦åˆ†æ•°
                ))
            
            train_dataloader = DataLoader(
                train_examples,
                shuffle=True,
                batch_size=batch_size,
            )
            
            # ä½¿ç”¨ CosineSimilarityLoss
            train_loss = losses.CosineSimilarityLoss(model=model)
        
        # è®­ç»ƒ
        warmup_steps = int(len(train_dataloader) * epochs * 0.1)
        
        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=warmup_steps,
            output_path=self.output_dir,
            show_progress_bar=True,
        )
        
        logger.info(f"âœ… å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜: {self.output_dir}")
    
    def evaluate(self, test_pairs: List[TrainingPair]) -> Dict[str, float]:
        """
        è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹
        
        Args:
            test_pairs: æµ‹è¯•æ•°æ®å¯¹
            
        Returns:
            Dict: è¯„ä¼°æŒ‡æ ‡
        """
        try:
            from sentence_transformers import SentenceTransformer
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
        except ImportError:
            logger.error("è¯·å®‰è£…å¿…è¦çš„åŒ…")
            return {}
        
        # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
        model = SentenceTransformer(self.output_dir)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for pair in test_pairs:
            anchor_emb = model.encode([pair.anchor])
            positive_emb = model.encode([pair.positive])
            sim = cosine_similarity(anchor_emb, positive_emb)[0][0]
            similarities.append(sim)
        
        return {
            "mean_similarity": float(np.mean(similarities)),
            "std_similarity": float(np.std(similarities)),
            "min_similarity": float(np.min(similarities)),
            "max_similarity": float(np.max(similarities)),
        }
    
    def get_model_path(self) -> str:
        """è·å–å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„"""
        return self.output_dir
    
    @property
    def num_training_pairs(self) -> int:
        """è®­ç»ƒå¯¹æ•°é‡"""
        return len(self._training_pairs)
    
    @property
    def num_training_triplets(self) -> int:
        """è®­ç»ƒä¸‰å…ƒç»„æ•°é‡"""
        return len(self._training_triplets)

