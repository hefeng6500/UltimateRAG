"""
GraphRAG é“¾

æ•´åˆå®ä½“æŠ½å–ã€å…³ç³»æŠ½å–ã€çŸ¥è¯†å›¾è°±ã€å›¾æ£€ç´¢ï¼Œ
å®ç°çŸ¥è¯†å›¾è°±å¢å¼ºçš„ RAGã€‚
"""

from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
import os

from loguru import logger
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src.stage_4.config import Stage4Config, get_stage4_config
from src.stage_1.vectorstore import VectorStoreManager
from src.stage_2.hybrid_retriever import HybridRetriever

from .entity_extractor import Entity, EntityExtractor
from .relation_extractor import Relation, RelationExtractor
from .knowledge_graph import KnowledgeGraph
from .graph_store import GraphStore, create_graph_store
from .graph_retriever import GraphRetriever, GraphRetrievalResult


@dataclass
class GraphRAGResult:
    """
    GraphRAG å¤„ç†ç»“æœ
    
    Attributes:
        answer: ç­”æ¡ˆ
        query: åŸå§‹æŸ¥è¯¢
        graph_context: å›¾æ£€ç´¢ä¸Šä¸‹æ–‡
        vector_context: å‘é‡æ£€ç´¢ä¸Šä¸‹æ–‡
        matched_entities: åŒ¹é…çš„å®ä½“
        related_relations: ç›¸å…³å…³ç³»
        sources: æ¥æºä¿¡æ¯
        confidence: ç½®ä¿¡åº¦
    """
    answer: str
    query: str
    graph_context: str = ""
    vector_context: str = ""
    matched_entities: List[Dict[str, Any]] = field(default_factory=list)
    related_relations: List[Dict[str, Any]] = field(default_factory=list)
    sources: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0


# GraphRAG ç”Ÿæˆæç¤ºè¯
GRAPH_RAG_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»¼åˆåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{graph_context}

ã€æ–‡æ¡£å†…å®¹ã€‘
{vector_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. ä¼˜å…ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å…³ç³»ä¿¡æ¯
2. ç»“åˆæ–‡æ¡£å†…å®¹è¿›è¡Œè¡¥å……è¯´æ˜
3. å¦‚æœæ¶‰åŠå®ä½“é—´å…³ç³»ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

# å…¨å±€æ‘˜è¦æç¤ºè¯
GLOBAL_SUMMARY_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºçŸ¥è¯†å›¾è°±çš„æ•´ä½“ç»“æ„ç”Ÿæˆå…¨å±€æ€§æ‘˜è¦ã€‚

çŸ¥è¯†å›¾è°±ç»Ÿè®¡ï¼š
- å®ä½“æ•°é‡ï¼š{num_entities}
- å…³ç³»æ•°é‡ï¼š{num_relations}
- ä¸»è¦å®ä½“ç±»å‹ï¼š{entity_types}
- ä¸»è¦å…³ç³»ç±»å‹ï¼š{relation_types}

ä¸»è¦å®ä½“åˆ—è¡¨ï¼š
{top_entities}

æ ¸å¿ƒå…³ç³»ï¼š
{top_relations}

è¯·ç”Ÿæˆä¸€ä¸ªå…¨å±€æ€§æ‘˜è¦ï¼Œæ¦‚æ‹¬è¿™ä¸ªçŸ¥è¯†å›¾è°±æ‰€æè¿°çš„ä¸»è¦å†…å®¹å’Œæ ¸å¿ƒå…³ç³»ï¼š"""


class GraphRAGChain:
    """
    GraphRAG é“¾
    
    å®ç°çŸ¥è¯†å›¾è°±å¢å¼ºçš„ RAGï¼š
    1. ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±
    2. ç»“åˆå›¾æ£€ç´¢å’Œå‘é‡æ£€ç´¢
    3. åˆ©ç”¨å›¾ç»“æ„ä¿¡æ¯å¢å¼ºç­”æ¡ˆ
    """
    
    def __init__(
        self,
        documents: Optional[List[Document]] = None,
        config: Optional[Stage4Config] = None,
        graph_store: Optional[GraphStore] = None,
        vectorstore_manager: Optional[VectorStoreManager] = None,
        graph_name: str = "default",
        force_rebuild: bool = False,
    ):
        """
        åˆå§‹åŒ– GraphRAG é“¾
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            config: é…ç½®
            graph_store: å›¾å­˜å‚¨
            vectorstore_manager: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            graph_name: å›¾è°±åç§°
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºå›¾è°±
        """
        self.config = config or get_stage4_config()
        self.graph_name = graph_name
        
        # åˆå§‹åŒ– LLM
        self._llm = self._create_llm()
        
        # åˆå§‹åŒ–å›¾å­˜å‚¨
        self._graph_store = graph_store or create_graph_store(config=self.config)
        
        # åˆå§‹åŒ–æˆ–åŠ è½½çŸ¥è¯†å›¾è°±
        if force_rebuild or not self._graph_store.exists(graph_name):
            self._knowledge_graph = KnowledgeGraph()
        else:
            self._knowledge_graph = self._graph_store.load(graph_name) or KnowledgeGraph()
        
        # åˆå§‹åŒ–å®ä½“å’Œå…³ç³»æŠ½å–å™¨
        self._entity_extractor = EntityExtractor(config=self.config, llm=self._llm)
        self._relation_extractor = RelationExtractor(config=self.config, llm=self._llm)
        
        # åˆå§‹åŒ–å›¾æ£€ç´¢å™¨
        self._graph_retriever = GraphRetriever(
            self._knowledge_graph,
            config=self.config,
            llm=self._llm,
        )
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆç”¨äºæ··åˆæ£€ç´¢ï¼‰
        self._vectorstore_manager = vectorstore_manager
        self._hybrid_retriever = None
        
        if documents:
            self._init_vector_retriever(documents)
        
        # å¦‚æœå¼ºåˆ¶é‡å»ºä¸”æœ‰æ–‡æ¡£ï¼Œæ„å»ºå›¾è°±
        if force_rebuild and documents:
            self.build_knowledge_graph(documents)
        
        logger.info(
            f"ğŸ¯ GraphRAG é“¾åˆå§‹åŒ–å®Œæˆ: "
            f"å›¾è°± '{graph_name}' (èŠ‚ç‚¹: {self._knowledge_graph.num_nodes}, "
            f"è¾¹: {self._knowledge_graph.num_edges})"
        )
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.7,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _init_vector_retriever(self, documents: List[Document]):
        """åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨"""
        from src.stage_1.config import get_config
        base_config = get_config()
        
        if self._vectorstore_manager is None:
            self._vectorstore_manager = VectorStoreManager(
                base_config,
                collection_name=f"graph_rag_{self.graph_name}"
            )
        
        self._hybrid_retriever = HybridRetriever(
            documents=documents,
            vectorstore_manager=self._vectorstore_manager,
            config=base_config,
        )
    
    def build_knowledge_graph(
        self,
        documents: Optional[List[Document]] = None,
        save: bool = True,
    ):
        """
        ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨åˆå§‹åŒ–æ—¶çš„æ–‡æ¡£ï¼‰
            save: æ˜¯å¦ä¿å­˜å›¾è°±
        """
        if documents is None:
            logger.warning("æ²¡æœ‰æä¾›æ–‡æ¡£ï¼Œæ— æ³•æ„å»ºå›¾è°±")
            return
        
        logger.info(f"ğŸ”¨ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±: {len(documents)} ä¸ªæ–‡æ¡£")
        
        all_entities = []
        all_relations = []
        
        for i, doc in enumerate(documents):
            logger.info(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}")
            
            text = doc.page_content
            source_doc = doc.metadata.get("file_name", f"doc_{i}")
            
            # 1. å®ä½“æŠ½å–
            entities = self._entity_extractor.extract(text, source_doc=source_doc)
            all_entities.extend(entities)
            
            # 2. å…³ç³»æŠ½å–
            if entities:
                relations = self._relation_extractor.extract(text, entities)
                all_relations.extend(relations)
        
        # 3. åˆå¹¶å®ä½“
        merged_entities = self._entity_extractor.merge_entities(all_entities)
        
        # 4. åˆå¹¶å…³ç³»
        merged_relations = self._relation_extractor.merge_relations(all_relations)
        
        # 5. æ„å»ºå›¾è°±
        for entity in merged_entities:
            self._knowledge_graph.add_entity(entity)
        
        for relation in merged_relations:
            self._knowledge_graph.add_relation(relation)
        
        # 6. ä¿å­˜å›¾è°±
        if save:
            self._graph_store.save(self._knowledge_graph, self.graph_name)
        
        # 7. æ›´æ–°æ£€ç´¢å™¨
        self._graph_retriever = GraphRetriever(
            self._knowledge_graph,
            config=self.config,
            llm=self._llm,
        )
        
        logger.info(
            f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: "
            f"{self._knowledge_graph.num_nodes} ä¸ªå®ä½“, "
            f"{self._knowledge_graph.num_edges} æ¡å…³ç³»"
        )
    
    def _format_vector_docs(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–å‘é‡æ£€ç´¢ç»“æœ"""
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("file_name", "æœªçŸ¥æ¥æº")
            content = doc.page_content.strip()[:500]  # é™åˆ¶é•¿åº¦
            formatted.append(f"[æ–‡æ¡£ {i}] (æ¥æº: {source})\n{content}")
        
        return "\n\n---\n\n".join(formatted)
    
    def ask(
        self,
        question: str,
        use_vector_retrieval: bool = True,
        top_k: int = None,
    ) -> GraphRAGResult:
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_vector_retrieval: æ˜¯å¦åŒæ—¶ä½¿ç”¨å‘é‡æ£€ç´¢
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            GraphRAGResult: å¤„ç†ç»“æœ
        """
        top_k = top_k or self.config.top_k
        
        logger.info(f"â“ GraphRAG é—®ç­”: {question}")
        
        # 1. å›¾æ£€ç´¢
        graph_result = self._graph_retriever.retrieve(question, top_k=top_k)
        graph_context = graph_result.context
        
        # 2. å‘é‡æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        vector_context = ""
        vector_docs = []
        if use_vector_retrieval and self._hybrid_retriever:
            vector_docs = self._hybrid_retriever.search(question, k=top_k)
            vector_context = self._format_vector_docs(vector_docs)
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        prompt = ChatPromptTemplate.from_template(GRAPH_RAG_PROMPT)
        
        response = self._llm.invoke(
            prompt.format(
                graph_context=graph_context or "æ— ç›¸å…³å›¾è°±ä¿¡æ¯",
                vector_context=vector_context or "æ— ç›¸å…³æ–‡æ¡£",
                question=question,
            )
        )
        
        answer = response.content
        
        # 4. æ„å»ºç»“æœ
        matched_entities = [e.to_dict() for e in graph_result.matched_entities]
        
        related_relations = []
        if graph_result.subgraph:
            related_relations = [r.to_dict() for r in graph_result.subgraph.edges[:10]]
        
        sources = []
        for doc in vector_docs:
            sources.append({
                "content": doc.page_content[:200] + "...",
                "source": doc.metadata.get("file_name", "æœªçŸ¥"),
                "type": "document",
            })
        for entity in graph_result.matched_entities:
            sources.append({
                "content": entity.description,
                "source": entity.name,
                "type": "entity",
            })
        
        result = GraphRAGResult(
            answer=answer,
            query=question,
            graph_context=graph_context,
            vector_context=vector_context,
            matched_entities=matched_entities,
            related_relations=related_relations,
            sources=sources,
            confidence=graph_result.confidence,
        )
        
        logger.info(f"âœ… GraphRAG é—®ç­”å®Œæˆ")
        return result
    
    def ask_simple(self, question: str) -> str:
        """ç®€åŒ–ç‰ˆé—®ç­”ï¼Œåªè¿”å›ç­”æ¡ˆ"""
        result = self.ask(question)
        return result.answer
    
    def generate_global_summary(self) -> str:
        """
        ç”ŸæˆçŸ¥è¯†å›¾è°±çš„å…¨å±€æ‘˜è¦
        
        Returns:
            str: å…¨å±€æ‘˜è¦
        """
        stats = self._knowledge_graph.get_statistics()
        
        # è·å–ä¸»è¦å®ä½“
        all_entities = self._knowledge_graph.get_all_entities()
        top_entities = "\n".join([
            f"- {e.name} ({e.type.value}): {e.description}"
            for e in all_entities[:20]
        ])
        
        # è·å–æ ¸å¿ƒå…³ç³»
        all_relations = self._knowledge_graph.get_all_relations()
        top_relations = "\n".join([
            f"- {r.source} --[{r.relation_type.value}]--> {r.target}"
            for r in all_relations[:20]
        ])
        
        prompt = ChatPromptTemplate.from_template(GLOBAL_SUMMARY_PROMPT)
        
        response = self._llm.invoke(
            prompt.format(
                num_entities=stats["num_nodes"],
                num_relations=stats["num_edges"],
                entity_types=", ".join(stats["entity_type_counts"].keys()),
                relation_types=", ".join(stats["relation_type_counts"].keys()),
                top_entities=top_entities or "æ— ",
                top_relations=top_relations or "æ— ",
            )
        )
        
        return response.content
    
    def get_entity_info(self, entity_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å®ä½“è¯¦ç»†ä¿¡æ¯
        
        Args:
            entity_name: å®ä½“åç§°
            
        Returns:
            Optional[Dict]: å®ä½“ä¿¡æ¯
        """
        entity = self._knowledge_graph.get_entity_by_name(entity_name)
        if not entity:
            return None
        
        subgraph = self._knowledge_graph.get_neighbors(entity_name, hops=1)
        
        return {
            "entity": entity.to_dict(),
            "neighbors": [e.to_dict() for e in subgraph.nodes if e.id != entity.id],
            "relations": [r.to_dict() for r in subgraph.edges],
        }
    
    def find_path(
        self,
        source_name: str,
        target_name: str,
    ) -> Optional[List[Dict[str, Any]]]:
        """
        æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„è·¯å¾„
        
        Args:
            source_name: æºå®ä½“åç§°
            target_name: ç›®æ ‡å®ä½“åç§°
            
        Returns:
            Optional[List]: è·¯å¾„ä¿¡æ¯
        """
        path = self._knowledge_graph.find_path(source_name, target_name)
        
        if not path:
            return None
        
        return [
            {
                "entity": entity.to_dict(),
                "relation": relation.to_dict() if relation else None,
            }
            for entity, relation in path
        ]
    
    @property
    def knowledge_graph(self) -> KnowledgeGraph:
        """è·å–çŸ¥è¯†å›¾è°±"""
        return self._knowledge_graph
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        return self._knowledge_graph.get_statistics()

