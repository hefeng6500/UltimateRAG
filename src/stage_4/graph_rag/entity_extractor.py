"""
å®ä½“æŠ½å–å™¨

ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æŠ½å–å‘½åå®ä½“ã€‚
"""

from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
from enum import Enum
import json
import hashlib

from loguru import logger
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from src.stage_4.config import Stage4Config, get_stage4_config


class EntityType(str, Enum):
    """å®ä½“ç±»å‹æšä¸¾"""
    PERSON = "Person"
    ORGANIZATION = "Organization"
    LOCATION = "Location"
    EVENT = "Event"
    CONCEPT = "Concept"
    PRODUCT = "Product"
    TIME = "Time"
    OTHER = "Other"


@dataclass
class Entity:
    """
    å®ä½“æ•°æ®ç±»
    
    Attributes:
        name: å®ä½“åç§°
        type: å®ä½“ç±»å‹
        description: å®ä½“æè¿°
        aliases: å®ä½“åˆ«ååˆ—è¡¨
        source_text: æ¥æºæ–‡æœ¬ç‰‡æ®µ
        source_doc: æ¥æºæ–‡æ¡£ID
        confidence: ç½®ä¿¡åº¦åˆ†æ•°
        metadata: é¢å¤–å…ƒæ•°æ®
    """
    name: str
    type: EntityType
    description: str = ""
    aliases: List[str] = field(default_factory=list)
    source_text: str = ""
    source_doc: str = ""
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def id(self) -> str:
        """ç”Ÿæˆå®ä½“å”¯ä¸€ID"""
        content = f"{self.name}:{self.type.value}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "name": self.name,
            "type": self.type.value,
            "description": self.description,
            "aliases": self.aliases,
            "source_text": self.source_text,
            "source_doc": self.source_doc,
            "confidence": self.confidence,
            "metadata": self.metadata,
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Entity":
        """ä»å­—å…¸åˆ›å»ºå®ä½“"""
        return cls(
            name=data["name"],
            type=EntityType(data["type"]),
            description=data.get("description", ""),
            aliases=data.get("aliases", []),
            source_text=data.get("source_text", ""),
            source_doc=data.get("source_doc", ""),
            confidence=data.get("confidence", 1.0),
            metadata=data.get("metadata", {}),
        )


# Pydantic æ¨¡å‹ç”¨äºç»“æ„åŒ–è¾“å‡º
class ExtractedEntity(BaseModel):
    """LLM è¾“å‡ºçš„å®ä½“ç»“æ„"""
    name: str = Field(description="å®ä½“åç§°")
    type: str = Field(description="å®ä½“ç±»å‹")
    description: str = Field(description="å®ä½“çš„ç®€çŸ­æè¿°")
    aliases: List[str] = Field(default_factory=list, description="å®ä½“çš„å…¶ä»–åç§°æˆ–åˆ«å")


class EntityExtractionResult(BaseModel):
    """å®ä½“æŠ½å–ç»“æœ"""
    entities: List[ExtractedEntity] = Field(description="æŠ½å–çš„å®ä½“åˆ—è¡¨")


# å®ä½“æŠ½å–æç¤ºè¯
ENTITY_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®ä½“æŠ½å–ä¸“å®¶ã€‚è¯·ä»ç»™å®šçš„æ–‡æœ¬ä¸­æŠ½å–æ‰€æœ‰é‡è¦çš„å‘½åå®ä½“ã€‚

æ”¯æŒçš„å®ä½“ç±»å‹ï¼š
- Person: äººç‰©ï¼ˆå¦‚ï¼šå¼ ä¸‰ã€é©¬äº‘ã€ä»»æ­£éï¼‰
- Organization: ç»„ç»‡/å…¬å¸ï¼ˆå¦‚ï¼šåä¸ºã€é˜¿é‡Œå·´å·´ã€ä¸­å›½é“¶è¡Œï¼‰
- Location: åœ°ç‚¹ï¼ˆå¦‚ï¼šåŒ—äº¬ã€æ·±åœ³ã€ç¡…è°·ï¼‰
- Event: äº‹ä»¶ï¼ˆå¦‚ï¼šä¸–ç•Œæ¯ã€åŒåä¸€ã€å¹´åº¦å¤§ä¼šï¼‰
- Concept: æ¦‚å¿µ/æœ¯è¯­ï¼ˆå¦‚ï¼šäººå·¥æ™ºèƒ½ã€åŒºå—é“¾ã€é‡å­è®¡ç®—ï¼‰
- Product: äº§å“ï¼ˆå¦‚ï¼šiPhoneã€ChatGPTã€å¾®ä¿¡ï¼‰
- Time: æ—¶é—´ï¼ˆå¦‚ï¼š2024å¹´ã€ç¬¬ä¸‰å­£åº¦ã€å»å¹´ï¼‰

æŠ½å–è¦æ±‚ï¼š
1. åªæŠ½å–æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°çš„å®ä½“
2. ä¸ºæ¯ä¸ªå®ä½“æä¾›ç®€çŸ­æè¿°
3. å¦‚æœå®ä½“æœ‰åˆ«åï¼Œè¯·ä¸€å¹¶åˆ—å‡º
4. å¿½ç•¥è¿‡äºé€šç”¨çš„è¯æ±‡

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
```json
{{
    "entities": [
        {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "description": "ç®€çŸ­æè¿°", "aliases": ["åˆ«å1", "åˆ«å2"]}},
        {{"name": "å®ä½“åç§°2", "type": "å®ä½“ç±»å‹", "description": "ç®€çŸ­æè¿°", "aliases": []}}
    ]
}}
```"""


class EntityExtractor:
    """
    å®ä½“æŠ½å–å™¨
    
    ä½¿ç”¨ LLM ä»æ–‡æœ¬ä¸­æŠ½å–å‘½åå®ä½“ã€‚
    """
    
    def __init__(
        self,
        config: Optional[Stage4Config] = None,
        llm: Optional[ChatOpenAI] = None,
    ):
        """
        åˆå§‹åŒ–å®ä½“æŠ½å–å™¨
        
        Args:
            config: Stage4 é…ç½®
            llm: LLM å®ä¾‹ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        """
        self.config = config or get_stage4_config()
        self._llm = llm or self._create_llm()
        self._prompt = ChatPromptTemplate.from_template(ENTITY_EXTRACTION_PROMPT)
        
        # å®ä½“ç±»å‹æ˜ å°„
        self._type_mapping = {
            "Person": EntityType.PERSON,
            "Organization": EntityType.ORGANIZATION,
            "Location": EntityType.LOCATION,
            "Event": EntityType.EVENT,
            "Concept": EntityType.CONCEPT,
            "Product": EntityType.PRODUCT,
            "Time": EntityType.TIME,
        }
        
        logger.info("ğŸ” å®ä½“æŠ½å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0,  # ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _parse_response(self, response: str) -> List[ExtractedEntity]:
        """è§£æ LLM å“åº”"""
        try:
            # å°è¯•æå– JSON
            content = response.strip()
            
            logger.debug(f"LLM åŸå§‹å“åº”: {content[:500]}...")
            
            # å¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                parts = content.split("```")
                if len(parts) >= 2:
                    content = parts[1].strip()
                    # ç§»é™¤å¯èƒ½çš„è¯­è¨€æ ‡è¯†ï¼ˆå¦‚ jsonï¼‰
                    if content.startswith("json"):
                        content = content[4:].strip()
            
            logger.debug(f"å¤„ç†åçš„ JSON: {content[:300]}...")
            
            data = json.loads(content)
            
            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
            if isinstance(data, list):
                entities_data = data
            elif isinstance(data, dict) and "entities" in data:
                entities_data = data["entities"]
            else:
                entities_data = [data]
            
            entities = []
            for item in entities_data:
                try:
                    entity = ExtractedEntity(
                        name=item.get("name", ""),
                        type=item.get("type", "Other"),
                        description=item.get("description", ""),
                        aliases=item.get("aliases", []),
                    )
                    if entity.name:  # å¿½ç•¥ç©ºåç§°
                        entities.append(entity)
                        logger.debug(f"è§£æåˆ°å®ä½“: {entity.name} ({entity.type})")
                except Exception as e:
                    logger.warning(f"è§£æå®ä½“å¤±è´¥: {item}, é”™è¯¯: {e}")
            
            logger.info(f"ğŸ“¦ æˆåŠŸè§£æ {len(entities)} ä¸ªå®ä½“")
            return entities
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"âŒ åŸå§‹å†…å®¹: {response[:500]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ è§£æå“åº”æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            logger.error(f"âŒ åŸå§‹å†…å®¹: {response[:500]}...")
            return []
    
    def extract(
        self,
        text: str,
        source_doc: str = "",
        max_entities: Optional[int] = None,
    ) -> List[Entity]:
        """
        ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            source_doc: æ¥æºæ–‡æ¡£ID
            max_entities: æœ€å¤§å®ä½“æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            
        Returns:
            List[Entity]: æŠ½å–çš„å®ä½“åˆ—è¡¨
        """
        if not text.strip():
            logger.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡å®ä½“æŠ½å–")
            return []
        
        max_entities = max_entities or self.config.max_entities_per_chunk
        
        logger.info(f"ğŸ” å¼€å§‹å®ä½“æŠ½å–ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        try:
            # æ„å»ºæç¤º
            prompt = self._prompt.format(text=text)
            
            # è°ƒç”¨ LLM
            logger.debug(f"è°ƒç”¨ LLM è¿›è¡Œå®ä½“æŠ½å–...")
            response = self._llm.invoke(prompt)
            
            logger.debug(f"LLM å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            
            # è§£æå“åº”
            extracted = self._parse_response(response.content)
            
            if not extracted:
                logger.warning(f"âš ï¸ æœªä»æ–‡æœ¬ä¸­æŠ½å–åˆ°ä»»ä½•å®ä½“")
                logger.warning(f"âš ï¸ æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
            
            # è½¬æ¢ä¸º Entity å¯¹è±¡
            entities = []
            for item in extracted[:max_entities]:
                entity_type = self._type_mapping.get(item.type, EntityType.OTHER)
                entity = Entity(
                    name=item.name,
                    type=entity_type,
                    description=item.description,
                    aliases=item.aliases,
                    source_text=text[:200],  # ä¿ç•™éƒ¨åˆ†æºæ–‡æœ¬
                    source_doc=source_doc,
                )
                entities.append(entity)
            
            logger.info(f"âœ… å®ä½“æŠ½å–å®Œæˆ: {len(entities)} ä¸ªå®ä½“")
            for e in entities:
                logger.debug(f"   - {e.name} ({e.type.value})")
            
            return entities
            
        except Exception as e:
            logger.error(f"âŒ å®ä½“æŠ½å–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def extract_batch(
        self,
        texts: List[str],
        source_docs: Optional[List[str]] = None,
    ) -> List[List[Entity]]:
        """
        æ‰¹é‡æŠ½å–å®ä½“
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            source_docs: æ¥æºæ–‡æ¡£IDåˆ—è¡¨
            
        Returns:
            List[List[Entity]]: æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„å®ä½“åˆ—è¡¨
        """
        source_docs = source_docs or [""] * len(texts)
        results = []
        
        for i, (text, source_doc) in enumerate(zip(texts, source_docs)):
            logger.info(f"æŠ½å–è¿›åº¦: {i+1}/{len(texts)}")
            entities = self.extract(text, source_doc)
            results.append(entities)
        
        total = sum(len(e) for e in results)
        logger.info(f"âœ… æ‰¹é‡æŠ½å–å®Œæˆ: {len(texts)} ä¸ªæ–‡æœ¬, å…± {total} ä¸ªå®ä½“")
        
        return results
    
    def merge_entities(self, entities: List[Entity]) -> List[Entity]:
        """
        åˆå¹¶ç›¸åŒæˆ–ç›¸ä¼¼çš„å®ä½“
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            List[Entity]: åˆå¹¶åçš„å®ä½“åˆ—è¡¨
        """
        if not entities:
            return []
        
        # æŒ‰åç§°åˆ†ç»„
        entity_groups: Dict[str, List[Entity]] = {}
        
        for entity in entities:
            # æ ‡å‡†åŒ–åç§°ï¼ˆå°å†™ï¼Œå»ç©ºæ ¼ï¼‰
            key = entity.name.lower().strip()
            if key not in entity_groups:
                entity_groups[key] = []
            entity_groups[key].append(entity)
        
        # åˆå¹¶æ¯ç»„å®ä½“
        merged = []
        for group in entity_groups.values():
            if len(group) == 1:
                merged.append(group[0])
            else:
                # åˆå¹¶ï¼šå–æœ€å¸¸è§çš„ç±»å‹ï¼Œåˆå¹¶æè¿°å’Œåˆ«å
                main = group[0]
                all_aliases = set(main.aliases)
                all_descriptions = [main.description]
                
                for e in group[1:]:
                    all_aliases.update(e.aliases)
                    all_aliases.add(e.name)  # å°†å…¶ä»–åç§°ä½œä¸ºåˆ«å
                    if e.description:
                        all_descriptions.append(e.description)
                
                # å»é™¤ä¸»åç§°æœ¬èº«
                all_aliases.discard(main.name)
                all_aliases.discard(main.name.lower())
                
                merged_entity = Entity(
                    name=main.name,
                    type=main.type,
                    description=" | ".join(set(all_descriptions)),
                    aliases=list(all_aliases),
                    source_doc=main.source_doc,
                    confidence=sum(e.confidence for e in group) / len(group),
                )
                merged.append(merged_entity)
        
        logger.info(f"åˆå¹¶å®ä½“: {len(entities)} -> {len(merged)}")
        return merged

