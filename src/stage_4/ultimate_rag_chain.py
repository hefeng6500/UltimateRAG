"""
ç»ˆæ RAG é“¾

æ•´åˆ Stage 1-4 æ‰€æœ‰ç»„ä»¶ï¼Œå®ç°æœ€å¼ºå¤§çš„ RAG ç³»ç»Ÿï¼š
- Stage 1: åŸºç¡€å‘é‡æ£€ç´¢
- Stage 2: æ··åˆæ£€ç´¢ + é‡æ’åº
- Stage 3: Agentic RAGï¼ˆè·¯ç”±ã€è‡ªåæ€ã€å·¥å…·ï¼‰
- Stage 4: GraphRAG
"""

from typing import List, Optional, Dict, Any, Literal
from dataclasses import dataclass, field
from enum import Enum

from loguru import logger
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from src.stage_1.config import get_config
from src.stage_1.vectorstore import VectorStoreManager
from src.stage_2.hybrid_retriever import HybridRetriever
from src.stage_2.reranker import Reranker, SimpleReranker
from src.stage_3.router import QueryRouter, RouteType
from src.stage_3.self_rag import SelfRAG
from src.stage_3.context_compressor import ContextCompressor, KeywordBasedCompressor

from .config import Stage4Config, get_stage4_config
from .graph_rag import (
    GraphRAGChain,
    KnowledgeGraph,
    GraphRetriever,
    GraphRetrievalResult,
)


class RetrievalMode(str, Enum):
    """æ£€ç´¢æ¨¡å¼"""
    VECTOR = "vector"       # çº¯å‘é‡æ£€ç´¢
    HYBRID = "hybrid"       # æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰
    GRAPH = "graph"         # çº¯å›¾æ£€ç´¢
    FUSION = "fusion"       # èåˆæ£€ç´¢ï¼ˆå‘é‡ + å›¾ï¼‰
    AUTO = "auto"           # è‡ªåŠ¨é€‰æ‹©


@dataclass
class UltimateRAGResult:
    """
    ç»ˆæ RAG å¤„ç†ç»“æœ
    
    æ•´åˆæ‰€æœ‰é˜¶æ®µçš„ç»“æœä¿¡æ¯ã€‚
    """
    answer: str
    query: str
    retrieval_mode: RetrievalMode
    route_type: Optional[RouteType] = None
    
    # æ£€ç´¢ä¿¡æ¯
    retrieved_docs: List[Dict[str, Any]] = field(default_factory=list)
    graph_entities: List[Dict[str, Any]] = field(default_factory=list)
    graph_relations: List[Dict[str, Any]] = field(default_factory=list)
    
    # è´¨é‡ä¿¡æ¯
    confidence: float = 0.0
    quality_score: float = 0.0
    iterations: int = 1
    
    # æ¥æºè¿½æº¯
    sources: List[Dict[str, Any]] = field(default_factory=list)
    reasoning_chain: List[str] = field(default_factory=list)
    
    # å·¥å…·ä½¿ç”¨
    tool_used: Optional[str] = None
    tool_result: Optional[str] = None


# ç»ˆæ RAG æç¤ºè¯
ULTIMATE_RAG_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»¼åˆåˆ©ç”¨çŸ¥è¯†å›¾è°±å’Œæ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚

ã€çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‘
{graph_context}

ã€ç›¸å…³æ–‡æ¡£ã€‘
{doc_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. ç»¼åˆåˆ©ç”¨å›¾è°±ä¿¡æ¯å’Œæ–‡æ¡£å†…å®¹
2. ä¼˜å…ˆä½¿ç”¨å›¾è°±ä¸­çš„ç»“æ„åŒ–å…³ç³»ä¿¡æ¯
3. å¦‚æœæ¶‰åŠå®ä½“å…³ç³»ï¼Œè¯·æ˜ç¡®è¯´æ˜
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜
5. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

# æŸ¥è¯¢åˆ†ææç¤ºè¯
QUERY_ANALYSIS_PROMPT = """åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­æœ€é€‚åˆçš„æ£€ç´¢æ¨¡å¼ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

æ£€ç´¢æ¨¡å¼è¯´æ˜ï¼š
- vector: ç®€å•çš„è¯­ä¹‰ç›¸ä¼¼æ£€ç´¢ï¼Œé€‚åˆä¸€èˆ¬æ€§é—®é¢˜
- hybrid: æ··åˆæ£€ç´¢ï¼Œé€‚åˆåŒ…å«ä¸“æœ‰åè¯æˆ–å…³é”®è¯çš„é—®é¢˜
- graph: å›¾æ£€ç´¢ï¼Œé€‚åˆå…³ç³»æŸ¥è¯¢ã€è·¯å¾„æŸ¥è¯¢
- fusion: èåˆæ£€ç´¢ï¼Œé€‚åˆå¤æ‚é—®é¢˜

è¯·åˆ†ææŸ¥è¯¢ç‰¹ç‚¹ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ£€ç´¢æ¨¡å¼ã€‚
è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š{{"mode": "æ£€ç´¢æ¨¡å¼", "reason": "é€‰æ‹©åŸå› "}}"""


class UltimateRAGChain:
    """
    ç»ˆæ RAG é“¾
    
    æ•´åˆ Stage 1-4 æ‰€æœ‰èƒ½åŠ›ï¼š
    - å‘é‡æ£€ç´¢ (Stage 1)
    - æ··åˆæ£€ç´¢ + é‡æ’åº (Stage 2)
    - æ™ºèƒ½è·¯ç”± + è‡ªåæ€ + å·¥å…· (Stage 3)
    - çŸ¥è¯†å›¾è°±æ£€ç´¢ (Stage 4)
    """
    
    def __init__(
        self,
        documents: Optional[List[Document]] = None,
        config: Optional[Stage4Config] = None,
        graph_name: str = "ultimate_rag",
        # åŠŸèƒ½å¼€å…³
        enable_routing: bool = True,
        enable_self_rag: bool = True,
        enable_graph_rag: bool = True,
        enable_reranking: bool = True,
        enable_compression: bool = True,
        # é‡å»ºé€‰é¡¹
        force_rebuild_graph: bool = False,
        force_rebuild_index: bool = False,
    ):
        """
        åˆå§‹åŒ–ç»ˆæ RAG é“¾
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            config: é…ç½®
            graph_name: çŸ¥è¯†å›¾è°±åç§°
            enable_routing: å¯ç”¨æ™ºèƒ½è·¯ç”±
            enable_self_rag: å¯ç”¨è‡ªåæ€
            enable_graph_rag: å¯ç”¨å›¾æ£€ç´¢
            enable_reranking: å¯ç”¨é‡æ’åº
            enable_compression: å¯ç”¨ä¸Šä¸‹æ–‡å‹ç¼©
            force_rebuild_graph: å¼ºåˆ¶é‡å»ºçŸ¥è¯†å›¾è°±
            force_rebuild_index: å¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•
        """
        self.config = config or get_stage4_config()
        self.base_config = get_config()
        self.graph_name = graph_name
        
        # åŠŸèƒ½å¼€å…³
        self.enable_routing = enable_routing
        self.enable_self_rag = enable_self_rag
        self.enable_graph_rag = enable_graph_rag
        self.enable_reranking = enable_reranking
        self.enable_compression = enable_compression
        
        # åˆå§‹åŒ– LLM
        self._llm = self._create_llm()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components(
            documents,
            force_rebuild_graph,
            force_rebuild_index,
        )
        
        logger.info(
            f"ğŸ¯ ç»ˆæ RAG é“¾åˆå§‹åŒ–å®Œæˆ:\n"
            f"   - è·¯ç”±: {enable_routing}\n"
            f"   - è‡ªåæ€: {enable_self_rag}\n"
            f"   - å›¾æ£€ç´¢: {enable_graph_rag}\n"
            f"   - é‡æ’åº: {enable_reranking}\n"
            f"   - ä¸Šä¸‹æ–‡å‹ç¼©: {enable_compression}"
        )
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.7,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _init_components(
        self,
        documents: Optional[List[Document]],
        force_rebuild_graph: bool,
        force_rebuild_index: bool,
    ):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        
        # 1. å‘é‡å­˜å‚¨
        self._vectorstore_manager = VectorStoreManager(
            self.base_config,
            collection_name=f"ultimate_rag_{self.graph_name}"
        )
        
        if force_rebuild_index:
            self._vectorstore_manager.clear()
            logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºå‘é‡å­˜å‚¨")
        
        # 2. æ··åˆæ£€ç´¢å™¨
        self._hybrid_retriever = None
        if documents:
            self._hybrid_retriever = HybridRetriever(
                documents=documents,
                vectorstore_manager=self._vectorstore_manager,
                config=self.base_config,
            )
        
        # 3. é‡æ’åºå™¨
        if self.enable_reranking:
            try:
                self._reranker = Reranker(config=self.base_config)
            except Exception:
                self._reranker = SimpleReranker(self.base_config)
        else:
            self._reranker = None
        
        # 4. è·¯ç”±å™¨
        if self.enable_routing:
            from src.stage_3.config import get_stage3_config
            stage3_config = get_stage3_config()
            self._router = QueryRouter(stage3_config)
        else:
            self._router = None
        
        # 5. è‡ªåæ€
        if self.enable_self_rag:
            from src.stage_3.config import get_stage3_config
            stage3_config = get_stage3_config()
            self._self_rag = SelfRAG(stage3_config)
        else:
            self._self_rag = None
        
        # 6. ä¸Šä¸‹æ–‡å‹ç¼©
        if self.enable_compression:
            try:
                self._compressor = ContextCompressor(self.config)
            except Exception:
                self._compressor = KeywordBasedCompressor(self.config)
        else:
            self._compressor = None
        
        # 7. GraphRAGï¼ˆæ ¸å¿ƒï¼‰
        if self.enable_graph_rag:
            self._graph_rag = GraphRAGChain(
                documents=documents,
                config=self.config,
                graph_name=self.graph_name,
                force_rebuild=force_rebuild_graph,
            )
        else:
            self._graph_rag = None
    
    def _analyze_query(self, query: str) -> RetrievalMode:
        """
        åˆ†ææŸ¥è¯¢ï¼Œç¡®å®šæ£€ç´¢æ¨¡å¼
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            RetrievalMode: æ£€ç´¢æ¨¡å¼
        """
        # å…³ç³»ç›¸å…³çš„å…³é”®è¯
        relation_keywords = [
            "å…³ç³»", "è”ç³»", "ä¹‹é—´", "å’Œ...çš„", "ä¸...çš„",
            "è·¯å¾„", "è¿æ¥", "å…³è”", "æ¶‰åŠåˆ°", "ç›¸å…³çš„",
        ]
        
        # å…¨å±€æ€§é—®é¢˜å…³é”®è¯
        global_keywords = [
            "æ€»ç»“", "æ¦‚æ‹¬", "å…¨éƒ¨", "æ‰€æœ‰", "æ•´ä½“",
            "è¶‹åŠ¿", "æ¨¡å¼", "åˆ†å¸ƒ", "ç»Ÿè®¡",
        ]
        
        query_lower = query.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…³ç³»æŸ¥è¯¢
        for kw in relation_keywords:
            if kw in query_lower:
                return RetrievalMode.GRAPH
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…¨å±€æ€§é—®é¢˜
        for kw in global_keywords:
            if kw in query_lower:
                return RetrievalMode.GRAPH
        
        # é»˜è®¤ä½¿ç”¨èåˆæ£€ç´¢
        return RetrievalMode.FUSION
    
    def _vector_retrieve(
        self,
        query: str,
        k: int,
    ) -> List[Document]:
        """å‘é‡æ£€ç´¢"""
        if self._hybrid_retriever:
            docs = self._hybrid_retriever.search(query, k=k * 2)
        else:
            docs = self._vectorstore_manager.similarity_search(query, k=k * 2)
        
        # é‡æ’åº
        if self._reranker and len(docs) > k:
            reranked = self._reranker.rerank(query, docs, top_k=k)
            docs = [doc for doc, _ in reranked]
        else:
            docs = docs[:k]
        
        return docs
    
    def _graph_retrieve(
        self,
        query: str,
        k: int,
    ) -> GraphRetrievalResult:
        """å›¾æ£€ç´¢"""
        if not self._graph_rag:
            return GraphRetrievalResult(query=query, context="å›¾æ£€ç´¢æœªå¯ç”¨")
        
        return self._graph_rag._graph_retriever.retrieve(query, top_k=k)
    
    def _fusion_retrieve(
        self,
        query: str,
        k: int,
    ) -> tuple[List[Document], GraphRetrievalResult]:
        """èåˆæ£€ç´¢"""
        # å‘é‡æ£€ç´¢
        docs = self._vector_retrieve(query, k)
        
        # å›¾æ£€ç´¢
        graph_result = self._graph_retrieve(query, k)
        
        return docs, graph_result
    
    def _format_docs(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£"""
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("file_name", "æœªçŸ¥æ¥æº")
            content = doc.page_content.strip()[:500]
            formatted.append(f"[æ–‡æ¡£ {i}] (æ¥æº: {source})\n{content}")
        
        return "\n\n---\n\n".join(formatted)
    
    def ask(
        self,
        question: str,
        retrieval_mode: RetrievalMode = RetrievalMode.AUTO,
        top_k: int = None,
    ) -> UltimateRAGResult:
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieval_mode: æ£€ç´¢æ¨¡å¼
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            UltimateRAGResult: å¤„ç†ç»“æœ
        """
        top_k = top_k or self.config.top_k
        
        logger.info(f"â“ ç»ˆæ RAG é—®ç­”: {question}")
        
        reasoning_chain = []
        
        # 1. è·¯ç”±å†³ç­–ï¼ˆå¯é€‰ï¼‰
        route_type = None
        if self.enable_routing and self._router:
            decision = self._router.route(question)
            route_type = decision.route_type
            reasoning_chain.append(
                f"è·¯ç”±å†³ç­–: {route_type.value} (ç½®ä¿¡åº¦: {decision.confidence:.2f})"
            )
            
            # å¦‚æœè·¯ç”±åˆ°å·¥å…·ï¼Œä½¿ç”¨ Stage 3 çš„å¤„ç†
            if route_type in [RouteType.CALCULATOR, RouteType.CODE_EXECUTION]:
                from src.stage_3.agentic_rag_chain import AgenticRAGChain
                # å§”æ‰˜ç»™ Stage 3 å¤„ç†
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨çŸ¥è¯†åº“
                pass
        
        # 2. ç¡®å®šæ£€ç´¢æ¨¡å¼
        if retrieval_mode == RetrievalMode.AUTO:
            retrieval_mode = self._analyze_query(question)
        reasoning_chain.append(f"æ£€ç´¢æ¨¡å¼: {retrieval_mode.value}")
        
        # 3. æ‰§è¡Œæ£€ç´¢
        docs = []
        graph_result = None
        
        if retrieval_mode == RetrievalMode.VECTOR:
            docs = self._vector_retrieve(question, top_k)
            reasoning_chain.append(f"å‘é‡æ£€ç´¢: {len(docs)} ä¸ªæ–‡æ¡£")
            
        elif retrieval_mode == RetrievalMode.HYBRID:
            docs = self._vector_retrieve(question, top_k)
            reasoning_chain.append(f"æ··åˆæ£€ç´¢: {len(docs)} ä¸ªæ–‡æ¡£")
            
        elif retrieval_mode == RetrievalMode.GRAPH:
            graph_result = self._graph_retrieve(question, top_k)
            reasoning_chain.append(
                f"å›¾æ£€ç´¢: {len(graph_result.matched_entities)} ä¸ªå®ä½“"
            )
            
        elif retrieval_mode == RetrievalMode.FUSION:
            docs, graph_result = self._fusion_retrieve(question, top_k)
            reasoning_chain.append(
                f"èåˆæ£€ç´¢: {len(docs)} ä¸ªæ–‡æ¡£, "
                f"{len(graph_result.matched_entities) if graph_result else 0} ä¸ªå®ä½“"
            )
        
        # 4. ä¸Šä¸‹æ–‡å‹ç¼©
        if self._compressor and docs:
            docs = self._compressor.compress_documents(question, docs)
            reasoning_chain.append("æ‰§è¡Œä¸Šä¸‹æ–‡å‹ç¼©")
        
        # 5. ç”Ÿæˆç­”æ¡ˆ
        doc_context = self._format_docs(docs) if docs else "æ— ç›¸å…³æ–‡æ¡£"
        graph_context = graph_result.context if graph_result else "æ— å›¾è°±ä¿¡æ¯"
        
        prompt = ChatPromptTemplate.from_template(ULTIMATE_RAG_PROMPT)
        
        response = self._llm.invoke(
            prompt.format(
                graph_context=graph_context,
                doc_context=doc_context,
                question=question,
            )
        )
        
        answer = response.content
        
        # 6. è‡ªåæ€ï¼ˆå¯é€‰ï¼‰
        iterations = 1
        quality_score = 0.0
        
        if self.enable_self_rag and self._self_rag:
            combined_context = f"{graph_context}\n\n{doc_context}"
            quality_eval = self._self_rag.evaluate_answer_quality(
                question, combined_context, answer
            )
            quality_score = quality_eval.score
            reasoning_chain.append(
                f"è´¨é‡è¯„ä¼°: {quality_eval.grade.value} (åˆ†æ•°: {quality_score:.2f})"
            )
        
        # 7. æ„å»ºç»“æœ
        retrieved_docs = [
            {
                "content": doc.page_content[:200] + "...",
                "source": doc.metadata.get("file_name", "æœªçŸ¥"),
            }
            for doc in docs
        ]
        
        graph_entities = []
        graph_relations = []
        if graph_result:
            graph_entities = [e.to_dict() for e in graph_result.matched_entities]
            if graph_result.subgraph:
                graph_relations = [
                    r.to_dict() for r in graph_result.subgraph.edges[:10]
                ]
        
        # æ¥æº
        sources = retrieved_docs.copy()
        for entity in (graph_result.matched_entities if graph_result else []):
            sources.append({
                "content": entity.description,
                "source": f"å®ä½“: {entity.name}",
                "type": "entity",
            })
        
        reasoning_chain.append("å¤„ç†å®Œæˆ")
        
        result = UltimateRAGResult(
            answer=answer,
            query=question,
            retrieval_mode=retrieval_mode,
            route_type=route_type,
            retrieved_docs=retrieved_docs,
            graph_entities=graph_entities,
            graph_relations=graph_relations,
            confidence=graph_result.confidence if graph_result else 0.5,
            quality_score=quality_score,
            iterations=iterations,
            sources=sources,
            reasoning_chain=reasoning_chain,
        )
        
        logger.info(f"âœ… ç»ˆæ RAG é—®ç­”å®Œæˆ")
        
        return result
    
    def ask_simple(self, question: str) -> str:
        """ç®€åŒ–ç‰ˆé—®ç­”"""
        result = self.ask(question)
        return result.answer
    
    def build_knowledge_graph(self, documents: List[Document]):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        if self._graph_rag:
            self._graph_rag.build_knowledge_graph(documents)
    
    def generate_global_summary(self) -> str:
        """ç”Ÿæˆå…¨å±€æ‘˜è¦"""
        if self._graph_rag:
            return self._graph_rag.generate_global_summary()
        return "GraphRAG æœªå¯ç”¨"
    
    def get_entity_info(self, entity_name: str) -> Optional[Dict[str, Any]]:
        """è·å–å®ä½“ä¿¡æ¯"""
        if self._graph_rag:
            return self._graph_rag.get_entity_info(entity_name)
        return None
    
    def find_path(
        self,
        source: str,
        target: str,
    ) -> Optional[List[Dict[str, Any]]]:
        """æŸ¥æ‰¾å®ä½“é—´è·¯å¾„"""
        if self._graph_rag:
            return self._graph_rag.find_path(source, target)
        return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "components": {
                "routing": self.enable_routing,
                "self_rag": self.enable_self_rag,
                "graph_rag": self.enable_graph_rag,
                "reranking": self.enable_reranking,
                "compression": self.enable_compression,
            }
        }
        
        if self._graph_rag:
            stats["knowledge_graph"] = self._graph_rag.get_statistics()
        
        return stats

