#!/usr/bin/env python3
"""
æœ¬åœ° LLM å¾®è°ƒä¸»å…¥å£

æä¾›å‘½ä»¤è¡Œç•Œé¢è¿›è¡Œï¼š
- ä¸€é”®å¾®è°ƒ
- æ¨¡å‹æµ‹è¯•
- äº¤äº’å¼å¯¹è¯

Usage:
    # ä¸€é”®å¾®è°ƒ
    python -m src.stage_4.finetune_main train
    
    # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯
    python -m src.stage_4.finetune_main chat
    
    # æµ‹è¯•æ¨¡å‹æ•ˆæœ
    python -m src.stage_4.finetune_main test
    
    # æŸ¥çœ‹å¸®åŠ©
    python -m src.stage_4.finetune_main --help
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional

from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def setup_logging(verbose: bool = False):
    """é…ç½®æ—¥å¿—"""
    logger.remove()
    level = "DEBUG" if verbose else "INFO"
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level=level,
    )


def train_model(
    data_path: str,
    model: str,
    output_dir: str,
    epochs: int,
    lora_rank: int,
    batch_size: int,
    max_seq_length: int,
    device: str,
    learning_rate: float,
):
    """æ‰§è¡Œæ¨¡å‹å¾®è°ƒ"""
    from src.stage_4.fine_tuning import LocalLLMFineTuner, LocalFineTuneConfig
    
    print("\n" + "=" * 60)
    print("ğŸš€ æœ¬åœ° LLM å¾®è°ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(data_path):
        print(f"\nâŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæˆ–æŒ‡å®šæ­£ç¡®çš„æ•°æ®è·¯å¾„")
        return False
    
    # åˆ›å»ºé…ç½®
    config = LocalFineTuneConfig(
        base_model=model,
        output_dir=output_dir,
        lora_rank=lora_rank,
        epochs=epochs,
        batch_size=batch_size,
        max_seq_length=max_seq_length,
        device=device,
        learning_rate=learning_rate,
    )
    
    # æ˜¾ç¤ºé…ç½®
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   - åŸºç¡€æ¨¡å‹: {config.base_model}")
    print(f"   - è®­ç»ƒæ•°æ®: {data_path}")
    print(f"   - è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"   - LoRA rank: {config.lora_rank}")
    print(f"   - è®­ç»ƒè½®æ•°: {config.epochs}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   - åºåˆ—é•¿åº¦: {config.max_seq_length}")
    print(f"   - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"   - è®­ç»ƒè®¾å¤‡: {config.device}")
    
    # ç¡®è®¤å¼€å§‹
    print("\nâš ï¸ è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¡®ä¿ï¼š")
    print("   1. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆçº¦ 5GBï¼‰")
    print("   2. å†…å­˜å……è¶³ï¼ˆå»ºè®® 16GB+ï¼‰")
    print("   3. é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 1-3GBï¼‰")
    
    confirm = input("\næ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆè®­ç»ƒ")
        return False
    
    # å¼€å§‹å¾®è°ƒ
    try:
        finetuner = LocalLLMFineTuner(config)
        result = finetuner.run_full_pipeline(data_path)
        
        print("\n" + "=" * 60)
        print("âœ… å¾®è°ƒå®Œæˆ!")
        print("=" * 60)
        print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {result['adapter_path']}")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {result['metrics'].get('train_loss', 'N/A')}")
        print(f"\nğŸ§ª æµ‹è¯•å›å¤: {result['test_response'][:200]}...")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   è¿è¡Œ `python -m src.stage_4.finetune_main chat` å¼€å§‹å¯¹è¯")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å¾®è°ƒå¤±è´¥: {e}")
        logger.exception("å¾®è°ƒè¿‡ç¨‹å‡ºé”™")
        return False


def interactive_chat(
    model_path: Optional[str],
    base_model: str,
    device: str,
):
    """äº¤äº’å¼å¯¹è¯"""
    from src.stage_4.fine_tuning import LocalLLMFineTuner, LocalFineTuneConfig
    
    print("\n" + "=" * 60)
    print("ğŸ’¬ å¾®è°ƒæ¨¡å‹å¯¹è¯")
    print("=" * 60)
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    adapter_path = model_path or "./models/finetuned_llm/adapter"
    
    if not os.path.exists(adapter_path):
        print(f"\nâŒ æ¨¡å‹é€‚é…å™¨ä¸å­˜åœ¨: {adapter_path}")
        print("è¯·å…ˆè¿è¡Œå¾®è°ƒï¼Œæˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return
    
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹é€‚é…å™¨: {adapter_path}")
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {base_model}")
    
    # åˆ›å»ºå¾®è°ƒå™¨å¹¶åŠ è½½æ¨¡å‹
    config = LocalFineTuneConfig(
        base_model=base_model,
        device=device,
    )
    
    try:
        finetuner = LocalLLMFineTuner(config)
        finetuner.load_adapter(adapter_path, base_model)
        
        print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
        print(f"   - è®¾å¤‡: {finetuner.device}")
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # äº¤äº’å¼å¯¹è¯
    print("\n" + "-" * 60)
    print("å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼Œ'clear' æ¸…å±ï¼‰")
    print("-" * 60)
    
    while True:
        try:
            question = input("\nğŸ§‘ ä½ : ").strip()
            
            if not question:
                continue
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if question.lower() == 'clear':
                os.system('clear' if os.name != 'nt' else 'cls')
                continue
            
            # ç”Ÿæˆå›å¤
            print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            response = finetuner.chat(
                question,
                max_new_tokens=512,
                temperature=0.7,
            )
            print(response)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")


def test_model(
    model_path: Optional[str],
    base_model: str,
    device: str,
    test_questions: Optional[list] = None,
):
    """æµ‹è¯•æ¨¡å‹æ•ˆæœ"""
    from src.stage_4.fine_tuning import LocalLLMFineTuner, LocalFineTuneConfig
    
    print("\n" + "=" * 60)
    print("ğŸ§ª æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    adapter_path = model_path or "./models/finetuned_llm/adapter"
    
    if not os.path.exists(adapter_path):
        print(f"\nâŒ æ¨¡å‹é€‚é…å™¨ä¸å­˜åœ¨: {adapter_path}")
        return
    
    # é»˜è®¤æµ‹è¯•é—®é¢˜
    if test_questions is None:
        test_questions = [
            "å°ç±³å…¬å¸æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿåˆ›å§‹äººæ˜¯è°ï¼Ÿ",
            "å°ç±³çš„ä¸»è¦ä¸šåŠ¡åŒ…æ‹¬å“ªäº›ï¼Ÿ",
            "å°ç±³åœ¨2024å¹´çš„è¥ä¸šé¢æ˜¯å¤šå°‘ï¼Ÿ",
            "å°ç±³å…¬å¸çš„æ€»éƒ¨åœ¨å“ªé‡Œï¼Ÿ",
            "å°ç±³ä»€ä¹ˆæ—¶å€™å®£å¸ƒè¿›å…¥é€ è½¦é¢†åŸŸçš„ï¼Ÿ",
        ]
    
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {adapter_path}")
    
    # åŠ è½½æ¨¡å‹
    config = LocalFineTuneConfig(
        base_model=base_model,
        device=device,
    )
    
    try:
        finetuner = LocalLLMFineTuner(config)
        finetuner.load_adapter(adapter_path, base_model)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•é—®ç­”
    print("\n" + "-" * 60)
    print("ğŸ“ æµ‹è¯•ç»“æœ")
    print("-" * 60)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nã€é—®é¢˜ {i}ã€‘{question}")
        
        try:
            response = finetuner.chat(
                question,
                max_new_tokens=256,
                temperature=0.7,
            )
            print(f"ã€å›ç­”ã€‘{response}")
        except Exception as e:
            print(f"ã€é”™è¯¯ã€‘{e}")
        
        print("-" * 40)
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")


def compare_models(
    base_model: str,
    adapter_path: str,
    device: str,
):
    """å¯¹æ¯”å¾®è°ƒå‰åæ•ˆæœ"""
    from src.stage_4.fine_tuning import LocalLLMFineTuner, LocalFineTuneConfig
    
    print("\n" + "=" * 60)
    print("ğŸ“Š å¾®è°ƒå‰åæ•ˆæœå¯¹æ¯”")
    print("=" * 60)
    
    test_questions = [
        "å°ç±³å…¬å¸æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ",
        "å°ç±³çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ",
    ]
    
    config = LocalFineTuneConfig(
        base_model=base_model,
        device=device,
    )
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½åŸå§‹æ¨¡å‹...")
    finetuner_base = LocalLLMFineTuner(config)
    finetuner_base.load_model()
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹
    print("\nğŸ“¦ åŠ è½½å¾®è°ƒæ¨¡å‹...")
    finetuner_tuned = LocalLLMFineTuner(config)
    finetuner_tuned.load_adapter(adapter_path, base_model)
    
    # å¯¹æ¯”æµ‹è¯•
    print("\n" + "-" * 60)
    
    for question in test_questions:
        print(f"\nã€é—®é¢˜ã€‘{question}")
        print()
        
        # åŸå§‹æ¨¡å‹å›ç­”
        try:
            response_base = finetuner_base.chat(question, max_new_tokens=200)
            print(f"ğŸ”¹ åŸå§‹æ¨¡å‹: {response_base[:300]}...")
        except Exception as e:
            print(f"ğŸ”¹ åŸå§‹æ¨¡å‹: é”™è¯¯ - {e}")
        
        print()
        
        # å¾®è°ƒæ¨¡å‹å›ç­”
        try:
            response_tuned = finetuner_tuned.chat(question, max_new_tokens=200)
            print(f"ğŸ”¸ å¾®è°ƒæ¨¡å‹: {response_tuned[:300]}...")
        except Exception as e:
            print(f"ğŸ”¸ å¾®è°ƒæ¨¡å‹: é”™è¯¯ - {e}")
        
        print("-" * 40)


def generate_training_data(
    data_dir: str,
    output_dir: str,
    pairs_per_doc: int,
):
    """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
    from src.stage_1.document_loader import DocumentLoader
    from src.stage_1.chunker import TextChunker
    from src.stage_4.fine_tuning import LLMFineTuner
    from src.stage_4.config import get_stage4_config
    
    print("\n" + "=" * 60)
    print("ğŸ“ ç”Ÿæˆè®­ç»ƒæ•°æ®")
    print("=" * 60)
    
    # åŠ è½½æ–‡æ¡£
    print(f"\nğŸ“‚ åŠ è½½æ–‡æ¡£: {data_dir}")
    loader = DocumentLoader()
    documents = loader.load_directory(data_dir)
    
    if not documents:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
        return
    
    print(f"   æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    # åˆ†å—
    print("\nâœ‚ï¸ æ–‡æ¡£åˆ†å—...")
    chunker = TextChunker()
    chunks = chunker.split_documents(documents)
    print(f"   ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print(f"\nğŸ”„ ç”Ÿæˆ QA å¯¹ (æ¯ä¸ªæ–‡æ¡£ {pairs_per_doc} å¯¹)...")
    config = get_stage4_config()
    finetuner = LLMFineTuner(config=config, output_dir=output_dir)
    
    qa_pairs = finetuner.generate_qa_pairs(chunks, pairs_per_doc=pairs_per_doc)
    
    print(f"   ç”Ÿæˆäº† {len(qa_pairs)} ä¸ª QA å¯¹")
    
    # ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜æ•°æ®...")
    finetuner.export_json()  # Alpaca æ ¼å¼
    finetuner.export_jsonl()  # OpenAI æ ¼å¼
    
    print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   - train_alpaca.json (Alpaca æ ¼å¼)")
    print(f"   - train_openai.jsonl (OpenAI æ ¼å¼)")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="UltimateRAG æœ¬åœ° LLM å¾®è°ƒå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®è¿›è¡Œå¾®è°ƒ
  python -m src.stage_4.finetune_main train
  
  # æŒ‡å®šæ¨¡å‹å’Œå‚æ•°
  python -m src.stage_4.finetune_main train --model Qwen/Qwen2.5-0.5B-Instruct --epochs 3
  
  # ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯
  python -m src.stage_4.finetune_main chat
  
  # æµ‹è¯•æ¨¡å‹æ•ˆæœ
  python -m src.stage_4.finetune_main test
  
  # ç”Ÿæˆè®­ç»ƒæ•°æ®
  python -m src.stage_4.finetune_main generate
  
  # å¯¹æ¯”å¾®è°ƒå‰åæ•ˆæœ
  python -m src.stage_4.finetune_main compare
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # ============== train å‘½ä»¤ ==============
    train_parser = subparsers.add_parser("train", help="å¾®è°ƒæ¨¡å‹")
    train_parser.add_argument(
        "--data", "-d",
        default="./data/finetune/train_alpaca.json",
        help="è®­ç»ƒæ•°æ®è·¯å¾„ (é»˜è®¤: ./data/finetune/train_alpaca.json)"
    )
    train_parser.add_argument(
        "--model", "-m",
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="åŸºç¡€æ¨¡å‹ (é»˜è®¤: Qwen/Qwen2.5-0.5B-Instruct)"
    )
    train_parser.add_argument(
        "--output", "-o",
        default="./models/finetuned_llm",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./models/finetuned_llm)"
    )
    train_parser.add_argument(
        "--epochs", "-e",
        type=int,
        default=3,
        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 3)"
    )
    train_parser.add_argument(
        "--lora-rank", "-r",
        type=int,
        default=8,
        help="LoRA rank (é»˜è®¤: 8)"
    )
    train_parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=1,
        help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1)"
    )
    train_parser.add_argument(
        "--max-seq-length",
        type=int,
        default=512,
        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 512)"
    )
    train_parser.add_argument(
        "--learning-rate", "-lr",
        type=float,
        default=2e-4,
        help="å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)"
    )
    train_parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda", "mps"],
        help="è®­ç»ƒè®¾å¤‡ (é»˜è®¤: auto)"
    )
    
    # ============== chat å‘½ä»¤ ==============
    chat_parser = subparsers.add_parser("chat", help="ä¸å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯")
    chat_parser.add_argument(
        "--model-path", "-p",
        default=None,
        help="æ¨¡å‹é€‚é…å™¨è·¯å¾„ (é»˜è®¤: ./models/finetuned_llm/adapter)"
    )
    chat_parser.add_argument(
        "--base-model", "-m",
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="åŸºç¡€æ¨¡å‹ (é»˜è®¤: Qwen/Qwen2.5-0.5B-Instruct)"
    )
    chat_parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda", "mps"],
        help="æ¨ç†è®¾å¤‡ (é»˜è®¤: auto)"
    )
    
    # ============== test å‘½ä»¤ ==============
    test_parser = subparsers.add_parser("test", help="æµ‹è¯•æ¨¡å‹æ•ˆæœ")
    test_parser.add_argument(
        "--model-path", "-p",
        default=None,
        help="æ¨¡å‹é€‚é…å™¨è·¯å¾„"
    )
    test_parser.add_argument(
        "--base-model", "-m",
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="åŸºç¡€æ¨¡å‹"
    )
    test_parser.add_argument(
        "--device",
        default="auto",
        help="æ¨ç†è®¾å¤‡"
    )
    
    # ============== compare å‘½ä»¤ ==============
    compare_parser = subparsers.add_parser("compare", help="å¯¹æ¯”å¾®è°ƒå‰åæ•ˆæœ")
    compare_parser.add_argument(
        "--model-path", "-p",
        default="./models/finetuned_llm/adapter",
        help="å¾®è°ƒæ¨¡å‹é€‚é…å™¨è·¯å¾„"
    )
    compare_parser.add_argument(
        "--base-model", "-m",
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="åŸºç¡€æ¨¡å‹"
    )
    compare_parser.add_argument(
        "--device",
        default="auto",
        help="æ¨ç†è®¾å¤‡"
    )
    
    # ============== generate å‘½ä»¤ ==============
    gen_parser = subparsers.add_parser("generate", help="ç”Ÿæˆè®­ç»ƒæ•°æ®")
    gen_parser.add_argument(
        "--data-dir", "-d",
        default="./data/documents",
        help="æ–‡æ¡£ç›®å½• (é»˜è®¤: ./data/documents)"
    )
    gen_parser.add_argument(
        "--output", "-o",
        default="./data/finetune",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./data/finetune)"
    )
    gen_parser.add_argument(
        "--pairs-per-doc",
        type=int,
        default=5,
        help="æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„ QA å¯¹æ•°é‡ (é»˜è®¤: 5)"
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    setup_logging(getattr(args, 'verbose', False))
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == "train":
        train_model(
            data_path=args.data,
            model=args.model,
            output_dir=args.output,
            epochs=args.epochs,
            lora_rank=args.lora_rank,
            batch_size=args.batch_size,
            max_seq_length=args.max_seq_length,
            device=args.device,
            learning_rate=args.learning_rate,
        )
    
    elif args.command == "chat":
        interactive_chat(
            model_path=args.model_path,
            base_model=args.base_model,
            device=args.device,
        )
    
    elif args.command == "test":
        test_model(
            model_path=args.model_path,
            base_model=args.base_model,
            device=args.device,
        )
    
    elif args.command == "compare":
        compare_models(
            base_model=args.base_model,
            adapter_path=args.model_path,
            device=args.device,
        )
    
    elif args.command == "generate":
        generate_training_data(
            data_dir=args.data_dir,
            output_dir=args.output,
            pairs_per_doc=args.pairs_per_doc,
        )
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

