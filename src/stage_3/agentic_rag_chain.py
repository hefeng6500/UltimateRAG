"""
Agentic RAG Chain

æ•´åˆæ‰€æœ‰ Stage 3 ç»„ä»¶ï¼Œå®ç°ä»£ç†å¼ RAGï¼š
- æ™ºèƒ½è·¯ç”±
- è‡ªåæ€æ£€ç´¢
- å·¥å…·è°ƒç”¨
- çˆ¶å­ç´¢å¼•
- ä¸Šä¸‹æ–‡å‹ç¼©
"""

from typing import List, Optional, Dict, Any, Literal
from dataclasses import dataclass, field
from enum import Enum
from loguru import logger

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

from src.stage_1.config import Config, get_config
from src.stage_1.vectorstore import VectorStoreManager
from src.stage_2.hybrid_retriever import HybridRetriever
from src.stage_2.query_rewriter import QueryRewriter
from src.stage_2.reranker import Reranker, SimpleReranker

from .config import Stage3Config, get_stage3_config
from .router import QueryRouter, RouteType, RouteDecision
from .self_rag import SelfRAG, QualityGrade
from .tools import WebSearchTool, CalculatorTool, CodeExecutorTool, ToolResult
from .parent_child_retriever import ParentChildRetriever
from .context_compressor import ContextCompressor, KeywordBasedCompressor


@dataclass
class AgenticRAGResult:
    """Agentic RAG å¤„ç†ç»“æœ"""
    answer: str
    route_type: RouteType
    confidence: float
    sources: List[Dict[str, Any]] = field(default_factory=list)
    tool_used: Optional[str] = None
    tool_result: Optional[str] = None
    iterations: int = 1
    quality_score: float = 0.0
    reasoning_chain: List[str] = field(default_factory=list)


# RAG ç”Ÿæˆæç¤ºè¯
AGENTIC_RAG_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œå…·å¤‡æ·±åº¦åˆ†æå’Œç»¼åˆèƒ½åŠ›ã€‚

è¯·åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®æ€§ï¼šåªåŸºäºæä¾›çš„ä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ 
2. å®Œæ•´æ€§ï¼šå°½å¯èƒ½å…¨é¢åœ°å›ç­”é—®é¢˜
3. æ¡ç†æ€§ï¼šä½¿ç”¨æ¸…æ™°çš„ç»“æ„ç»„ç»‡ç­”æ¡ˆ
4. è¯šå®æ€§ï¼šå¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜

åŒæ—¶ï¼Œå…è®¸ä½ ï¼š
- å½’çº³æ€»ç»“
- é£æ ¼åˆ†æ
- è§‚ç‚¹é€‰æ‹©
- å¥å­è¯„ä»·
- åˆç†æ¨æ–­ï¼ˆå¿…é¡»åŸºäºæ–‡æ¡£ç»™å‡ºçš„å†…å®¹ï¼‰

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

# ç›´æ¥å›ç­”æç¤ºè¯ï¼ˆä¸éœ€è¦æ£€ç´¢ï¼‰
DIRECT_ANSWER_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""


class AgenticRAGChain:
    """
    ä»£ç†å¼ RAG é“¾
    
    æ•´åˆæ™ºèƒ½è·¯ç”±ã€è‡ªåæ€ã€å·¥å…·è°ƒç”¨ã€çˆ¶å­ç´¢å¼•ã€ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œ
    å®ç°æ›´æ™ºèƒ½çš„é—®ç­”ç³»ç»Ÿã€‚
    """
    
    def __init__(
        self,
        documents: List[Document] = None,
        vectorstore_manager: Optional[VectorStoreManager] = None,
        config: Optional[Stage3Config] = None,
        # åŠŸèƒ½å¼€å…³
        enable_routing: bool = True,
        enable_self_rag: bool = True,
        enable_tools: bool = True,
        enable_parent_child: bool = True,
        enable_compression: bool = True,
        enable_reranking: bool = True,
        force_reindex: bool = False
    ):
        """
        åˆå§‹åŒ– Agentic RAG é“¾
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            vectorstore_manager: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            config: Stage3 é…ç½®
            enable_routing: å¯ç”¨æ™ºèƒ½è·¯ç”±
            enable_self_rag: å¯ç”¨è‡ªåæ€
            enable_tools: å¯ç”¨å·¥å…·è°ƒç”¨
            enable_parent_child: å¯ç”¨çˆ¶å­ç´¢å¼•
            enable_compression: å¯ç”¨ä¸Šä¸‹æ–‡å‹ç¼©
            enable_reranking: å¯ç”¨é‡æ’åº
        """
        self.config = config or get_stage3_config()
        self.base_config = get_config()
        
        # åŠŸèƒ½å¼€å…³
        self.enable_routing = enable_routing
        self.enable_self_rag = enable_self_rag
        self.enable_tools = enable_tools
        self.enable_parent_child = enable_parent_child
        self.enable_compression = enable_compression
        self.enable_reranking = enable_reranking
        self.force_reindex = force_reindex
        
        # åˆå§‹åŒ– LLM
        self._llm = self._create_llm()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components(documents, vectorstore_manager)
        
        logger.info(
            f"ğŸ¤– Agentic RAG é“¾åˆå§‹åŒ–å®Œæˆ:\n"
            f"   - è·¯ç”±: {enable_routing}\n"
            f"   - è‡ªåæ€: {enable_self_rag}\n"
            f"   - å·¥å…·: {enable_tools}\n"
            f"   - çˆ¶å­ç´¢å¼•: {enable_parent_child}\n"
            f"   - ä¸Šä¸‹æ–‡å‹ç¼©: {enable_compression}\n"
            f"   - é‡æ’åº: {enable_reranking}"
        )
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.7,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def _init_components(
        self,
        documents: List[Document],
        vectorstore_manager: Optional[VectorStoreManager]
    ):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        
        # 1. å‘é‡å­˜å‚¨
        self.vectorstore_manager = vectorstore_manager or VectorStoreManager(
            self.base_config,
            collection_name="agentic_rag"
        )
        
        # 2. è·¯ç”±å™¨
        if self.enable_routing:
            self._router = QueryRouter(self.config)
        
        # 3. è‡ªåæ€ RAG
        if self.enable_self_rag:
            self._self_rag = SelfRAG(self.config)
        
        # 4. å·¥å…·
        if self.enable_tools:
            self._tools = {
                "web_search": WebSearchTool(),
                "calculator": CalculatorTool(),
                "code_executor": CodeExecutorTool(timeout=self.config.code_executor_timeout)
            }
        
        # 5. çˆ¶å­ç´¢å¼•æ£€ç´¢å™¨
        if self.enable_parent_child and documents:
            pc_vectorstore_manager = VectorStoreManager(
                self.base_config,
                collection_name="parent_child_index"
            )
            
            # å¦‚æœå¼ºåˆ¶é‡å»ºç´¢å¼•ï¼Œå…ˆæ¸…ç©º
            if self.force_reindex:
                pc_vectorstore_manager.clear()
                logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºçˆ¶å­ç´¢å¼•å‘é‡åº“")
            
            self._parent_child_retriever = ParentChildRetriever(
                config=self.config,
                vectorstore_manager=pc_vectorstore_manager
            )
            # åªæœ‰å½“å‘é‡åº“ä¸ºç©ºæˆ–å¼ºåˆ¶é‡å»ºæ—¶æ‰æ·»åŠ æ–‡æ¡£ï¼Œé¿å…é‡å¤æ·»åŠ 
            if self.force_reindex or pc_vectorstore_manager.vectorstore._collection.count() == 0:
                self._parent_child_retriever.add_documents(documents)
            else:
                logger.info(
                    f"ğŸ“¦ ä½¿ç”¨å·²æœ‰çˆ¶å­ç´¢å¼•: "
                    f"{pc_vectorstore_manager.vectorstore._collection.count()} ä¸ªå­å—"
                )
        else:
            self._parent_child_retriever = None
        
        # 6. æ··åˆæ£€ç´¢å™¨
        if documents:
            self._hybrid_retriever = HybridRetriever(
                documents=documents,
                vectorstore_manager=self.vectorstore_manager,
                config=self.base_config
            )
        else:
            self._hybrid_retriever = None
        
        # 7. æŸ¥è¯¢é‡å†™å™¨
        self._query_rewriter = QueryRewriter(self.base_config)
        
        # 8. é‡æ’åºå™¨
        if self.enable_reranking:
            try:
                self._reranker = Reranker(config=self.base_config)
            except Exception:
                self._reranker = SimpleReranker(self.base_config)
        
        # 9. ä¸Šä¸‹æ–‡å‹ç¼©å™¨
        if self.enable_compression:
            try:
                self._compressor = ContextCompressor(self.config)
            except Exception:
                self._compressor = KeywordBasedCompressor(self.config)
    
    def _format_docs(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£"""
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("file_name", "æœªçŸ¥æ¥æº")
            content = doc.page_content.strip()
            formatted.append(f"[æ–‡æ¡£ {i}] (æ¥æº: {source})\n{content}")
        
        return "\n\n---\n\n".join(formatted)
    
    def _retrieve(self, query: str, k: int = None) -> List[Document]:
        """
        æ‰§è¡Œæ£€ç´¢
        
        æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨çˆ¶å­ç´¢å¼•æˆ–æ™®é€šæ£€ç´¢ã€‚
        """
        k = k or self.config.top_k
        
        if self.enable_parent_child and self._parent_child_retriever:
            docs = self._parent_child_retriever.retrieve(query, k=k * 2)
        elif self._hybrid_retriever:
            docs = self._hybrid_retriever.search(query, k=k * 2)
        else:
            docs = self.vectorstore_manager.similarity_search(query, k=k * 2)
        
        # é‡æ’åº
        if self.enable_reranking and len(docs) > k:
            reranked = self._reranker.rerank(query, docs, top_k=k)
            docs = [doc for doc, _ in reranked]
        else:
            docs = docs[:k]
        
        return docs
    
    def _handle_knowledge_base(
        self,
        question: str,
        reasoning_chain: List[str]
    ) -> tuple[str, List[Document], int]:
        """
        å¤„ç†çŸ¥è¯†åº“æ£€ç´¢
        
        Returns:
            Tuple[str, List[Document], int]: (ç­”æ¡ˆ, æ–‡æ¡£åˆ—è¡¨, è¿­ä»£æ¬¡æ•°)
        """
        reasoning_chain.append("æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢...")
        
        # æŸ¥è¯¢é‡å†™
        queries = self._query_rewriter.generate_multi_queries(question)
        reasoning_chain.append(f"ç”Ÿæˆ {len(queries)} ä¸ªæŸ¥è¯¢å˜ä½“")
        
        # æ‰§è¡Œæ£€ç´¢
        all_docs = []
        seen_contents = set()
        
        for q in queries:
            docs = self._retrieve(q)
            for doc in docs:
                content_hash = hash(doc.page_content)
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    all_docs.append(doc)
        
        reasoning_chain.append(f"æ£€ç´¢åˆ° {len(all_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")
        
        # ä¸Šä¸‹æ–‡å‹ç¼©
        if self.enable_compression and all_docs:
            all_docs = self._compressor.compress_documents(question, all_docs)
            reasoning_chain.append("æ‰§è¡Œä¸Šä¸‹æ–‡å‹ç¼©")
        
        # æˆªå– top-k
        docs = all_docs[:self.config.top_k]
        
        if not docs:
            return "æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], 1
        
        # ç”Ÿæˆç­”æ¡ˆ
        context = self._format_docs(docs)
        prompt = ChatPromptTemplate.from_template(AGENTIC_RAG_PROMPT)
        
        response = self._llm.invoke(
            prompt.format(context=context, question=question)
        )
        answer = response.content
        
        # è‡ªåæ€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        iterations = 1
        if self.enable_self_rag:
            quality_eval = self._self_rag.evaluate_answer_quality(
                question, context, answer
            )
            reasoning_chain.append(
                f"ç­”æ¡ˆè´¨é‡è¯„ä¼°: {quality_eval.grade.value} "
                f"(åˆ†æ•°: {quality_eval.score:.2f})"
            )
            
            # å¦‚æœè´¨é‡ä¸è¶³ï¼Œå°è¯•è¿­ä»£
            while (
                self._self_rag.should_iterate(quality_eval) and 
                iterations < self.config.self_rag_max_iterations
            ):
                iterations += 1
                reasoning_chain.append(f"å¼€å§‹ç¬¬ {iterations} è½®è¿­ä»£...")
                
                # ä¼˜åŒ–æŸ¥è¯¢
                refined_query = self._self_rag.refine_query(
                    question,
                    context[:500],
                    quality_eval.missing_info
                )
                
                # é‡æ–°æ£€ç´¢
                new_docs = self._retrieve(refined_query)
                
                # åˆå¹¶æ–‡æ¡£
                for doc in new_docs:
                    content_hash = hash(doc.page_content)
                    if content_hash not in seen_contents:
                        seen_contents.add(content_hash)
                        docs.append(doc)
                
                # é‡æ–°ç”Ÿæˆç­”æ¡ˆ
                context = self._format_docs(docs[:self.config.top_k + iterations])
                response = self._llm.invoke(
                    prompt.format(context=context, question=question)
                )
                answer = response.content
                
                # é‡æ–°è¯„ä¼°
                quality_eval = self._self_rag.evaluate_answer_quality(
                    question, context, answer
                )
                reasoning_chain.append(
                    f"ç¬¬ {iterations} è½®è´¨é‡: {quality_eval.grade.value}"
                )
        
        return answer, docs, iterations
    
    def _handle_web_search(
        self,
        question: str,
        reasoning_chain: List[str]
    ) -> tuple[str, Optional[ToolResult]]:
        """å¤„ç† Web æœç´¢"""
        reasoning_chain.append("æ‰§è¡Œ Web æœç´¢...")
        
        tool = self._tools.get("web_search")
        if not tool:
            return "Web æœç´¢åŠŸèƒ½æœªå¯ç”¨ã€‚", None
        
        result = tool.run(question)
        
        if not result.is_success:
            reasoning_chain.append(f"æœç´¢å¤±è´¥: {result.error}")
            return f"æœç´¢å¤±è´¥: {result.error}", result
        
        reasoning_chain.append(f"æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {result.metadata.get('count', 0)} ä¸ªç»“æœ")
        
        # åŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        prompt = ChatPromptTemplate.from_template(
            """åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ï¼š

æœç´¢ç»“æœï¼š
{search_results}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç»¼åˆæœç´¢ç»“æœï¼Œç”¨ä¸­æ–‡ç»™å‡ºå‡†ç¡®çš„å›ç­”ï¼š"""
        )
        
        response = self._llm.invoke(
            prompt.format(search_results=result.output, question=question)
        )
        
        return response.content, result
    
    def _handle_calculator(
        self,
        question: str,
        reasoning_chain: List[str]
    ) -> tuple[str, Optional[ToolResult]]:
        """å¤„ç†è®¡ç®—è¯·æ±‚"""
        reasoning_chain.append("æ‰§è¡Œæ•°å­¦è®¡ç®—...")
        
        tool = self._tools.get("calculator")
        if not tool:
            return "è®¡ç®—å™¨åŠŸèƒ½æœªå¯ç”¨ã€‚", None
        
        # å°è¯•ä»é—®é¢˜ä¸­æå–è¡¨è¾¾å¼
        # ç®€å•æ–¹æ³•ï¼šè®© LLM æå–
        extract_prompt = ChatPromptTemplate.from_template(
            """ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–éœ€è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼ã€‚
åªè¾“å‡ºä¸€ä¸ªå¯ä»¥ç›´æ¥è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚

é—®é¢˜ï¼š{question}

æ•°å­¦è¡¨è¾¾å¼ï¼š"""
        )
        
        response = self._llm.invoke(extract_prompt.format(question=question))
        expression = response.content.strip()
        
        reasoning_chain.append(f"æå–è¡¨è¾¾å¼: {expression}")
        
        result = tool.run(expression)
        
        if result.is_success:
            reasoning_chain.append(f"è®¡ç®—æˆåŠŸ")
            return result.output, result
        else:
            reasoning_chain.append(f"è®¡ç®—å¤±è´¥: {result.error}")
            return f"è®¡ç®—å¤±è´¥: {result.error}", result
    
    def _handle_code_execution(
        self,
        question: str,
        reasoning_chain: List[str]
    ) -> tuple[str, Optional[ToolResult]]:
        """å¤„ç†ä»£ç æ‰§è¡Œ"""
        reasoning_chain.append("ç”Ÿæˆå¹¶æ‰§è¡Œä»£ç ...")
        
        tool = self._tools.get("code_executor")
        if not tool:
            return "ä»£ç æ‰§è¡ŒåŠŸèƒ½æœªå¯ç”¨ã€‚", None
        
        # è®© LLM ç”Ÿæˆä»£ç 
        code_prompt = ChatPromptTemplate.from_template(
            """æ ¹æ®ç”¨æˆ·éœ€æ±‚ç¼–å†™ Python ä»£ç ã€‚
ä»£ç åº”è¯¥ç®€æ´ã€å®‰å…¨ï¼Œå¹¶é€šè¿‡ print è¾“å‡ºç»“æœã€‚
åªè¾“å‡ºä»£ç ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

å¯ç”¨çš„æ¨¡å—ï¼šmath, random, datetime, collections, itertools, statistics, json, re

ç”¨æˆ·éœ€æ±‚ï¼š{question}

Python ä»£ç ï¼š"""
        )
        
        response = self._llm.invoke(code_prompt.format(question=question))
        code = response.content.strip()
        
        # æ¸…ç†ä»£ç ï¼ˆç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼‰
        if code.startswith("```"):
            lines = code.split("\n")
            code = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])
        
        reasoning_chain.append(f"ç”Ÿæˆä»£ç : {code[:100]}...")
        
        result = tool.run(code)
        
        if result.is_success:
            reasoning_chain.append("ä»£ç æ‰§è¡ŒæˆåŠŸ")
            
            # æ•´åˆç­”æ¡ˆ
            answer_prompt = ChatPromptTemplate.from_template(
                """ç”¨æˆ·é—®é¢˜ï¼š{question}

æ‰§è¡Œçš„ä»£ç ï¼š
```python
{code}
```

æ‰§è¡Œç»“æœï¼š
{result}

è¯·åŸºäºä»£ç æ‰§è¡Œç»“æœï¼Œç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"""
            )
            
            final_response = self._llm.invoke(
                answer_prompt.format(
                    question=question,
                    code=code,
                    result=result.output
                )
            )
            
            return final_response.content, result
        else:
            reasoning_chain.append(f"ä»£ç æ‰§è¡Œå¤±è´¥: {result.error}")
            return f"ä»£ç æ‰§è¡Œå¤±è´¥: {result.error}", result
    
    def _handle_direct_answer(
        self,
        question: str,
        reasoning_chain: List[str]
    ) -> str:
        """å¤„ç†ç›´æ¥å›ç­”"""
        reasoning_chain.append("ç”Ÿæˆç›´æ¥å›ç­”...")
        
        prompt = ChatPromptTemplate.from_template(DIRECT_ANSWER_PROMPT)
        response = self._llm.invoke(prompt.format(question=question))
        
        return response.content
    
    def ask(self, question: str) -> AgenticRAGResult:
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            AgenticRAGResult: å¤„ç†ç»“æœ
        """
        logger.info(f"â“ æ”¶åˆ°é—®é¢˜: {question}")
        
        reasoning_chain = []
        
        # 1. è·¯ç”±å†³ç­–
        if self.enable_routing:
            decision = self._router.route(question)
            route_type = decision.route_type
            confidence = decision.confidence
            reasoning_chain.append(f"è·¯ç”±å†³ç­–: {route_type.value} (ç½®ä¿¡åº¦: {confidence:.2f})")
            reasoning_chain.append(f"åŸå› : {decision.reasoning}")
        else:
            # é»˜è®¤ä½¿ç”¨çŸ¥è¯†åº“
            route_type = RouteType.KNOWLEDGE_BASE
            confidence = 1.0
            reasoning_chain.append("è·¯ç”±å·²ç¦ç”¨ï¼Œä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢")
        
        # 2. æ ¹æ®è·¯ç”±æ‰§è¡Œç›¸åº”å¤„ç†
        answer = ""
        sources = []
        tool_used = None
        tool_result = None
        iterations = 1
        quality_score = 0.0
        
        if route_type == RouteType.KNOWLEDGE_BASE:
            answer, docs, iterations = self._handle_knowledge_base(
                question, reasoning_chain
            )
            sources = [
                {
                    "content": doc.page_content[:200] + "...",
                    "source": doc.metadata.get("file_name", "æœªçŸ¥"),
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
            
        elif route_type == RouteType.WEB_SEARCH:
            if self.enable_tools:
                answer, tool_res = self._handle_web_search(question, reasoning_chain)
                tool_used = "web_search"
                tool_result = tool_res.output if tool_res else None
            else:
                answer = "Web æœç´¢åŠŸèƒ½æœªå¯ç”¨ã€‚"
                reasoning_chain.append("å·¥å…·å·²ç¦ç”¨")
                
        elif route_type == RouteType.CALCULATOR:
            if self.enable_tools:
                answer, tool_res = self._handle_calculator(question, reasoning_chain)
                tool_used = "calculator"
                tool_result = tool_res.output if tool_res else None
            else:
                answer = "è®¡ç®—å™¨åŠŸèƒ½æœªå¯ç”¨ã€‚"
                reasoning_chain.append("å·¥å…·å·²ç¦ç”¨")
                
        elif route_type == RouteType.CODE_EXECUTION:
            if self.enable_tools:
                answer, tool_res = self._handle_code_execution(question, reasoning_chain)
                tool_used = "code_executor"
                tool_result = tool_res.output if tool_res else None
            else:
                answer = "ä»£ç æ‰§è¡ŒåŠŸèƒ½æœªå¯ç”¨ã€‚"
                reasoning_chain.append("å·¥å…·å·²ç¦ç”¨")
                
        elif route_type == RouteType.DIRECT_ANSWER:
            answer = self._handle_direct_answer(question, reasoning_chain)
        
        reasoning_chain.append("å¤„ç†å®Œæˆ")
        
        result = AgenticRAGResult(
            answer=answer,
            route_type=route_type,
            confidence=confidence,
            sources=sources,
            tool_used=tool_used,
            tool_result=tool_result,
            iterations=iterations,
            quality_score=quality_score,
            reasoning_chain=reasoning_chain
        )
        
        logger.info(f"âœ… é—®ç­”å®Œæˆ: {route_type.value}")
        
        return result
    
    def ask_simple(self, question: str) -> str:
        """
        ç®€åŒ–ç‰ˆé—®ç­”ï¼ˆåªè¿”å›ç­”æ¡ˆï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: ç­”æ¡ˆ
        """
        result = self.ask(question)
        return result.answer

