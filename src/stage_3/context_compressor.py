"""
ä¸Šä¸‹æ–‡å‹ç¼©æ¨¡å—

å®ç° Context Compressionï¼š
- ä»æ£€ç´¢ç»“æœä¸­æå–æœ€ç›¸å…³çš„å¥å­
- åˆ é™¤æ— å…³å†…å®¹ï¼ŒèŠ‚çœ Token
- ä¿ç•™æ ¸å¿ƒä¿¡æ¯
"""

from typing import List, Optional, Tuple
from loguru import logger

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from .config import Stage3Config, get_stage3_config


class ExtractedContent(BaseModel):
    """æå–çš„ç›¸å…³å†…å®¹"""
    relevant_sentences: List[str] = Field(
        description="ä¸é—®é¢˜æœ€ç›¸å…³çš„å¥å­åˆ—è¡¨"
    )
    relevance_score: float = Field(
        description="æ•´ä½“ç›¸å…³æ€§åˆ†æ•° 0-1",
        ge=0.0,
        le=1.0
    )
    summary: str = Field(
        description="å†…å®¹çš„ç®€çŸ­æ‘˜è¦"
    )


# å†…å®¹æå–æç¤ºè¯
EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æå–ä¸“å®¶ã€‚
ä»ç»™å®šæ–‡æ¡£ä¸­æå–ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„å¥å­ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

æ–‡æ¡£å†…å®¹ï¼š
{document}

è¦æ±‚ï¼š
1. åªæå–ç›´æ¥ç›¸å…³çš„å¥å­
2. ä¿æŒå¥å­åŸæ ·ï¼Œä¸è¦ä¿®æ”¹
3. å¦‚æœæ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¿”å›ç©ºåˆ—è¡¨
4. ç»™å‡ºæ•´ä½“ç›¸å…³æ€§è¯„åˆ†

è¯·æå–ç›¸å…³å†…å®¹ã€‚"""

# å¿«é€Ÿå‹ç¼©æç¤ºè¯
COMPRESSION_PROMPT = """å‹ç¼©ä»¥ä¸‹æ–‡æœ¬ï¼Œåªä¿ç•™ä¸é—®é¢˜ç›¸å…³çš„æ ¸å¿ƒä¿¡æ¯ï¼š

é—®é¢˜ï¼š{question}

åŸæ–‡ï¼š
{document}

å‹ç¼©åçš„æ–‡æœ¬ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼Œåˆ é™¤æ— å…³å†…å®¹ï¼‰ï¼š"""


class ContextCompressor:
    """
    ä¸Šä¸‹æ–‡å‹ç¼©å™¨
    
    ä»æ£€ç´¢ç»“æœä¸­æå–æœ€ç›¸å…³çš„å†…å®¹ï¼Œå‹ç¼©ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
    """
    
    def __init__(self, config: Optional[Stage3Config] = None):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡å‹ç¼©å™¨
        
        Args:
            config: Stage3 é…ç½®
        """
        self.config = config or get_stage3_config()
        self._llm = self._create_llm()
        self._extractor = self._llm.with_structured_output(ExtractedContent)
        
        logger.info("ğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_llm(self) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.1,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return ChatOpenAI(**kwargs)
    
    def extract_relevant_content(
        self,
        question: str,
        document: Document
    ) -> Tuple[List[str], float]:
        """
        ä»æ–‡æ¡£ä¸­æå–ç›¸å…³å†…å®¹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document: å¾…å¤„ç†æ–‡æ¡£
            
        Returns:
            Tuple[List[str], float]: (ç›¸å…³å¥å­åˆ—è¡¨, ç›¸å…³æ€§åˆ†æ•°)
        """
        prompt = ChatPromptTemplate.from_template(EXTRACTION_PROMPT)
        
        try:
            result = self._extractor.invoke(
                prompt.format(
                    question=question,
                    document=document.page_content[:2000]
                )
            )
            return result.relevant_sentences, result.relevance_score
        except Exception as e:
            logger.warning(f"âš ï¸ å†…å®¹æå–å¤±è´¥: {e}")
            return [], 0.5
    
    def compress_document(
        self,
        question: str,
        document: Document
    ) -> Document:
        """
        å‹ç¼©å•ä¸ªæ–‡æ¡£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document: å¾…å‹ç¼©æ–‡æ¡£
            
        Returns:
            Document: å‹ç¼©åçš„æ–‡æ¡£
        """
        prompt = ChatPromptTemplate.from_template(COMPRESSION_PROMPT)
        
        try:
            response = self._llm.invoke(
                prompt.format(
                    question=question,
                    document=document.page_content[:2000]
                )
            )
            
            compressed_content = response.content.strip()
            
            # åˆ›å»ºæ–°æ–‡æ¡£ä¿ç•™å…ƒæ•°æ®
            compressed_doc = Document(
                page_content=compressed_content,
                metadata={
                    **document.metadata,
                    "compressed": True,
                    "original_length": len(document.page_content),
                    "compressed_length": len(compressed_content)
                }
            )
            
            return compressed_doc
        except Exception as e:
            logger.warning(f"âš ï¸ æ–‡æ¡£å‹ç¼©å¤±è´¥: {e}")
            return document
    
    def compress_documents(
        self,
        question: str,
        documents: List[Document],
        method: str = "extract"
    ) -> List[Document]:
        """
        æ‰¹é‡å‹ç¼©æ–‡æ¡£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            documents: æ–‡æ¡£åˆ—è¡¨
            method: å‹ç¼©æ–¹æ³• ("extract" æå–ç›¸å…³å¥å­, "compress" LLM å‹ç¼©)
            
        Returns:
            List[Document]: å‹ç¼©åçš„æ–‡æ¡£åˆ—è¡¨
        """
        compressed_docs = []
        total_original = 0
        total_compressed = 0
        
        for doc in documents:
            total_original += len(doc.page_content)
            
            if method == "extract":
                sentences, score = self.extract_relevant_content(question, doc)
                
                if sentences and score >= self.config.min_relevant_score:
                    compressed_content = "\n".join(sentences)
                    compressed_doc = Document(
                        page_content=compressed_content,
                        metadata={
                            **doc.metadata,
                            "compressed": True,
                            "relevance_score": score,
                            "original_length": len(doc.page_content),
                            "compressed_length": len(compressed_content)
                        }
                    )
                    compressed_docs.append(compressed_doc)
                    total_compressed += len(compressed_content)
                elif score >= self.config.min_relevant_score:
                    # æ²¡æœ‰æå–åˆ°å¥å­ä½†ç›¸å…³æ€§å¤Ÿï¼Œä¿ç•™åŸæ–‡æ¡£
                    compressed_docs.append(doc)
                    total_compressed += len(doc.page_content)
            else:
                compressed_doc = self.compress_document(question, doc)
                compressed_docs.append(compressed_doc)
                total_compressed += len(compressed_doc.page_content)
        
        compression_ratio = (
            1 - total_compressed / total_original 
            if total_original > 0 else 0
        )
        
        logger.info(
            f"ğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©å®Œæˆ: "
            f"{len(documents)} -> {len(compressed_docs)} ä¸ªæ–‡æ¡£, "
            f"å‹ç¼©æ¯”: {compression_ratio:.1%}"
        )
        
        return compressed_docs


class KeywordBasedCompressor:
    """
    åŸºäºå…³é”®è¯çš„å¿«é€Ÿå‹ç¼©å™¨ï¼ˆä¸ä¾èµ– LLMï¼‰
    
    ä½¿ç”¨å…³é”®è¯åŒ¹é…å¿«é€Ÿè¿‡æ»¤ä¸ç›¸å…³çš„å¥å­ã€‚
    """
    
    def __init__(self, config: Optional[Stage3Config] = None):
        """åˆå§‹åŒ–å…³é”®è¯å‹ç¼©å™¨"""
        self.config = config or get_stage3_config()
        logger.info("ğŸ”‘ å…³é”®è¯å‹ç¼©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _extract_keywords(self, text: str) -> set:
        """
        æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            set: å…³é”®è¯é›†åˆ
        """
        import re
        
        # ç®€å•åˆ†è¯
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stopwords = {
            'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'è¿™', 'é‚£', 'æœ‰', 'ä¸ª', 'ä¸º',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from'
        }
        
        return {w for w in words if w not in stopwords and len(w) > 1}
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        åˆ†å‰²å¥å­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        import re
        
        # æŒ‰ä¸­è‹±æ–‡å¥å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def compress(
        self,
        question: str,
        document: Document,
        min_score: float = 0.3
    ) -> Document:
        """
        åŸºäºå…³é”®è¯å‹ç¼©æ–‡æ¡£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            document: å¾…å‹ç¼©æ–‡æ¡£
            min_score: æœ€å°åŒ¹é…åˆ†æ•°
            
        Returns:
            Document: å‹ç¼©åçš„æ–‡æ¡£
        """
        # æå–é—®é¢˜å…³é”®è¯
        question_keywords = self._extract_keywords(question)
        
        if not question_keywords:
            return document
        
        # åˆ†å‰²å¥å­
        sentences = self._split_sentences(document.page_content)
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§
        relevant_sentences = []
        for sentence in sentences:
            sentence_keywords = self._extract_keywords(sentence)
            
            if not sentence_keywords:
                continue
            
            # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
            intersection = len(question_keywords & sentence_keywords)
            union = len(question_keywords | sentence_keywords)
            score = intersection / union if union > 0 else 0
            
            if score >= min_score:
                relevant_sentences.append(sentence)
        
        if not relevant_sentences:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å¥å­ï¼Œä¿ç•™åŸæ–‡æ¡£
            return document
        
        compressed_content = "ã€‚".join(relevant_sentences)
        
        return Document(
            page_content=compressed_content,
            metadata={
                **document.metadata,
                "compressed": True,
                "method": "keyword",
                "original_length": len(document.page_content),
                "compressed_length": len(compressed_content)
            }
        )
    
    def compress_documents(
        self,
        question: str,
        documents: List[Document],
        min_score: float = 0.3
    ) -> List[Document]:
        """
        æ‰¹é‡å‹ç¼©æ–‡æ¡£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            documents: æ–‡æ¡£åˆ—è¡¨
            min_score: æœ€å°åŒ¹é…åˆ†æ•°
            
        Returns:
            List[Document]: å‹ç¼©åçš„æ–‡æ¡£åˆ—è¡¨
        """
        compressed = [
            self.compress(question, doc, min_score)
            for doc in documents
        ]
        
        total_original = sum(len(d.page_content) for d in documents)
        total_compressed = sum(len(d.page_content) for d in compressed)
        ratio = 1 - total_compressed / total_original if total_original > 0 else 0
        
        logger.info(f"ğŸ”‘ å…³é”®è¯å‹ç¼©å®Œæˆ: å‹ç¼©æ¯” {ratio:.1%}")
        
        return compressed

