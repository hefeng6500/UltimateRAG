#!/usr/bin/env python3
"""
æ–‡æ¡£é‡å»ºç´¢å¼•è„šæœ¬

å¯å¤ç”¨è„šæœ¬ï¼Œç”¨äºé‡æ–°å¯¹ documents ç›®å½•ä¸‹çš„æ–‡æ¡£è¿›è¡Œåˆ‡å—å’Œ embeddingã€‚
æ”¯æŒåœ¨ Stage 1ã€2ã€3 ä¸­ä½¿ç”¨ã€‚

ä½¿ç”¨æ–¹å¼:
    # ä½¿ç”¨é»˜è®¤å‚æ•°é‡å»ºæ‰€æœ‰ stage çš„ç´¢å¼•
    python -m src.rebuild_index
    
    # ä»…é‡å»º stage 1 çš„ç´¢å¼•
    python -m src.rebuild_index --stage 1
    
    # ä½¿ç”¨è¯­ä¹‰åˆ†å—é‡å»º stage 2 çš„ç´¢å¼•
    python -m src.rebuild_index --stage 2 --semantic
    
    # é‡å»ºæ‰€æœ‰ stage ä¸”ä½¿ç”¨è¯­ä¹‰åˆ†å—
    python -m src.rebuild_index --stage all --semantic
    
    # æŒ‡å®šæ–‡æ¡£ç›®å½•å’Œ chunk ç›®å½•
    python -m src.rebuild_index --data ./data/documents --chunks-dir ./data/chunks
"""

import sys
import shutil
import argparse
from pathlib import Path
from typing import List, Optional, Literal
from loguru import logger

from langchain_core.documents import Document

# å¯¼å…¥å„ä¸ªç»„ä»¶
from src.stage_1.config import Config, get_config
from src.stage_1.document_loader import DocumentLoader
from src.stage_1.chunker import TextChunker
from src.stage_1.vectorstore import VectorStoreManager


def setup_logger():
    """é…ç½® loguru æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>",
        level="INFO",
        colorize=True
    )


def clear_chunks_cache(chunks_dir: str = "./data/chunks"):
    """
    æ¸…é™¤ chunk ç¼“å­˜ç›®å½•
    
    Args:
        chunks_dir: chunk ç¼“å­˜ç›®å½•è·¯å¾„
    """
    chunks_path = Path(chunks_dir)
    if chunks_path.exists():
        shutil.rmtree(chunks_path)
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤ chunk ç¼“å­˜: {chunks_path}")
    chunks_path.mkdir(parents=True, exist_ok=True)


def clear_vectorstore(
    config: Config,
    collection_name: str
):
    """
    æ¸…é™¤å‘é‡åº“ä¸­çš„æŒ‡å®šé›†åˆ
    
    Args:
        config: é…ç½®å¯¹è±¡
        collection_name: é›†åˆåç§°
    """
    try:
        manager = VectorStoreManager(config, collection_name=collection_name)
        manager.clear()
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤å‘é‡åº“é›†åˆ: {collection_name}")
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…é™¤å‘é‡åº“é›†åˆå¤±è´¥ ({collection_name}): {e}")


def load_documents(data_path: str) -> List[Document]:
    """
    åŠ è½½æ–‡æ¡£
    
    Args:
        data_path: æ–‡æ¡£è·¯å¾„
        
    Returns:
        List[Document]: æ–‡æ¡£åˆ—è¡¨
    """
    loader = DocumentLoader()
    documents = loader.load(data_path)
    
    if not documents:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return []
    
    logger.info(f"ğŸ“„ å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def chunk_documents_fixed(
    documents: List[Document],
    config: Config,
    chunks_dir: str = "./data/chunks"
) -> List[Document]:
    """
    ä½¿ç”¨å›ºå®šå¤§å°åˆ†å—
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        chunks_dir: chunk ç¼“å­˜ç›®å½•
        
    Returns:
        List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
    """
    chunker = TextChunker(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        chunks_dir=chunks_dir
    )
    
    chunks = chunker.split_documents(
        documents,
        use_cache=False,  # å¼ºåˆ¶é‡æ–°åˆ†å—
        force_resplit=True
    )
    
    logger.info(f"âœ‚ï¸ å›ºå®šåˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªå—")
    return chunks


def chunk_documents_semantic(
    documents: List[Document],
    config: Config,
    chunks_dir: str = "./data/chunks"
) -> List[Document]:
    """
    ä½¿ç”¨è¯­ä¹‰åˆ†å—
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        chunks_dir: chunk ç¼“å­˜ç›®å½•
        
    Returns:
        List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
    """
    try:
        from src.stage_2.semantic_chunker import SemanticChunker
        
        chunker = SemanticChunker(
            config=config,
            chunks_dir=chunks_dir
        )
        
        chunks = chunker.split_documents(
            documents,
            use_cache=False,  # å¼ºåˆ¶é‡æ–°åˆ†å—
            force_resplit=True
        )
        
        logger.info(f"ğŸ§  è¯­ä¹‰åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªå—")
        return chunks
    except ImportError:
        logger.error("âŒ è¯­ä¹‰åˆ†å—éœ€è¦ Stage 2 çš„æ¨¡å—æ”¯æŒ")
        raise


def enrich_metadata(documents: List[Document]) -> List[Document]:
    """
    ä¸ºæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®ï¼ˆStage 2/3 åŠŸèƒ½ï¼‰
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        List[Document]: å¢å¼ºå…ƒæ•°æ®åçš„æ–‡æ¡£åˆ—è¡¨
    """
    try:
        from src.stage_2.metadata_extractor import MetadataExtractor
        
        extractor = MetadataExtractor()
        enriched = extractor.enrich_documents(documents)
        logger.info("ğŸ“‹ å…ƒæ•°æ®æå–å®Œæˆ")
        return enriched
    except ImportError:
        logger.warning("âš ï¸ å…ƒæ•°æ®æå–æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡")
        return documents


def rebuild_index(
    stage: Literal["1", "2", "3", "all"],
    data_path: str = "./data/documents",
    chunks_dir: str = "./data/chunks",
    use_semantic: bool = False,
    use_metadata: bool = True,
    clear_cache: bool = True
):
    """
    é‡å»ºæ–‡æ¡£ç´¢å¼•
    
    Args:
        stage: ç›®æ ‡ stage ("1", "2", "3" æˆ– "all")
        data_path: æ–‡æ¡£è·¯å¾„
        chunks_dir: chunk ç¼“å­˜ç›®å½•
        use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†å—
        use_metadata: æ˜¯å¦æå–å…ƒæ•°æ®
        clear_cache: æ˜¯å¦æ¸…é™¤æ—§ç¼“å­˜
    """
    # åŠ è½½é…ç½®
    config = get_config()
    
    if not config.validate():
        logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        return False
    
    # ç¡®å®šè¦å¤„ç†çš„ stage
    stages = ["1", "2", "3"] if stage == "all" else [stage]
    
    # Stage ä¸ collection åç§°çš„æ˜ å°„
    stage_collections = {
        "1": "rag_documents",
        "2": "advanced_rag",
        "3": "agentic_rag"
    }
    
    # 1. æ¸…é™¤ç¼“å­˜
    if clear_cache:
        logger.info("=" * 50)
        logger.info("ğŸ§¹ æ¸…é™¤æ—§æ•°æ®...")
        logger.info("=" * 50)
        
        # æ¸…é™¤ chunk ç¼“å­˜
        clear_chunks_cache(chunks_dir)
        
        # æ¸…é™¤å¯¹åº” stage çš„å‘é‡åº“
        for s in stages:
            collection_name = stage_collections[s]
            clear_vectorstore(config, collection_name)
    
    # 2. åŠ è½½æ–‡æ¡£
    logger.info("=" * 50)
    logger.info("ğŸ“„ åŠ è½½æ–‡æ¡£...")
    logger.info("=" * 50)
    
    documents = load_documents(data_path)
    if not documents:
        return False
    
    # 3. å…ƒæ•°æ®æå–ï¼ˆå¯é€‰ï¼‰
    if use_metadata and stage in ["2", "3", "all"]:
        logger.info("=" * 50)
        logger.info("ğŸ“‹ æå–å…ƒæ•°æ®...")
        logger.info("=" * 50)
        documents = enrich_metadata(documents)
    
    # 4. åˆ†å—
    logger.info("=" * 50)
    if use_semantic:
        logger.info("ğŸ§  è¯­ä¹‰åˆ†å—...")
    else:
        logger.info("âœ‚ï¸ å›ºå®šåˆ†å—...")
    logger.info("=" * 50)
    
    if use_semantic:
        chunks = chunk_documents_semantic(documents, config, chunks_dir)
    else:
        chunks = chunk_documents_fixed(documents, config, chunks_dir)
    
    if not chunks:
        logger.error("âŒ åˆ†å—å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•å—")
        return False
    
    # 5. å‘é‡åŒ–å¹¶å­˜å…¥å„ stage çš„å‘é‡åº“
    logger.info("=" * 50)
    logger.info("ğŸ—„ï¸ å‘é‡åŒ–å­˜å‚¨...")
    logger.info("=" * 50)
    
    for s in stages:
        collection_name = stage_collections[s]
        logger.info(f"ğŸ“¦ Stage {s} ({collection_name})...")
        
        manager = VectorStoreManager(config, collection_name=collection_name)
        manager.add_documents(chunks)
        
        # éªŒè¯
        count = manager.vectorstore._collection.count()
        logger.info(f"âœ… Stage {s} å®Œæˆ: {count} ä¸ªå‘é‡")
    
    # 6. å®Œæˆ
    logger.info("=" * 50)
    logger.info("ğŸ‰ ç´¢å¼•é‡å»ºå®Œæˆ!")
    logger.info("=" * 50)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ“Š é‡å»ºæ‘˜è¦")
    print("=" * 50)
    print(f"  â€¢ æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"  â€¢ åˆ†å—æ•°é‡: {len(chunks)}")
    print(f"  â€¢ åˆ†å—æ–¹å¼: {'è¯­ä¹‰åˆ†å—' if use_semantic else 'å›ºå®šåˆ†å—'}")
    print(f"  â€¢ ç›®æ ‡ Stage: {', '.join(stages)}")
    print(f"  â€¢ å…ƒæ•°æ®æå–: {'æ˜¯' if use_metadata else 'å¦'}")
    print("=" * 50 + "\n")
    
    return True


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    setup_logger()
    
    parser = argparse.ArgumentParser(
        description="æ–‡æ¡£é‡å»ºç´¢å¼•è„šæœ¬ - é‡æ–°å¯¹æ–‡æ¡£è¿›è¡Œåˆ‡å—å’Œ embedding",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤å‚æ•°é‡å»ºæ‰€æœ‰ stage çš„ç´¢å¼•
  python -m src.rebuild_index
  
  # ä»…é‡å»º stage 1 çš„ç´¢å¼•
  python -m src.rebuild_index --stage 1
  
  # ä½¿ç”¨è¯­ä¹‰åˆ†å—é‡å»º stage 2 çš„ç´¢å¼•
  python -m src.rebuild_index --stage 2 --semantic
  
  # é‡å»ºæ‰€æœ‰ stage ä¸”ä½¿ç”¨è¯­ä¹‰åˆ†å—
  python -m src.rebuild_index --stage all --semantic
"""
    )
    
    parser.add_argument(
        "--stage",
        type=str,
        default="all",
        choices=["1", "2", "3", "all"],
        help="ç›®æ ‡ Stage (1, 2, 3 æˆ– all)ï¼Œé»˜è®¤: all"
    )
    
    parser.add_argument(
        "--data",
        type=str,
        default="./data/documents",
        help="æ–‡æ¡£è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰ï¼Œé»˜è®¤: ./data/documents"
    )
    
    parser.add_argument(
        "--chunks-dir",
        type=str,
        default="./data/chunks",
        help="chunk ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤: ./data/chunks"
    )
    
    parser.add_argument(
        "--semantic",
        action="store_true",
        help="ä½¿ç”¨è¯­ä¹‰åˆ†å—ï¼ˆå¦åˆ™ä½¿ç”¨å›ºå®šåˆ†å—ï¼‰"
    )
    
    parser.add_argument(
        "--no-metadata",
        action="store_true",
        help="è·³è¿‡å…ƒæ•°æ®æå–"
    )
    
    parser.add_argument(
        "--keep-cache",
        action="store_true",
        help="ä¿ç•™æ—§çš„ç¼“å­˜æ•°æ®ï¼ˆä¸æ¸…é™¤ï¼‰"
    )
    
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("ğŸ”„ æ–‡æ¡£é‡å»ºç´¢å¼•å·¥å…·")
    print("=" * 60 + "\n")
    
    # ç¡®è®¤æ“ä½œ
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return
    
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {data_path.absolute()}")
    print(f"ğŸ“¦ ç›®æ ‡ Stage: {args.stage}")
    print(f"âœ‚ï¸ åˆ†å—æ–¹å¼: {'è¯­ä¹‰åˆ†å—' if args.semantic else 'å›ºå®šåˆ†å—'}")
    print(f"ğŸ“‹ å…ƒæ•°æ®æå–: {'å¦' if args.no_metadata else 'æ˜¯'}")
    print(f"ğŸ§¹ æ¸…é™¤æ—§ç¼“å­˜: {'å¦' if args.keep_cache else 'æ˜¯'}")
    print()
    
    # æ‰§è¡Œé‡å»º
    success = rebuild_index(
        stage=args.stage,
        data_path=str(data_path),
        chunks_dir=args.chunks_dir,
        use_semantic=args.semantic,
        use_metadata=not args.no_metadata,
        clear_cache=not args.keep_cache
    )
    
    if not success:
        logger.error("âŒ ç´¢å¼•é‡å»ºå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()

