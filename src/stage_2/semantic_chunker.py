"""
è¯­ä¹‰åˆ†å—æ¨¡å—

å®ç°åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
ç›¸æ¯”å›ºå®šåˆ†å—ï¼Œè¯­ä¹‰åˆ†å—èƒ½æ›´å¥½åœ°ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚
"""

from typing import List, Optional
from loguru import logger

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings

import sys
sys.path.append("..")
from stage_1.config import Config, get_config


class SemanticChunker:
    """
    è¯­ä¹‰åˆ†å—å™¨
    
    åŸºäºè¯­ä¹‰è¾¹ç•Œè¿›è¡Œåˆ†å—ï¼Œè€Œä¸æ˜¯ç®€å•çš„å­—ç¬¦æ•°åˆ‡åˆ†ã€‚
    ä½¿ç”¨å¥å­åµŒå…¥æ¥åˆ¤æ–­è¯­ä¹‰æ–­ç‚¹ã€‚
    """
    
    def __init__(
        self,
        config: Optional[Config] = None,
        breakpoint_threshold: float = 0.5,
        min_chunk_size: int = 100,
        max_chunk_size: int = 1000
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            breakpoint_threshold: è¯­ä¹‰æ–­ç‚¹é˜ˆå€¼ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šå®¹æ˜“æ–­å¼€ï¼‰
            min_chunk_size: æœ€å°å—å¤§å°
            max_chunk_size: æœ€å¤§å—å¤§å°
        """
        self.config = config or get_config()
        self.breakpoint_threshold = breakpoint_threshold
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        
        # åˆå§‹åŒ– embedding æ¨¡å‹ï¼ˆç”¨äºè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        self._embeddings = self._create_embeddings()
        
        # ä½¿ç”¨å¥å­åˆ†å‰²å™¨ä½œä¸ºé¢„å¤„ç†
        self._sentence_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=0,
            separators=["\n\n", "\n", "ã€‚", ".", "ï¼", "!", "ï¼Ÿ", "?", "ï¼›", ";"],
            is_separator_regex=False,
        )
        
        logger.info(
            f"ğŸ§  è¯­ä¹‰åˆ†å—å™¨åˆå§‹åŒ–: threshold={breakpoint_threshold}, "
            f"min={min_chunk_size}, max={max_chunk_size}"
        )
    
    def _create_embeddings(self) -> OpenAIEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        kwargs = {
            "model": self.config.embedding_model,
            "api_key": self.config.openai_api_key,
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        return OpenAIEmbeddings(**kwargs)
    
    def _compute_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))
    
    def _merge_small_chunks(self, chunks: List[str]) -> List[str]:
        """åˆå¹¶è¿‡å°çš„å—"""
        merged = []
        current = ""
        
        for chunk in chunks:
            if len(current) + len(chunk) <= self.max_chunk_size:
                current += chunk
            else:
                if current:
                    merged.append(current.strip())
                current = chunk
        
        if current:
            merged.append(current.strip())
        
        # è¿‡æ»¤å¤ªå°çš„å—
        return [c for c in merged if len(c) >= self.min_chunk_size]
    
    def split_text(self, text: str) -> List[str]:
        """
        åŸºäºè¯­ä¹‰è¾¹ç•Œåˆ†å‰²æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text or len(text) < self.min_chunk_size:
            return [text] if text else []
        
        # 1. é¦–å…ˆæŒ‰å¥å­åˆ†å‰²
        sentences = self._sentence_splitter.split_text(text)
        
        if len(sentences) <= 1:
            return [text]
        
        # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„åµŒå…¥
        try:
            embeddings = self._embeddings.embed_documents(sentences)
        except Exception as e:
            logger.warning(f"âš ï¸ åµŒå…¥è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å›ºå®šåˆ†å—: {e}")
            return self._merge_small_chunks(sentences)
        
        # 3. æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾æ–­ç‚¹
        chunks = []
        current_chunk = sentences[0]
        
        for i in range(1, len(sentences)):
            # è®¡ç®—å½“å‰å¥å­ä¸å‰ä¸€ä¸ªå¥å­çš„ç›¸ä¼¼åº¦
            similarity = self._compute_similarity(embeddings[i-1], embeddings[i])
            
            # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œä¸”å½“å‰å—è¶³å¤Ÿå¤§ï¼Œåˆ™æ–­å¼€
            if similarity < self.breakpoint_threshold and len(current_chunk) >= self.min_chunk_size:
                chunks.append(current_chunk.strip())
                current_chunk = sentences[i]
            else:
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤§å°
                if len(current_chunk) + len(sentences[i]) > self.max_chunk_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentences[i]
                else:
                    current_chunk += " " + sentences[i]
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # 4. åˆå¹¶è¿‡å°çš„å—
        result = self._merge_small_chunks(chunks)
        
        logger.info(f"âœ… è¯­ä¹‰åˆ†å—å®Œæˆ: {len(sentences)} ä¸ªå¥å­ -> {len(result)} ä¸ªè¯­ä¹‰å—")
        return result
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œè¯­ä¹‰åˆ†å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return []
        
        result = []
        for doc in documents:
            chunks = self.split_text(doc.page_content)
            for i, chunk in enumerate(chunks):
                new_doc = Document(
                    page_content=chunk,
                    metadata={
                        **doc.metadata,
                        "chunk_index": i,
                        "chunk_size": len(chunk),
                        "chunking_method": "semantic"
                    }
                )
                result.append(new_doc)
        
        logger.info(f"âœ… æ–‡æ¡£è¯­ä¹‰åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(result)} ä¸ªå—")
        return result
