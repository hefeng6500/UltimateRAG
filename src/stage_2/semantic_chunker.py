"""
è¯­ä¹‰åˆ†å—æ¨¡å—

å®ç°åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
ç›¸æ¯”å›ºå®šåˆ†å—ï¼Œè¯­ä¹‰åˆ†å—èƒ½æ›´å¥½åœ°ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚

ä¼˜åŒ–åŠŸèƒ½ï¼š
- æ”¯æŒå°†åˆ†å—ç»“æœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
- æ”¯æŒä»æœ¬åœ°æ–‡ä»¶è¯»å–å·²ä¿å­˜çš„åˆ†å—
"""

import json
import hashlib
from pathlib import Path
from typing import List, Optional
from loguru import logger

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings

from src.stage_1.config import Config, get_config
from src.stage_1.embedder import BatchedEmbeddings, ALIYUN_EMBEDDING_BATCH_SIZE


class SemanticChunker:
    """
    è¯­ä¹‰åˆ†å—å™¨
    
    åŸºäºè¯­ä¹‰è¾¹ç•Œè¿›è¡Œåˆ†å—ï¼Œè€Œä¸æ˜¯ç®€å•çš„å­—ç¬¦æ•°åˆ‡åˆ†ã€‚
    ä½¿ç”¨å¥å­åµŒå…¥æ¥åˆ¤æ–­è¯­ä¹‰æ–­ç‚¹ã€‚
    
    æ”¯æŒå°†åˆ†å—ç»“æœä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡å¯ç›´æ¥è¯»å–ã€‚
    """
    
    def __init__(
        self,
        config: Optional[Config] = None,
        breakpoint_threshold: float = 0.5,
        min_chunk_size: int = 100,
        max_chunk_size: int = 1000,
        chunks_dir: str = "./data/chunks"
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
            breakpoint_threshold: è¯­ä¹‰æ–­ç‚¹é˜ˆå€¼ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šå®¹æ˜“æ–­å¼€ï¼‰
            min_chunk_size: æœ€å°å—å¤§å°
            max_chunk_size: æœ€å¤§å—å¤§å°
            chunks_dir: åˆ†å—æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.config = config or get_config()
        self.breakpoint_threshold = breakpoint_threshold
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.chunks_dir = Path(chunks_dir)
        
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self.chunks_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– embedding æ¨¡å‹ï¼ˆç”¨äºè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        self._embeddings = self._create_embeddings()
        
        # ä½¿ç”¨å¥å­åˆ†å‰²å™¨ä½œä¸ºé¢„å¤„ç†
        self._sentence_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=0,
            separators=["\n\n", "\n", "ã€‚", ".", "ï¼", "!", "ï¼Ÿ", "?", "ï¼›", ";"],
            is_separator_regex=False,
        )
        
        logger.info(
            f"ğŸ§  è¯­ä¹‰åˆ†å—å™¨åˆå§‹åŒ–: threshold={breakpoint_threshold}, "
            f"min={min_chunk_size}, max={max_chunk_size}"
        )
    
    def _create_embeddings(self) -> Embeddings:
        """
        åˆ›å»ºåµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨ BatchedEmbeddings åŒ…è£…å™¨æ”¯æŒé˜¿é‡Œäº‘æ‰¹é‡é™åˆ¶ï¼‰
        
        Returns:
            Embeddings: å·²åŒ…è£…åˆ†æ‰¹å¤„ç†çš„åµŒå…¥æ¨¡å‹
        """
        kwargs = {
            "model": self.config.embedding_model,
            "api_key": self.config.openai_api_key,
            "check_embedding_ctx_length": False,  # é˜¿é‡Œäº‘å…¼å®¹æ¨¡å¼éœ€è¦å…³é—­
        }
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        
        base_embeddings = OpenAIEmbeddings(**kwargs)
        # ä½¿ç”¨ BatchedEmbeddings åŒ…è£…ï¼Œæ¯æ‰¹æœ€å¤š 10 æ¡ï¼ˆé˜¿é‡Œäº‘é™åˆ¶ï¼‰
        return BatchedEmbeddings(inner=base_embeddings, batch_size=ALIYUN_EMBEDDING_BATCH_SIZE)
    
    def _get_cache_path(self, documents: List[Document]) -> Path:
        """
        æ ¹æ®æ–‡æ¡£å†…å®¹ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ ‡è¯†
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.page_content.encode('utf-8'))
        
        # åŠ å…¥åˆ†å—å‚æ•°ä½œä¸ºå“ˆå¸Œçš„ä¸€éƒ¨åˆ†ï¼ˆè¯­ä¹‰åˆ†å—ç‰¹æœ‰å‚æ•°ï¼‰
        params = f"semantic_{self.breakpoint_threshold}_{self.min_chunk_size}_{self.max_chunk_size}"
        content_hash.update(params.encode('utf-8'))
        
        return self.chunks_dir / f"chunks_semantic_{content_hash.hexdigest()[:16]}.json"
    
    def _save_chunks(self, chunks: List[Document], cache_path: Path):
        """
        å°†åˆ†å—ç»“æœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            chunks: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        data = []
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "index": i,
                "content": chunk.page_content,
                "metadata": chunk.metadata,
                "size": len(chunk.page_content)
            }
            data.append(chunk_data)
            
            # åŒæ—¶ä¿å­˜å•ç‹¬çš„ chunk æ–‡ä»¶ä»¥ä¾¿æŸ¥çœ‹
            chunk_file = self.chunks_dir / f"chunk_{i:04d}.md"
            self._save_single_chunk(chunk, i, chunk_file)
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({
                "total_chunks": len(chunks),
                "chunking_method": "semantic",
                "breakpoint_threshold": self.breakpoint_threshold,
                "min_chunk_size": self.min_chunk_size,
                "max_chunk_size": self.max_chunk_size,
                "chunks": data
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è¯­ä¹‰åˆ†å—æ•°æ®å·²ä¿å­˜: {cache_path}")
    
    def _save_single_chunk(self, chunk: Document, index: int, file_path: Path):
        """
        ä¿å­˜å•ä¸ª chunk ä¸ºå¯è¯»çš„ Markdown æ–‡ä»¶
        
        Args:
            chunk: åˆ†å—æ–‡æ¡£
            index: åˆ†å—ç´¢å¼•
            file_path: æ–‡ä»¶è·¯å¾„
        """
        source = chunk.metadata.get("file_name", "æœªçŸ¥æ¥æº")
        method = chunk.metadata.get("chunking_method", "semantic")
        content = f"""# Chunk {index}

## å…ƒæ•°æ®
- **æ¥æºæ–‡ä»¶**: {source}
- **å­—ç¬¦æ•°**: {len(chunk.page_content)}
- **åˆ†å—ç´¢å¼•**: {index}
- **åˆ†å—æ–¹æ³•**: {method}

## å†…å®¹

```
{chunk.page_content}
```
"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _load_chunks(self, cache_path: Path) -> Optional[List[Document]]:
        """
        ä»æœ¬åœ°æ–‡ä»¶è¯»å–åˆ†å—ç»“æœ
        
        Args:
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            
        Returns:
            Optional[List[Document]]: åˆ†å—æ–‡æ¡£åˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            chunks = []
            for chunk_data in data["chunks"]:
                doc = Document(
                    page_content=chunk_data["content"],
                    metadata=chunk_data["metadata"]
                )
                chunks.append(doc)
            
            logger.info(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½è¯­ä¹‰åˆ†å—: {len(chunks)} ä¸ªå—")
            return chunks
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _compute_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))
    
    def _merge_small_chunks(self, chunks: List[str]) -> List[str]:
        """åˆå¹¶è¿‡å°çš„å—"""
        merged = []
        current = ""
        
        for chunk in chunks:
            if len(current) + len(chunk) <= self.max_chunk_size:
                current += chunk
            else:
                if current:
                    merged.append(current.strip())
                current = chunk
        
        if current:
            merged.append(current.strip())
        
        # è¿‡æ»¤å¤ªå°çš„å—
        return [c for c in merged if len(c) >= self.min_chunk_size]
    
    def split_text(self, text: str) -> List[str]:
        """
        åŸºäºè¯­ä¹‰è¾¹ç•Œåˆ†å‰²æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text or len(text) < self.min_chunk_size:
            return [text] if text else []
        
        # 1. é¦–å…ˆæŒ‰å¥å­åˆ†å‰²
        sentences = self._sentence_splitter.split_text(text)
        
        if len(sentences) <= 1:
            return [text]
        
        # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„åµŒå…¥
        try:
            embeddings = self._embeddings.embed_documents(sentences)
        except Exception as e:
            logger.warning(f"âš ï¸ åµŒå…¥è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å›ºå®šåˆ†å—: {e}")
            return self._merge_small_chunks(sentences)
        
        # 3. æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾æ–­ç‚¹
        chunks = []
        current_chunk = sentences[0]
        
        for i in range(1, len(sentences)):
            # è®¡ç®—å½“å‰å¥å­ä¸å‰ä¸€ä¸ªå¥å­çš„ç›¸ä¼¼åº¦
            similarity = self._compute_similarity(embeddings[i-1], embeddings[i])
            
            # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œä¸”å½“å‰å—è¶³å¤Ÿå¤§ï¼Œåˆ™æ–­å¼€
            if similarity < self.breakpoint_threshold and len(current_chunk) >= self.min_chunk_size:
                chunks.append(current_chunk.strip())
                current_chunk = sentences[i]
            else:
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤§å°
                if len(current_chunk) + len(sentences[i]) > self.max_chunk_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentences[i]
                else:
                    current_chunk += " " + sentences[i]
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # 4. åˆå¹¶è¿‡å°çš„å—
        result = self._merge_small_chunks(chunks)
        
        logger.info(f"âœ… è¯­ä¹‰åˆ†å—å®Œæˆ: {len(sentences)} ä¸ªå¥å­ -> {len(result)} ä¸ªè¯­ä¹‰å—")
        return result
    
    def split_documents(
        self, 
        documents: List[Document],
        use_cache: bool = True,
        force_resplit: bool = False
    ) -> List[Document]:
        """
        å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œè¯­ä¹‰åˆ†å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_resplit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†å—ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        cache_path = self._get_cache_path(documents)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache and not force_resplit:
            cached_chunks = self._load_chunks(cache_path)
            if cached_chunks:
                return cached_chunks
        
        # æ‰§è¡Œè¯­ä¹‰åˆ†å—
        logger.info("ğŸ”„ å¼€å§‹è¯­ä¹‰åˆ†å—å¤„ç†...")
        result = []
        for doc in documents:
            chunks = self.split_text(doc.page_content)
            for i, chunk in enumerate(chunks):
                new_doc = Document(
                    page_content=chunk,
                    metadata={
                        **doc.metadata,
                        "chunk_index": i,
                        "chunk_size": len(chunk),
                        "chunking_method": "semantic"
                    }
                )
                result.append(new_doc)
        
        logger.info(f"âœ… æ–‡æ¡£è¯­ä¹‰åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(result)} ä¸ªå—")
        
        # æ‰“å°åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        sizes = [len(c.page_content) for c in result]
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        logger.info(
            f"ğŸ“Š åˆ†å—ç»Ÿè®¡: å¹³å‡å¤§å°={avg_size:.0f}, "
            f"æœ€å°={min(sizes) if sizes else 0}, "
            f"æœ€å¤§={max(sizes) if sizes else 0}"
        )
        
        # ä¿å­˜åˆ°æœ¬åœ°
        self._save_chunks(result, cache_path)
        
        return result
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜çš„åˆ†å—æ–‡ä»¶"""
        import shutil
        if self.chunks_dir.exists():
            shutil.rmtree(self.chunks_dir)
            self.chunks_dir.mkdir(parents=True, exist_ok=True)
            logger.info("ğŸ—‘ï¸ è¯­ä¹‰åˆ†å—ç¼“å­˜å·²æ¸…ç©º")
    
    def list_cached_chunks(self) -> List[Path]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„ chunk æ–‡ä»¶
        
        Returns:
            List[Path]: chunk æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not self.chunks_dir.exists():
            return []
        
        return sorted(self.chunks_dir.glob("chunk_*.md"))
