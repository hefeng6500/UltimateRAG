"""
RAG é—®ç­”é“¾æ¨¡å—

å®ç°å®Œæ•´çš„ RAG é—®ç­”æµç¨‹ï¼š
ç”¨æˆ·æé—® -> å‘é‡æ£€ç´¢ -> æ„å»º Prompt -> LLM ç”Ÿæˆå›ç­”
"""

from typing import List, Optional, Dict, Any
from loguru import logger

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

from .config import Config, get_config
from .vectorstore import VectorStoreManager


# é»˜è®¤çš„ RAG Prompt æ¨¡æ¿
DEFAULT_RAG_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚
ä¸è¦ç¼–é€ ä¿¡æ¯ï¼ŒåªåŸºäºå‚è€ƒæ–‡æ¡£çš„å†…å®¹è¿›è¡Œå›ç­”ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

# DEFAULT_RAG_PROMPT = """
# ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œæ·±åº¦é˜…è¯»ç†è§£ã€æ€»ç»“å’Œåˆ†æçš„ä¸­æ–‡åŠ©æ‰‹ã€‚

# è¯·åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä½†å…è®¸ä½ è¿›è¡Œï¼š
# - å½’çº³æ€»ç»“
# - é£æ ¼åˆ†æ
# - è§‚ç‚¹é€‰æ‹©
# - å¥å­è¯„ä»·
# - åˆç†æ¨æ–­ï¼ˆå¿…é¡»åŸºäºæ–‡æ¡£ç»™å‡ºçš„å†…å®¹ï¼‰

# ç¦æ­¢ç¼–é€ è¶…å‡ºæ–‡æ¡£ä¸å­˜åœ¨çš„äº‹å®ä¿¡æ¯ï¼Œä½†å…è®¸åœ¨æ–‡ä¸­ä¿¡æ¯åŸºç¡€ä¸Šåšè§£é‡Šæ€§ã€åˆ†ææ€§å’Œåˆ¤æ–­æ€§çš„æ‰©å±•ã€‚

# å¦‚æœæ–‡æ¡£å®Œå…¨æ— å…³ï¼Œæ‰å›ç­”ï¼š"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

# ã€å‚è€ƒæ–‡æ¡£ã€‘
# {context}

# ã€ç”¨æˆ·é—®é¢˜ã€‘
# {question}

# è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š
# """


class RAGChain:
    """
    RAG é—®ç­”é“¾
    
    ç»„è£…æ£€ç´¢å™¨ã€Prompt å’Œ LLMï¼Œå®ç°ç«¯åˆ°ç«¯çš„é—®ç­”åŠŸèƒ½ã€‚
    """
    
    def __init__(
        self,
        vectorstore_manager: Optional[VectorStoreManager] = None,
        config: Optional[Config] = None,
        prompt_template: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– RAG é—®ç­”é“¾
        
        Args:
            vectorstore_manager: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            config: é…ç½®å¯¹è±¡
            prompt_template: è‡ªå®šä¹‰ Prompt æ¨¡æ¿
        """
        self.config = config or get_config()
        self.vectorstore_manager = vectorstore_manager or VectorStoreManager(self.config)
        
        # åˆå§‹åŒ– LLM
        self._llm = self._create_llm()
        
        # åˆ›å»º Prompt æ¨¡æ¿
        self._prompt = ChatPromptTemplate.from_template(
            prompt_template or DEFAULT_RAG_PROMPT
        )
        
        # æ„å»º RAG é“¾
        self._chain = self._build_chain()
        
        logger.info("ğŸ”— RAG é—®ç­”é“¾åˆå§‹åŒ–å®Œæˆ")
    
    def _create_llm(self) -> ChatOpenAI:
        """
        åˆ›å»º LLM å®ä¾‹
        
        Returns:
            ChatOpenAI: LangChain ChatOpenAI å®ä¾‹
        """
        kwargs = {
            "model": self.config.model_name,
            "api_key": self.config.openai_api_key,
            "temperature": 0.7,
        }
        
        # å¦‚æœè®¾ç½®äº†è‡ªå®šä¹‰ base_url
        if self.config.openai_base_url:
            kwargs["base_url"] = self.config.openai_base_url
        
        llm = ChatOpenAI(**kwargs)
        logger.info(f"ğŸ¤– LLM åˆå§‹åŒ–å®Œæˆ: {self.config.model_name}")
        
        return llm
    
    def _format_docs(self, docs: List[Document]) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–åçš„æ–‡æ¡£æ–‡æœ¬
        """
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("file_name", "æœªçŸ¥æ¥æº")
            content = doc.page_content.strip()
            formatted.append(f"[æ–‡æ¡£ {i}] (æ¥æº: {source})\n{content}")
        
        return "\n\n---\n\n".join(formatted)
    
    def _build_chain(self):
        """
        æ„å»º RAG é“¾
        
        Returns:
            Runnable: LangChain Runnable é“¾
        """
        retriever = self.vectorstore_manager.as_retriever()
        
        chain = (
            {
                "context": retriever | self._format_docs,
                "question": RunnablePassthrough()
            }
            | self._prompt
            | self._llm
            | StrOutputParser()
        )
        
        return chain
    
    def ask(self, question: str) -> str:
        """
        æé—®å¹¶è·å–å›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: LLM ç”Ÿæˆçš„å›ç­”
        """
        logger.info(f"â“ æ”¶åˆ°é—®é¢˜: {question}")
        
        try:
            answer = self._chain.invoke(question)
            logger.info(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆ")
            return answer
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            raise
    
    def ask_with_sources(self, question: str) -> Dict[str, Any]:
        """
        æé—®å¹¶è¿”å›å›ç­”åŠæ¥æºæ–‡æ¡£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Dict: åŒ…å« answer å’Œ sources çš„å­—å…¸
        """
        logger.info(f"â“ æ”¶åˆ°é—®é¢˜ (å¸¦æ¥æº): {question}")
        
        # å…ˆæ£€ç´¢æ–‡æ¡£
        docs = self.vectorstore_manager.similarity_search(question)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._format_docs(docs)
        
        # ç”Ÿæˆå›ç­”
        prompt = self._prompt.format(context=context, question=question)
        answer = self._llm.invoke(prompt).content
        
        # æ•´ç†æ¥æºä¿¡æ¯
        sources = [
            {
                "content": doc.page_content[:200] + "...",
                "source": doc.metadata.get("file_name", "æœªçŸ¥"),
                "metadata": doc.metadata
            }
            for doc in docs
        ]
        
        return {
            "answer": answer,
            "sources": sources,
            "question": question
        }
    
    def stream(self, question: str):
        """
        æµå¼ç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            str: å›ç­”çš„æ–‡æœ¬å—
        """
        logger.info(f"â“ æ”¶åˆ°é—®é¢˜ (æµå¼): {question}")
        
        for chunk in self._chain.stream(question):
            yield chunk
