"""
æ–‡æœ¬åˆ†å—æ¨¡å—

å®žçŽ° FixedSizeChunkingï¼ˆå›ºå®šå¤§å°åˆ†å—ï¼‰ç­–ç•¥ã€‚
Phase 1 ä½¿ç”¨åŸºç¡€çš„å›ºå®šåˆ†å—ï¼ŒåŽç»­é˜¶æ®µå°†å¼•å…¥è¯­ä¹‰åˆ†å—ã€‚

ä¼˜åŒ–åŠŸèƒ½ï¼š
- æ”¯æŒå°†åˆ†å—ç»“æžœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
- æ”¯æŒä»Žæœ¬åœ°æ–‡ä»¶è¯»å–å·²ä¿å­˜çš„åˆ†å—
"""

import json
import hashlib
from pathlib import Path
from typing import List, Optional
from loguru import logger

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class TextChunker:
    """
    æ–‡æœ¬åˆ†å—å™¨
    
    å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„å°å—ï¼Œä¾¿äºŽå‘é‡åŒ–å’Œæ£€ç´¢ã€‚
    ä½¿ç”¨ RecursiveCharacterTextSplitter å®žçŽ°æ™ºèƒ½åˆ†å‰²ã€‚
    
    æ”¯æŒå°†åˆ†å—ç»“æžœä¿å­˜åˆ°æœ¬åœ°ï¼Œä¸‹æ¬¡å¯ç›´æŽ¥è¯»å–ã€‚
    """
    
    def __init__(
        self,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        separators: List[str] = None,
        chunks_dir: str = "./data/chunks"
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
            separators: åˆ†å‰²ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æŽ’åº
            chunks_dir: åˆ†å—æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunks_dir = Path(chunks_dir)
        
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self.chunks_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤åˆ†å‰²ç¬¦ï¼šæŒ‰æ®µè½ -> å¥å­ -> è¯è¯­é¡ºåºåˆ†å‰²
        self.separators = separators or [
            "\n\n",  # æ®µè½
            "\n",    # æ¢è¡Œ
            "ã€‚",    # ä¸­æ–‡å¥å·
            ".",     # è‹±æ–‡å¥å·
            "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
            "!",     # è‹±æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",    # ä¸­æ–‡é—®å·
            "?",     # è‹±æ–‡é—®å·
            "ï¼›",    # ä¸­æ–‡åˆ†å·
            ";",     # è‹±æ–‡åˆ†å·
            "ï¼Œ",    # ä¸­æ–‡é€—å·
            ",",     # è‹±æ–‡é€—å·
            " ",     # ç©ºæ ¼
            "",      # å­—ç¬¦çº§åˆ«
        ]
        
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=self.separators,
            length_function=len,
            is_separator_regex=False,
        )
        
        logger.info(
            f"âœ‚ï¸ æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–: "
            f"chunk_size={chunk_size}, overlap={chunk_overlap}, "
            f"å­˜å‚¨ç›®å½•={self.chunks_dir}"
        )
    
    def _get_cache_path(self, documents: List[Document]) -> Path:
        """
        æ ¹æ®æ–‡æ¡£å†…å®¹ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            Path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ ‡è¯†
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.page_content.encode('utf-8'))
        
        # åŠ å…¥åˆ†å—å‚æ•°ä½œä¸ºå“ˆå¸Œçš„ä¸€éƒ¨åˆ†
        params = f"{self.chunk_size}_{self.chunk_overlap}"
        content_hash.update(params.encode('utf-8'))
        
        return self.chunks_dir / f"chunks_{content_hash.hexdigest()[:16]}.json"
    
    def _save_chunks(self, chunks: List[Document], cache_path: Path):
        """
        å°†åˆ†å—ç»“æžœä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            chunks: åˆ†å—åŽçš„æ–‡æ¡£åˆ—è¡¨
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        data = []
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "index": i,
                "content": chunk.page_content,
                "metadata": chunk.metadata,
                "size": len(chunk.page_content)
            }
            data.append(chunk_data)
            
            # åŒæ—¶ä¿å­˜å•ç‹¬çš„ chunk æ–‡ä»¶ä»¥ä¾¿æŸ¥çœ‹
            chunk_file = self.chunks_dir / f"chunk_{i:04d}.md"
            self._save_single_chunk(chunk, i, chunk_file)
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({
                "total_chunks": len(chunks),
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "chunks": data
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ’¾ åˆ†å—æ•°æ®å·²ä¿å­˜: {cache_path}")
    
    def _save_single_chunk(self, chunk: Document, index: int, file_path: Path):
        """
        ä¿å­˜å•ä¸ª chunk ä¸ºå¯è¯»çš„ Markdown æ–‡ä»¶
        
        Args:
            chunk: åˆ†å—æ–‡æ¡£
            index: åˆ†å—ç´¢å¼•
            file_path: æ–‡ä»¶è·¯å¾„
        """
        source = chunk.metadata.get("file_name", "æœªçŸ¥æ¥æº")
        content = f"""# Chunk {index}

## å…ƒæ•°æ®
- **æ¥æºæ–‡ä»¶**: {source}
- **å­—ç¬¦æ•°**: {len(chunk.page_content)}
- **åˆ†å—ç´¢å¼•**: {index}

## å†…å®¹

```
{chunk.page_content}
```
"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _load_chunks(self, cache_path: Path) -> Optional[List[Document]]:
        """
        ä»Žæœ¬åœ°æ–‡ä»¶è¯»å–åˆ†å—ç»“æžœ
        
        Args:
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            
        Returns:
            Optional[List[Document]]: åˆ†å—æ–‡æ¡£åˆ—è¡¨ï¼Œå¦‚æžœä¸å­˜åœ¨åˆ™è¿”å›ž None
        """
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            chunks = []
            for chunk_data in data["chunks"]:
                doc = Document(
                    page_content=chunk_data["content"],
                    metadata=chunk_data["metadata"]
                )
                chunks.append(doc)
            
            logger.info(f"ðŸ“‚ ä»Žç¼“å­˜åŠ è½½åˆ†å—: {len(chunks)} ä¸ªå—")
            return chunks
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def split_documents(
        self, 
        documents: List[Document],
        use_cache: bool = True,
        force_resplit: bool = False
    ) -> List[Document]:
        """
        å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œåˆ†å—
        
        Args:
            documents: åŽŸå§‹æ–‡æ¡£åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_resplit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†å—ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            List[Document]: åˆ†å—åŽçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        cache_path = self._get_cache_path(documents)
        
        # å°è¯•ä»Žç¼“å­˜åŠ è½½
        if use_cache and not force_resplit:
            cached_chunks = self._load_chunks(cache_path)
            if cached_chunks:
                return cached_chunks
        
        # æ‰§è¡Œåˆ†å—
        logger.info("ðŸ”„ å¼€å§‹åˆ†å—å¤„ç†...")
        chunks = self.splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ ç´¢å¼•ä¿¡æ¯
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_index"] = i
            chunk.metadata["chunk_size"] = len(chunk.page_content)
        
        logger.info(
            f"âœ… åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªå—"
        )
        
        # æ‰“å°åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        sizes = [len(c.page_content) for c in chunks]
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        logger.info(
            f"ðŸ“Š åˆ†å—ç»Ÿè®¡: å¹³å‡å¤§å°={avg_size:.0f}, "
            f"æœ€å°={min(sizes) if sizes else 0}, "
            f"æœ€å¤§={max(sizes) if sizes else 0}"
        )
        
        # ä¿å­˜åˆ°æœ¬åœ°
        self._save_chunks(chunks, cache_path)
        
        return chunks
    
    def split_text(self, text: str) -> List[str]:
        """
        å¯¹çº¯æ–‡æœ¬è¿›è¡Œåˆ†å—
        
        Args:
            text: åŽŸå§‹æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å—åŽçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text:
            return []
        
        chunks = self.splitter.split_text(text)
        logger.info(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆ: {len(text)} å­—ç¬¦ -> {len(chunks)} ä¸ªå—")
        return chunks
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜çš„åˆ†å—æ–‡ä»¶"""
        import shutil
        if self.chunks_dir.exists():
            shutil.rmtree(self.chunks_dir)
            self.chunks_dir.mkdir(parents=True, exist_ok=True)
            logger.info("ðŸ—‘ï¸ åˆ†å—ç¼“å­˜å·²æ¸…ç©º")
    
    def list_cached_chunks(self) -> List[Path]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„ chunk æ–‡ä»¶
        
        Returns:
            List[Path]: chunk æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not self.chunks_dir.exists():
            return []
        
        return sorted(self.chunks_dir.glob("chunk_*.md"))
