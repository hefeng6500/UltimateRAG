"""
æ–‡æœ¬åˆ†å—æ¨¡å—

å®ç° FixedSizeChunkingï¼ˆå›ºå®šå¤§å°åˆ†å—ï¼‰ç­–ç•¥ã€‚
Phase 1 ä½¿ç”¨åŸºç¡€çš„å›ºå®šåˆ†å—ï¼Œåç»­é˜¶æ®µå°†å¼•å…¥è¯­ä¹‰åˆ†å—ã€‚
"""

from typing import List
from loguru import logger

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


class TextChunker:
    """
    æ–‡æœ¬åˆ†å—å™¨
    
    å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„å°å—ï¼Œä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢ã€‚
    ä½¿ç”¨ RecursiveCharacterTextSplitter å®ç°æ™ºèƒ½åˆ†å‰²ã€‚
    """
    
    def __init__(
        self,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        separators: List[str] = None
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
            separators: åˆ†å‰²ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # é»˜è®¤åˆ†å‰²ç¬¦ï¼šæŒ‰æ®µè½ -> å¥å­ -> è¯è¯­é¡ºåºåˆ†å‰²
        self.separators = separators or [
            "\n\n",  # æ®µè½
            "\n",    # æ¢è¡Œ
            "ã€‚",    # ä¸­æ–‡å¥å·
            ".",     # è‹±æ–‡å¥å·
            "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
            "!",     # è‹±æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",    # ä¸­æ–‡é—®å·
            "?",     # è‹±æ–‡é—®å·
            "ï¼›",    # ä¸­æ–‡åˆ†å·
            ";",     # è‹±æ–‡åˆ†å·
            "ï¼Œ",    # ä¸­æ–‡é€—å·
            ",",     # è‹±æ–‡é€—å·
            " ",     # ç©ºæ ¼
            "",      # å­—ç¬¦çº§åˆ«
        ]
        
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=self.separators,
            length_function=len,
            is_separator_regex=False,
        )
        
        logger.info(
            f"âœ‚ï¸ æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–: "
            f"chunk_size={chunk_size}, overlap={chunk_overlap}"
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        å¯¹æ–‡æ¡£åˆ—è¡¨è¿›è¡Œåˆ†å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        chunks = self.splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ ç´¢å¼•ä¿¡æ¯
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_index"] = i
            chunk.metadata["chunk_size"] = len(chunk.page_content)
        
        logger.info(
            f"âœ… åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªå—"
        )
        
        # æ‰“å°åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        sizes = [len(c.page_content) for c in chunks]
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        logger.debug(
            f"ğŸ“Š åˆ†å—ç»Ÿè®¡: å¹³å‡å¤§å°={avg_size:.0f}, "
            f"æœ€å°={min(sizes) if sizes else 0}, "
            f"æœ€å¤§={max(sizes) if sizes else 0}"
        )
        
        return chunks
    
    def split_text(self, text: str) -> List[str]:
        """
        å¯¹çº¯æ–‡æœ¬è¿›è¡Œåˆ†å—
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not text:
            return []
        
        chunks = self.splitter.split_text(text)
        logger.info(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆ: {len(text)} å­—ç¬¦ -> {len(chunks)} ä¸ªå—")
        return chunks
