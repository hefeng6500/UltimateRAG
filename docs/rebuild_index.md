# 文档重建索引工具

本工具用于重新对 `data/documents` 目录下的文档进行切块（chunking）和向量化（embedding），支持在 Stage 1、2、3 中统一使用。

## 快速开始

```bash
# 重建所有 stage 的索引（使用固定分块）
python -m src.rebuild_index

# 重建所有 stage 的索引（使用语义分块）
python -m src.rebuild_index --semantic
```

## 命令行参数

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `--stage {1,2,3,all}` | 目标 Stage | `all` |
| `--data PATH` | 文档路径（文件或目录） | `./data/documents` |
| `--chunks-dir PATH` | chunk 缓存目录 | `./data/chunks` |
| `--semantic` | 使用语义分块 | 否（使用固定分块） |
| `--no-metadata` | 跳过元数据提取 | 否 |
| `--keep-cache` | 保留旧缓存数据 | 否（清除旧缓存） |

## 使用示例

### 1. 重建单个 Stage 的索引

```bash
# 仅重建 Stage 1 的索引
python -m src.rebuild_index --stage 1

# 仅重建 Stage 2 的索引
python -m src.rebuild_index --stage 2

# 仅重建 Stage 3 的索引
python -m src.rebuild_index --stage 3
```

### 2. 使用语义分块

语义分块会根据文本的语义边界进行切分，相比固定分块能更好地保持上下文连贯性。

```bash
# 使用语义分块重建 Stage 2 的索引
python -m src.rebuild_index --stage 2 --semantic

# 使用语义分块重建所有 Stage 的索引
python -m src.rebuild_index --semantic
```

### 3. 指定文档目录

```bash
# 从自定义目录加载文档
python -m src.rebuild_index --data ./my_documents

# 指定单个文件
python -m src.rebuild_index --data ./data/documents/my_file.pdf
```

### 4. 保留旧缓存

默认情况下，脚本会清除旧的 chunk 缓存和向量库数据。如果你只想增量添加，可以使用 `--keep-cache`：

```bash
python -m src.rebuild_index --keep-cache
```

### 5. 跳过元数据提取

元数据提取会为文档添加额外信息（如文件类型、创建时间等），主要用于 Stage 2 和 Stage 3。

```bash
python -m src.rebuild_index --no-metadata
```

## 执行流程

脚本执行时会按以下步骤进行：

```
1. 清除旧数据
   ├── 清除 chunk 缓存 (./data/chunks)
   └── 清除向量库集合

2. 加载文档
   └── 从指定路径加载所有支持的文档格式

3. 元数据提取（可选）
   └── 为文档添加额外元信息

4. 分块处理
   ├── 固定分块：按固定字符数切分
   └── 语义分块：按语义边界切分

5. 向量化存储
   └── 将分块存入各 Stage 对应的向量库集合
```

## Stage 与向量库集合对应关系

| Stage | 集合名称 | 说明 |
|-------|----------|------|
| 1 | `rag_documents` | 基础 RAG |
| 2 | `advanced_rag` | 高级 RAG |
| 3 | `agentic_rag` | Agentic RAG |

## 分块方式对比

### 固定分块（默认）

- **优点**：速度快，不需要调用 embedding API
- **缺点**：可能在语义中间截断
- **适用场景**：快速测试、简单文档

### 语义分块（`--semantic`）

- **优点**：保持语义完整性，检索效果更好
- **缺点**：需要调用 embedding API，速度较慢
- **适用场景**：生产环境、长文档、复杂文档

## 输出示例

```
============================================================
🔄 文档重建索引工具
============================================================

📁 数据路径: /path/to/data/documents
📦 目标 Stage: all
✂️ 分块方式: 固定分块
📋 元数据提取: 是
🧹 清除旧缓存: 是

==================================================
🧹 清除旧数据...
==================================================
🗑️ 已清除 chunk 缓存: data/chunks
🗑️ 已清除向量库集合: rag_documents
🗑️ 已清除向量库集合: advanced_rag
🗑️ 已清除向量库集合: agentic_rag

==================================================
📄 加载文档...
==================================================
📄 已加载 5 个文档

==================================================
📋 提取元数据...
==================================================
📋 元数据提取完成

==================================================
✂️ 固定分块...
==================================================
✂️ 固定分块完成: 5 个文档 -> 42 个块

==================================================
🗄️ 向量化存储...
==================================================
📦 Stage 1 (rag_documents)...
✅ Stage 1 完成: 42 个向量
📦 Stage 2 (advanced_rag)...
✅ Stage 2 完成: 42 个向量
📦 Stage 3 (agentic_rag)...
✅ Stage 3 完成: 42 个向量

==================================================
🎉 索引重建完成!
==================================================

==================================================
📊 重建摘要
==================================================
  • 文档数量: 5
  • 分块数量: 42
  • 分块方式: 固定分块
  • 目标 Stage: 1, 2, 3
  • 元数据提取: 是
==================================================
```

## 常见问题

### Q: 为什么要清除旧缓存？

A: 清除旧缓存可以确保索引数据的一致性。如果文档内容已更新，旧的缓存可能导致索引不正确。

### Q: 语义分块很慢怎么办？

A: 语义分块需要调用 embedding API 计算语义相似度，速度取决于文档大小和 API 响应速度。对于大量文档，建议：
1. 先用固定分块测试
2. 生产环境再使用语义分块
3. 分批处理大量文档

### Q: 如何只更新部分文档？

A: 目前脚本会重建完整索引。如需增量更新，可以：
1. 使用 `--keep-cache` 保留现有数据
2. 手动调用各 Stage 的 `add_documents` 方法

### Q: 支持哪些文档格式？

A: 支持的格式取决于 `DocumentLoader` 的配置，通常包括：
- PDF (`.pdf`)
- Word (`.docx`)
- Markdown (`.md`)
- 文本文件 (`.txt`)
- 其他 LangChain 支持的格式

